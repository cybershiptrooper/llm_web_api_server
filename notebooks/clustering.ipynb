{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "sys.path.append('../')\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "# abs path of ../\n",
    "dir = os.path.abspath(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\")\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=2048, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "full_doc = \"\\n\".join([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text.page_content for text in texts]\n",
    "doc_list = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_data(data, remove_new_lines=True):\n",
    "    # Remove Emails\n",
    "    ans = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    if remove_new_lines:\n",
    "        ans = [re.sub('\\s+', ' ', sent) for sent in ans]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    ans = [re.sub(\"\\'\", \"\", sent) for sent in ans]\n",
    "\n",
    "    # remove weird characters\n",
    "    ans = [re.sub(\"[^a-zA-Z0-9,\\n]+\", \" \", sent) for sent in ans]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list_processed = process_data(doc_list)\n",
    "text_list_processed = process_data(text_list)\n",
    "full_doc_processed = \" \".join(doc_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb = Chroma.from_documents(documents=texts, embedding=SentenceTransformerEmbeddings())\n",
    "for text in texts:\n",
    "    text.page_content = process_data([text.page_content])[0]\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents= texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': 'attention is all you need ashish vaswani google brain avaswani google com noam shazeer google brain noam google com niki parmar google research nikip google com jakob uszkoreit google research usz google com llion jones google research llion google com aidan n gomez university of toronto aidan cs toronto edu ukasz kaiser google brain lukaszkaiser google com illia polosukhin illia polosukhin gmail com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task, improving over the existing best results, including ensembles, by over 2 bleu on the wmt 2014 english to french translation task, our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus, a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data 1 introduction recurrent neural networks, long short term memory', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention is all you need ashish vaswani google brain avaswani google com noam shazeer google brain noam google com niki parmar google research nikip google com jakob uszkoreit google research usz google com llion jones google research llion google com aidan n gomez university of toronto aidan cs toronto edu ukasz kaiser google brain lukaszkaiser google com illia polosukhin illia polosukhin gmail com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantly less time to train our model achieves 28 4 bleu on the wmt 2014 english to german translation task, improving over the existing best results, including ensembles, by over 2 bleu on the wmt 2014 english to french translation task, our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus, a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data 1 introduction recurrent neural networks, long short term memory', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4798646867275238),\n",
       " (Document(lc_kwargs={'page_content': 'attention visualizations input input layer5 it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult eos pad pad pad pad pad pad it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult eos pad pad pad pad pad pad figure 3 an example of the attention mechanism following long distance dependencies in the encoder self attention in layer 5 of 6 many of the attention heads attend to a distant dependency of the verb making , completing the phrase making more dif cult attentions here shown only for the word making different colors represent different heads best viewed in color 13', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention visualizations input input layer5 it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult eos pad pad pad pad pad pad it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult eos pad pad pad pad pad pad figure 3 an example of the attention mechanism following long distance dependencies in the encoder self attention in layer 5 of 6 many of the attention heads attend to a distant dependency of the verb making , completing the phrase making more dif cult attentions here shown only for the word making different colors represent different heads best viewed in color 13', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4822063744068146),\n",
       " (Document(lc_kwargs={'page_content': 'et al google s neural machine translation system bridging the gap between human and machine translation arxiv preprint arxiv 1609 08144, 2016 39 jie zhou, ying cao, xuguang wang, peng li, and wei xu deep recurrent models with fast forward connections for neural machine translation corr, abs 1606 04199, 2016 40 muhua zhu, yue zhang, wenliang chen, min zhang, and jingbo zhu fast and accurate shift reduce constituent parsing in proceedings of the 51st annual meeting of the acl volume 1 long papers , pages 434 443 acl, august 2013 12', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 11, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='et al google s neural machine translation system bridging the gap between human and machine translation arxiv preprint arxiv 1609 08144, 2016 39 jie zhou, ying cao, xuguang wang, peng li, and wei xu deep recurrent models with fast forward connections for neural machine translation corr, abs 1606 04199, 2016 40 muhua zhu, yue zhang, wenliang chen, min zhang, and jingbo zhu fast and accurate shift reduce constituent parsing in proceedings of the 51st annual meeting of the acl volume 1 long papers , pages 434 443 acl, august 2013 12', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 11, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4826749861240387),\n",
       " (Document(lc_kwargs={'page_content': 'input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad figure 4 two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution top full attentions for head 5 bottom isolated attentions from just the word its for attention heads 5 and 6 note that the attentions are very sharp for this word 14', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad figure 4 two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution top full attentions for head 5 bottom isolated attentions from just the word its for attention heads 5 and 6 note that the attentions are very sharp for this word 14', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4837290644645691),\n",
       " (Document(lc_kwargs={'page_content': 'input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad figure 5 many of the attention heads exhibit behaviour that seems related to the structure of the sentence we give two such examples above, from two different heads from the encoder self attention at layer 5 of 6 the heads clearly learned to perform different tasks 15', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad input input layer5 the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad the law will never be perfect, but its application should be just this is what we are missing, in my opinion eos pad figure 5 many of the attention heads exhibit behaviour that seems related to the structure of the sentence we give two such examples above, from two different heads from the encoder self attention at layer 5 of 6 the heads clearly learned to perform different tasks 15', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4888313114643097)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(\"write a blog post on this\", k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot - doesn't work without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "mname=\"facebook/bart-large-mnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Make a quiz with 3 questions based on these pdfs. Add images to make it pretty',\n",
       " 'labels': ['look for a specific part of document',\n",
       "  'look in the whole document'],\n",
       " 'scores': [0.7734959721565247, 0.22650405764579773]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Make a quiz with 3 questions based on these pdfs. Add images to make it pretty\", [\"look in the whole document\", \"look for a specific part of document\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode([text.page_content for text in texts], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, count = c_tf_idf([doc.page_content for doc in documents], m=len(documents))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, OpenAI\n",
    "import openai\n",
    "# ctfidf_model = ClassTfidfTransformer()\n",
    "# topic_model = BERTopic(ctfidf_model=ctfidf_model )\n",
    "# set openai api key\n",
    "\n",
    "kbm = KeyBERTInspired()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "gpt = OpenAI()\n",
    "# need to fit this to some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x29bcf9b20>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from bertopic.vectorizers import OnlineCountVectorizer, ClassTfidfTransformer\n",
    "\n",
    "# Prepare sub-models that support online learning\n",
    "umap_model = IncrementalPCA(n_components=10)\n",
    "cluster_model = MiniBatchKMeans(n_clusters=5, random_state=0)\n",
    "vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01)\n",
    "topic_model = BERTopic(representation_model=kbm,\n",
    "                    #    ctfidf_model= ClassTfidfTransformer(),\n",
    "                       umap_model=BaseDimensionalityReduction(),\n",
    "                       hdbscan_model=cluster_model,\n",
    "                       vectorizer_model=vectorizer_model, \n",
    "                       n_gram_range=(1, 3),)\n",
    "topic_model.partial_fit(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " ['0_attention_neural_memory',\n",
       "  '1_models_optimizer_model',\n",
       "  '2_attentions_attention_layer5',\n",
       "  '3_attention_encoder_softmax',\n",
       "  '4_transformer_encoding_parser'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_model.get_topics()), topic_model.generate_topic_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distr, _ = topic_model.approximate_distribution(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/IPython/core/formatters.py:922\u001b[0m, in \u001b[0;36mIPythonDisplayFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    920\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     method()\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/plotly/basedatatypes.py:834\u001b[0m, in \u001b[0;36mBaseFigure._ipython_display_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mrender_on_display \u001b[39mand\u001b[39;00m pio\u001b[39m.\u001b[39mrenderers\u001b[39m.\u001b[39mdefault:\n\u001b[0;32m--> 834\u001b[0m     pio\u001b[39m.\u001b[39;49mshow(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    836\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/plotly/io/_renderers.py:396\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(fig, renderer, validate, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         )\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nbformat \u001b[39mor\u001b[39;00m Version(nbformat\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m Version(\u001b[39m\"\u001b[39m\u001b[39m4.2.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    397\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     ipython_display\u001b[39m.\u001b[39mdisplay(bundle, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    402\u001b[0m \u001b[39m# external renderers\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#C8D2D7",
          "line": {
           "color": "#6E8484",
           "width": 1
          }
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          0.526879400901921,
          0.11852822479110402,
          0.2080501082142599,
          0.13833753538897336
         ],
         "y": [
          "<b>Topic 0</b>: attention_neural_memory_...",
          "<b>Topic 1</b>: models_optimizer_model_l...",
          "<b>Topic 3</b>: attention_encoder_softma...",
          "<b>Topic 4</b>: transformer_encoding_par..."
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Topic Probability Distribution</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Probability"
         }
        }
       }
      },
      "text/html": [
       "<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"8131dc59-649f-4831-8a17-d4cc055105e8\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8131dc59-649f-4831-8a17-d4cc055105e8\")) {                    Plotly.newPlot(                        \"8131dc59-649f-4831-8a17-d4cc055105e8\",                        [{\"marker\":{\"color\":\"#C8D2D7\",\"line\":{\"color\":\"#6E8484\",\"width\":1}},\"orientation\":\"h\",\"x\":[0.526879400901921,0.11852822479110402,0.2080501082142599,0.13833753538897336],\"y\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e: attention_neural_memory_...\",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e: models_optimizer_model_l...\",\"\\u003cb\\u003eTopic 3\\u003c\\u002fb\\u003e: attention_encoder_softma...\",\"\\u003cb\\u003eTopic 4\\u003c\\u002fb\\u003e: transformer_encoding_par...\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopic Probability Distribution\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"xaxis\":{\"title\":{\"text\":\"Probability\"}},\"width\":800,\"height\":600},                        {\"responsive\": true}                    )                };                            </script>        </div>"
      ],
      "text/plain": [
       "Figure({\n",
       "    'data': [{'marker': {'color': '#C8D2D7', 'line': {'color': '#6E8484', 'width': 1}},\n",
       "              'orientation': 'h',\n",
       "              'type': 'bar',\n",
       "              'x': [0.526879400901921, 0.11852822479110402, 0.2080501082142599,\n",
       "                    0.13833753538897336],\n",
       "              'y': [<b>Topic 0</b>: attention_neural_memory_..., <b>Topic 1</b>:\n",
       "                    models_optimizer_model_l..., <b>Topic 3</b>:\n",
       "                    attention_encoder_softma..., <b>Topic 4</b>:\n",
       "                    transformer_encoding_par...]}],\n",
       "    'layout': {'height': 600,\n",
       "               'hoverlabel': {'bgcolor': 'white', 'font': {'family': 'Rockwell', 'size': 16}},\n",
       "               'template': '...',\n",
       "               'title': {'font': {'color': 'Black', 'size': 22},\n",
       "                         'text': '<b>Topic Probability Distribution</b>',\n",
       "                         'x': 0.5,\n",
       "                         'xanchor': 'center',\n",
       "                         'y': 0.95,\n",
       "                         'yanchor': 'top'},\n",
       "               'width': 800,\n",
       "               'xaxis': {'title': {'text': 'Probability'}}}\n",
       "})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.visualize_distribution(topic_distr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 4, 0, 3, 2], [0.35789427, 0.33362287, 0.32280946, 0.317675, 0.2204592])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.find_topics(\"write a blog post on how transformers are trained\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 1, 3, 4], [0.3414302, 0.34117335, 0.27029964, 0.22633791, 0.1809462])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rank text_processed_list based on similarity with query\n",
    "topic_model.find_topics(\"write a blog post on how the causal attention works\", top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 4, 1, 2], [0.5033019, 0.44644505, 0.4347789, 0.38372797, 0.37768108])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.find_topics(text_list_processed[1], top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 5)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_distr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = topic_model.get_document_info(text_list_processed)\n",
    "docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: Int64Index([0, 1, 2, 3, 4, 14, 24, 25, 26, 27, 28, 29, 30, 31, 32], dtype='int64'),\n",
       " 1: Int64Index([16, 19, 20], dtype='int64'),\n",
       " 2: Int64Index([33, 34, 35], dtype='int64'),\n",
       " 3: Int64Index([5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 17], dtype='int64'),\n",
       " 4: Int64Index([18, 21, 22, 23], dtype='int64')}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_per_topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "stopwords = stopwords.union({'from', 'subject', 're', 'edu', 'use', 'et', 'al', 'eos'})\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36, 36)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "import gensim\n",
    "text_list_processed_gensim = list(sent_to_words(text_list_processed))\n",
    "words = [[word for word in text if word not in stopwords] for text in text_list_processed_gensim]\n",
    "len(words), len(text_list_processed_gensim), len(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['attention',\n",
       "  'need',\n",
       "  'ashish',\n",
       "  'vaswani',\n",
       "  'google',\n",
       "  'brain',\n",
       "  'avaswani',\n",
       "  'google',\n",
       "  'com',\n",
       "  'noam',\n",
       "  'shazeer',\n",
       "  'google',\n",
       "  'brain',\n",
       "  'noam',\n",
       "  'google',\n",
       "  'com',\n",
       "  'niki',\n",
       "  'parmar',\n",
       "  'google',\n",
       "  'research',\n",
       "  'nikip',\n",
       "  'google',\n",
       "  'com',\n",
       "  'jakob',\n",
       "  'uszkoreit',\n",
       "  'google',\n",
       "  'research',\n",
       "  'usz',\n",
       "  'google',\n",
       "  'com',\n",
       "  'llion',\n",
       "  'jones',\n",
       "  'google',\n",
       "  'research',\n",
       "  'llion',\n",
       "  'google',\n",
       "  'com',\n",
       "  'aidan',\n",
       "  'gomez',\n",
       "  'university',\n",
       "  'toronto',\n",
       "  'aidan',\n",
       "  'cs',\n",
       "  'toronto',\n",
       "  'ukasz',\n",
       "  'kaiser',\n",
       "  'google',\n",
       "  'brain',\n",
       "  'lukaszkaiser',\n",
       "  'google',\n",
       "  'com',\n",
       "  'illia',\n",
       "  'polosukhin',\n",
       "  'illia',\n",
       "  'polosukhin',\n",
       "  'gmail',\n",
       "  'com',\n",
       "  'abstract',\n",
       "  'dominant',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'models',\n",
       "  'based',\n",
       "  'complex',\n",
       "  'recurrent',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'include',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'best',\n",
       "  'performing',\n",
       "  'models',\n",
       "  'connect',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'propose',\n",
       "  'new',\n",
       "  'simple',\n",
       "  'network',\n",
       "  'architecture',\n",
       "  'transformer',\n",
       "  'based',\n",
       "  'solely',\n",
       "  'attention',\n",
       "  'mechanisms',\n",
       "  'dispensing',\n",
       "  'recurrence',\n",
       "  'convolutions',\n",
       "  'entirely',\n",
       "  'experiments',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'tasks',\n",
       "  'models',\n",
       "  'superior',\n",
       "  'quality',\n",
       "  'parallelizable',\n",
       "  'requiring',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'time',\n",
       "  'train',\n",
       "  'model',\n",
       "  'achieves',\n",
       "  'bleu',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'german',\n",
       "  'translation',\n",
       "  'task',\n",
       "  'improving',\n",
       "  'existing',\n",
       "  'best',\n",
       "  'results',\n",
       "  'including',\n",
       "  'ensembles',\n",
       "  'bleu',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'french',\n",
       "  'translation',\n",
       "  'task',\n",
       "  'model',\n",
       "  'establishes',\n",
       "  'new',\n",
       "  'single',\n",
       "  'model',\n",
       "  'state',\n",
       "  'art',\n",
       "  'bleu',\n",
       "  'score',\n",
       "  'training',\n",
       "  'days',\n",
       "  'gpus',\n",
       "  'small',\n",
       "  'fraction',\n",
       "  'training',\n",
       "  'costs',\n",
       "  'best',\n",
       "  'models',\n",
       "  'literature',\n",
       "  'transformer',\n",
       "  'generalizes',\n",
       "  'tasks',\n",
       "  'applying',\n",
       "  'successfully',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'large',\n",
       "  'limited',\n",
       "  'training',\n",
       "  'data',\n",
       "  'introduction',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory'],\n",
       " ['costs',\n",
       "  'best',\n",
       "  'models',\n",
       "  'literature',\n",
       "  'transformer',\n",
       "  'generalizes',\n",
       "  'tasks',\n",
       "  'applying',\n",
       "  'successfully',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'large',\n",
       "  'limited',\n",
       "  'training',\n",
       "  'data',\n",
       "  'introduction',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'gated',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'particular',\n",
       "  'rmly',\n",
       "  'established',\n",
       "  'state',\n",
       "  'art',\n",
       "  'approaches',\n",
       "  'sequence',\n",
       "  'modeling',\n",
       "  'equal',\n",
       "  'contribution',\n",
       "  'listing',\n",
       "  'order',\n",
       "  'random',\n",
       "  'jakob',\n",
       "  'proposed',\n",
       "  'replacing',\n",
       "  'rnns',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'started',\n",
       "  'effort',\n",
       "  'evaluate',\n",
       "  'idea',\n",
       "  'ashish',\n",
       "  'illia',\n",
       "  'designed',\n",
       "  'implemented',\n",
       "  'rst',\n",
       "  'transformer',\n",
       "  'models',\n",
       "  'crucially',\n",
       "  'involved',\n",
       "  'aspect',\n",
       "  'work',\n",
       "  'noam',\n",
       "  'proposed',\n",
       "  'scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'parameter',\n",
       "  'free',\n",
       "  'position',\n",
       "  'representation',\n",
       "  'person',\n",
       "  'involved',\n",
       "  'nearly',\n",
       "  'detail',\n",
       "  'niki',\n",
       "  'designed',\n",
       "  'implemented',\n",
       "  'tuned',\n",
       "  'evaluated',\n",
       "  'countless',\n",
       "  'model',\n",
       "  'variants',\n",
       "  'original',\n",
       "  'codebase',\n",
       "  'tensor',\n",
       "  'tensor',\n",
       "  'llion',\n",
       "  'experimented',\n",
       "  'novel',\n",
       "  'model',\n",
       "  'variants',\n",
       "  'responsible',\n",
       "  'initial',\n",
       "  'codebase',\n",
       "  'ef',\n",
       "  'cient',\n",
       "  'inference',\n",
       "  'visualizations',\n",
       "  'lukasz',\n",
       "  'aidan',\n",
       "  'spent',\n",
       "  'countless',\n",
       "  'long',\n",
       "  'days',\n",
       "  'designing',\n",
       "  'parts',\n",
       "  'implementing',\n",
       "  'tensor',\n",
       "  'tensor',\n",
       "  'replacing',\n",
       "  'earlier',\n",
       "  'codebase',\n",
       "  'greatly',\n",
       "  'improving',\n",
       "  'results',\n",
       "  'massively',\n",
       "  'accelerating',\n",
       "  'research',\n",
       "  'work',\n",
       "  'performed',\n",
       "  'google',\n",
       "  'brain',\n",
       "  'work',\n",
       "  'performed',\n",
       "  'google',\n",
       "  'research',\n",
       "  'st',\n",
       "  'conference',\n",
       "  'neural',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'systems',\n",
       "  'nips',\n",
       "  'long',\n",
       "  'beach',\n",
       "  'usa',\n",
       "  'arxiv',\n",
       "  'cs',\n",
       "  'cl',\n",
       "  'dec'],\n",
       " ['transduction',\n",
       "  'problems',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'numerous',\n",
       "  'efforts',\n",
       "  'continued',\n",
       "  'push',\n",
       "  'boundaries',\n",
       "  'recurrent',\n",
       "  'language',\n",
       "  'models',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'architectures',\n",
       "  'recurrent',\n",
       "  'models',\n",
       "  'typically',\n",
       "  'factor',\n",
       "  'computation',\n",
       "  'symbol',\n",
       "  'positions',\n",
       "  'input',\n",
       "  'output',\n",
       "  'sequences',\n",
       "  'aligning',\n",
       "  'positions',\n",
       "  'steps',\n",
       "  'computation',\n",
       "  'time',\n",
       "  'generate',\n",
       "  'sequence',\n",
       "  'hidden',\n",
       "  'states',\n",
       "  'ht',\n",
       "  'function',\n",
       "  'previous',\n",
       "  'hidden',\n",
       "  'state',\n",
       "  'ht',\n",
       "  'input',\n",
       "  'position',\n",
       "  'inherently',\n",
       "  'sequential',\n",
       "  'nature',\n",
       "  'precludes',\n",
       "  'parallelization',\n",
       "  'training',\n",
       "  'examples',\n",
       "  'critical',\n",
       "  'longer',\n",
       "  'sequence',\n",
       "  'lengths',\n",
       "  'memory',\n",
       "  'constraints',\n",
       "  'limit',\n",
       "  'batching',\n",
       "  'examples',\n",
       "  'recent',\n",
       "  'work',\n",
       "  'achieved',\n",
       "  'signi',\n",
       "  'cant',\n",
       "  'improvements',\n",
       "  'computational',\n",
       "  'ef',\n",
       "  'ciency',\n",
       "  'factorization',\n",
       "  'tricks',\n",
       "  'conditional',\n",
       "  'computation',\n",
       "  'improving',\n",
       "  'model',\n",
       "  'performance',\n",
       "  'case',\n",
       "  'fundamental',\n",
       "  'constraint',\n",
       "  'sequential',\n",
       "  'computation',\n",
       "  'remains',\n",
       "  'attention',\n",
       "  'mechanisms',\n",
       "  'integral',\n",
       "  'compelling',\n",
       "  'sequence',\n",
       "  'modeling',\n",
       "  'transduc',\n",
       "  'tion',\n",
       "  'models',\n",
       "  'tasks',\n",
       "  'allowing',\n",
       "  'modeling',\n",
       "  'dependencies',\n",
       "  'regard',\n",
       "  'distance',\n",
       "  'input',\n",
       "  'output',\n",
       "  'sequences',\n",
       "  'cases',\n",
       "  'attention',\n",
       "  'mechanisms',\n",
       "  'conjunction',\n",
       "  'recurrent',\n",
       "  'network',\n",
       "  'work',\n",
       "  'propose',\n",
       "  'transformer',\n",
       "  'model',\n",
       "  'architecture',\n",
       "  'eschewing',\n",
       "  'recurrence',\n",
       "  'instead',\n",
       "  'relying',\n",
       "  'entirely',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'draw',\n",
       "  'global',\n",
       "  'dependencies',\n",
       "  'input',\n",
       "  'output',\n",
       "  'transformer',\n",
       "  'allows',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'parallelization',\n",
       "  'reach',\n",
       "  'new',\n",
       "  'state',\n",
       "  'art',\n",
       "  'translation',\n",
       "  'quality',\n",
       "  'trained',\n",
       "  'little',\n",
       "  'hours',\n",
       "  'gpus',\n",
       "  'background',\n",
       "  'goal',\n",
       "  'reducing',\n",
       "  'sequential',\n",
       "  'computation',\n",
       "  'forms',\n",
       "  'foundation',\n",
       "  'extended',\n",
       "  'neural',\n",
       "  'gpu',\n",
       "  'bytenet',\n",
       "  'convs',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'basic',\n",
       "  'building',\n",
       "  'block'],\n",
       " ['background',\n",
       "  'goal',\n",
       "  'reducing',\n",
       "  'sequential',\n",
       "  'computation',\n",
       "  'forms',\n",
       "  'foundation',\n",
       "  'extended',\n",
       "  'neural',\n",
       "  'gpu',\n",
       "  'bytenet',\n",
       "  'convs',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'basic',\n",
       "  'building',\n",
       "  'block',\n",
       "  'computing',\n",
       "  'hidden',\n",
       "  'representations',\n",
       "  'parallel',\n",
       "  'input',\n",
       "  'output',\n",
       "  'positions',\n",
       "  'models',\n",
       "  'number',\n",
       "  'operations',\n",
       "  'required',\n",
       "  'relate',\n",
       "  'signals',\n",
       "  'arbitrary',\n",
       "  'input',\n",
       "  'output',\n",
       "  'positions',\n",
       "  'grows',\n",
       "  'distance',\n",
       "  'positions',\n",
       "  'linearly',\n",
       "  'convs',\n",
       "  'logarithmically',\n",
       "  'bytenet',\n",
       "  'makes',\n",
       "  'dif',\n",
       "  'cult',\n",
       "  'learn',\n",
       "  'dependencies',\n",
       "  'distant',\n",
       "  'positions',\n",
       "  'transformer',\n",
       "  'reduced',\n",
       "  'constant',\n",
       "  'number',\n",
       "  'operations',\n",
       "  'albeit',\n",
       "  'cost',\n",
       "  'reduced',\n",
       "  'effective',\n",
       "  'resolution',\n",
       "  'averaging',\n",
       "  'attention',\n",
       "  'weighted',\n",
       "  'positions',\n",
       "  'effect',\n",
       "  'counteract',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'described',\n",
       "  'section',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'called',\n",
       "  'intra',\n",
       "  'attention',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'relating',\n",
       "  'different',\n",
       "  'positions',\n",
       "  'single',\n",
       "  'sequence',\n",
       "  'order',\n",
       "  'compute',\n",
       "  'representation',\n",
       "  'sequence',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'successfully',\n",
       "  'variety',\n",
       "  'tasks',\n",
       "  'including',\n",
       "  'reading',\n",
       "  'comprehension',\n",
       "  'abstractive',\n",
       "  'summarization',\n",
       "  'textual',\n",
       "  'entailment',\n",
       "  'learning',\n",
       "  'task',\n",
       "  'independent',\n",
       "  'sentence',\n",
       "  'representations',\n",
       "  'end',\n",
       "  'end',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'based',\n",
       "  'recurrent',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'instead',\n",
       "  'sequence',\n",
       "  'aligned',\n",
       "  'recurrence',\n",
       "  'shown',\n",
       "  'perform',\n",
       "  'simple',\n",
       "  'language',\n",
       "  'question',\n",
       "  'answering',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'tasks',\n",
       "  'best',\n",
       "  'knowledge',\n",
       "  'transformer',\n",
       "  'rst',\n",
       "  'transduction',\n",
       "  'model',\n",
       "  'relying',\n",
       "  'entirely',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'compute',\n",
       "  'representations',\n",
       "  'input',\n",
       "  'output',\n",
       "  'sequence',\n",
       "  'aligned',\n",
       "  'rnns',\n",
       "  'convolution',\n",
       "  'following',\n",
       "  'sections',\n",
       "  'describe',\n",
       "  'transformer',\n",
       "  'motivate',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'discuss',\n",
       "  'advantages',\n",
       "  'models',\n",
       "  'model',\n",
       "  'architecture',\n",
       "  'competitive',\n",
       "  'neural',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'models'],\n",
       " ['lution',\n",
       "  'following',\n",
       "  'sections',\n",
       "  'describe',\n",
       "  'transformer',\n",
       "  'motivate',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'discuss',\n",
       "  'advantages',\n",
       "  'models',\n",
       "  'model',\n",
       "  'architecture',\n",
       "  'competitive',\n",
       "  'neural',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'models',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'structure',\n",
       "  'encoder',\n",
       "  'maps',\n",
       "  'input',\n",
       "  'sequence',\n",
       "  'symbol',\n",
       "  'representations',\n",
       "  'xn',\n",
       "  'sequence',\n",
       "  'continuous',\n",
       "  'representations',\n",
       "  'zn',\n",
       "  'given',\n",
       "  'decoder',\n",
       "  'generates',\n",
       "  'output',\n",
       "  'sequence',\n",
       "  'ym',\n",
       "  'symbols',\n",
       "  'element',\n",
       "  'time',\n",
       "  'step',\n",
       "  'model',\n",
       "  'auto',\n",
       "  'regressive',\n",
       "  'consuming',\n",
       "  'previously',\n",
       "  'generated',\n",
       "  'symbols',\n",
       "  'additional',\n",
       "  'input',\n",
       "  'generating',\n",
       "  'transformer',\n",
       "  'follows',\n",
       "  'overall',\n",
       "  'architecture',\n",
       "  'stacked',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'point',\n",
       "  'wise',\n",
       "  'fully',\n",
       "  'connected',\n",
       "  'layers',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'shown',\n",
       "  'left',\n",
       "  'right',\n",
       "  'halves',\n",
       "  'figure',\n",
       "  'respectively'],\n",
       " ['figure',\n",
       "  'transformer',\n",
       "  'model',\n",
       "  'architecture',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'stacks',\n",
       "  'encoder',\n",
       "  'encoder',\n",
       "  'composed',\n",
       "  'stack',\n",
       "  'identical',\n",
       "  'layers',\n",
       "  'layer',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'rst',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'second',\n",
       "  'simple',\n",
       "  'position',\n",
       "  'wise',\n",
       "  'fully',\n",
       "  'connected',\n",
       "  'feed',\n",
       "  'forward',\n",
       "  'network',\n",
       "  'employ',\n",
       "  'residual',\n",
       "  'connection',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'followed',\n",
       "  'layer',\n",
       "  'normalization',\n",
       "  'output',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'layernorm',\n",
       "  'sublayer',\n",
       "  'sublayer',\n",
       "  'function',\n",
       "  'implemented',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'facilitate',\n",
       "  'residual',\n",
       "  'connections',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'model',\n",
       "  'embedding',\n",
       "  'layers',\n",
       "  'produce',\n",
       "  'outputs',\n",
       "  'dimension',\n",
       "  'dmodel',\n",
       "  'decoder',\n",
       "  'decoder',\n",
       "  'composed',\n",
       "  'stack',\n",
       "  'identical',\n",
       "  'layers',\n",
       "  'addition',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'encoder',\n",
       "  'layer',\n",
       "  'decoder',\n",
       "  'inserts',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'performs',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'output',\n",
       "  'encoder',\n",
       "  'stack',\n",
       "  'similar',\n",
       "  'encoder',\n",
       "  'employ',\n",
       "  'residual',\n",
       "  'connections',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'followed',\n",
       "  'layer',\n",
       "  'normalization',\n",
       "  'modify',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'decoder',\n",
       "  'stack',\n",
       "  'prevent',\n",
       "  'positions',\n",
       "  'attending',\n",
       "  'subsequent',\n",
       "  'positions',\n",
       "  'masking',\n",
       "  'combined',\n",
       "  'fact',\n",
       "  'output',\n",
       "  'embeddings',\n",
       "  'offset',\n",
       "  'position',\n",
       "  'ensures',\n",
       "  'predictions',\n",
       "  'position',\n",
       "  'depend',\n",
       "  'known',\n",
       "  'outputs',\n",
       "  'positions',\n",
       "  'attention',\n",
       "  'attention',\n",
       "  'function',\n",
       "  'described',\n",
       "  'mapping',\n",
       "  'query',\n",
       "  'set',\n",
       "  'key',\n",
       "  'value',\n",
       "  'pairs',\n",
       "  'output',\n",
       "  'query',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'output',\n",
       "  'vectors',\n",
       "  'output',\n",
       "  'computed',\n",
       "  'weighted',\n",
       "  'sum',\n",
       "  'values',\n",
       "  'weight',\n",
       "  'assigned'],\n",
       " ['function',\n",
       "  'described',\n",
       "  'mapping',\n",
       "  'query',\n",
       "  'set',\n",
       "  'key',\n",
       "  'value',\n",
       "  'pairs',\n",
       "  'output',\n",
       "  'query',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'output',\n",
       "  'vectors',\n",
       "  'output',\n",
       "  'computed',\n",
       "  'weighted',\n",
       "  'sum',\n",
       "  'values',\n",
       "  'weight',\n",
       "  'assigned',\n",
       "  'value',\n",
       "  'computed',\n",
       "  'compatibility',\n",
       "  'function',\n",
       "  'query',\n",
       "  'corresponding',\n",
       "  'key'],\n",
       " ['scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'figure',\n",
       "  'left',\n",
       "  'scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'right',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'consists',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'running',\n",
       "  'parallel',\n",
       "  'scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'particular',\n",
       "  'attention',\n",
       "  'scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'figure',\n",
       "  'input',\n",
       "  'consists',\n",
       "  'queries',\n",
       "  'keys',\n",
       "  'dimension',\n",
       "  'dk',\n",
       "  'values',\n",
       "  'dimension',\n",
       "  'dv',\n",
       "  'compute',\n",
       "  'dot',\n",
       "  'products',\n",
       "  'query',\n",
       "  'keys',\n",
       "  'divide',\n",
       "  'dk',\n",
       "  'apply',\n",
       "  'softmax',\n",
       "  'function',\n",
       "  'obtain',\n",
       "  'weights',\n",
       "  'values',\n",
       "  'practice',\n",
       "  'compute',\n",
       "  'attention',\n",
       "  'function',\n",
       "  'set',\n",
       "  'queries',\n",
       "  'simultaneously',\n",
       "  'packed',\n",
       "  'matrix',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'packed',\n",
       "  'matrices',\n",
       "  'compute',\n",
       "  'matrix',\n",
       "  'outputs',\n",
       "  'attention',\n",
       "  'softmax',\n",
       "  'qkt',\n",
       "  'dk',\n",
       "  'commonly',\n",
       "  'attention',\n",
       "  'functions',\n",
       "  'additive',\n",
       "  'attention',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'multi',\n",
       "  'plicative',\n",
       "  'attention',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'identical',\n",
       "  'algorithm',\n",
       "  'scaling',\n",
       "  'factor',\n",
       "  'dk',\n",
       "  'additive',\n",
       "  'attention',\n",
       "  'computes',\n",
       "  'compatibility',\n",
       "  'function',\n",
       "  'feed',\n",
       "  'forward',\n",
       "  'network',\n",
       "  'single',\n",
       "  'hidden',\n",
       "  'layer',\n",
       "  'similar',\n",
       "  'theoretical',\n",
       "  'complexity',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'faster',\n",
       "  'space',\n",
       "  'ef',\n",
       "  'cient',\n",
       "  'practice',\n",
       "  'implemented',\n",
       "  'highly',\n",
       "  'optimized',\n",
       "  'matrix',\n",
       "  'multiplication',\n",
       "  'code',\n",
       "  'small',\n",
       "  'values',\n",
       "  'dk',\n",
       "  'mechanisms',\n",
       "  'perform',\n",
       "  'similarly',\n",
       "  'additive',\n",
       "  'attention',\n",
       "  'outperforms',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'scaling',\n",
       "  'larger',\n",
       "  'values',\n",
       "  'dk',\n",
       "  'suspect',\n",
       "  'large',\n",
       "  'values',\n",
       "  'dk',\n",
       "  'dot',\n",
       "  'products',\n",
       "  'grow',\n",
       "  'large',\n",
       "  'magnitude',\n",
       "  'pushing',\n",
       "  'softmax',\n",
       "  'function',\n",
       "  'regions',\n",
       "  'extremely',\n",
       "  'small',\n",
       "  'gradients',\n",
       "  'counteract',\n",
       "  'effect',\n",
       "  'scale',\n",
       "  'dot',\n",
       "  'products',\n",
       "  'dk',\n",
       "  'multi',\n",
       "  'head'],\n",
       " ['dot',\n",
       "  'products',\n",
       "  'grow',\n",
       "  'large',\n",
       "  'magnitude',\n",
       "  'pushing',\n",
       "  'softmax',\n",
       "  'function',\n",
       "  'regions',\n",
       "  'extremely',\n",
       "  'small',\n",
       "  'gradients',\n",
       "  'counteract',\n",
       "  'effect',\n",
       "  'scale',\n",
       "  'dot',\n",
       "  'products',\n",
       "  'dk',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'instead',\n",
       "  'performing',\n",
       "  'single',\n",
       "  'attention',\n",
       "  'function',\n",
       "  'dmodel',\n",
       "  'dimensional',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'queries',\n",
       "  'found',\n",
       "  'bene',\n",
       "  'cial',\n",
       "  'linearly',\n",
       "  'project',\n",
       "  'queries',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'times',\n",
       "  'different',\n",
       "  'learned',\n",
       "  'linear',\n",
       "  'projections',\n",
       "  'dk',\n",
       "  'dk',\n",
       "  'dv',\n",
       "  'dimensions',\n",
       "  'respectively',\n",
       "  'projected',\n",
       "  'versions',\n",
       "  'queries',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'perform',\n",
       "  'attention',\n",
       "  'function',\n",
       "  'parallel',\n",
       "  'yielding',\n",
       "  'dv',\n",
       "  'dimensional',\n",
       "  'output',\n",
       "  'values',\n",
       "  'concatenated',\n",
       "  'projected',\n",
       "  'resulting',\n",
       "  'nal',\n",
       "  'values',\n",
       "  'depicted',\n",
       "  'figure',\n",
       "  'illustrate',\n",
       "  'dot',\n",
       "  'products',\n",
       "  'large',\n",
       "  'assume',\n",
       "  'components',\n",
       "  'independent',\n",
       "  'random',\n",
       "  'variables',\n",
       "  'mean',\n",
       "  'variance',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'dk',\n",
       "  'qiki',\n",
       "  'mean',\n",
       "  'variance',\n",
       "  'dk'],\n",
       " ['multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'allows',\n",
       "  'model',\n",
       "  'jointly',\n",
       "  'attend',\n",
       "  'information',\n",
       "  'different',\n",
       "  'representation',\n",
       "  'subspaces',\n",
       "  'different',\n",
       "  'positions',\n",
       "  'single',\n",
       "  'attention',\n",
       "  'head',\n",
       "  'averaging',\n",
       "  'inhibits',\n",
       "  'multihead',\n",
       "  'concat',\n",
       "  'head',\n",
       "  'headh',\n",
       "  'headi',\n",
       "  'attention',\n",
       "  'qw',\n",
       "  'kw',\n",
       "  'projections',\n",
       "  'parameter',\n",
       "  'matrices',\n",
       "  'rdmodel',\n",
       "  'dk',\n",
       "  'rdmodel',\n",
       "  'dk',\n",
       "  'rdmodel',\n",
       "  'dv',\n",
       "  'rhdv',\n",
       "  'dmodel',\n",
       "  'work',\n",
       "  'employ',\n",
       "  'parallel',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'heads',\n",
       "  'dk',\n",
       "  'dv',\n",
       "  'dmodel',\n",
       "  'reduced',\n",
       "  'dimension',\n",
       "  'head',\n",
       "  'total',\n",
       "  'computational',\n",
       "  'cost',\n",
       "  'similar',\n",
       "  'single',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'dimensionality',\n",
       "  'applications',\n",
       "  'attention',\n",
       "  'model',\n",
       "  'transformer',\n",
       "  'uses',\n",
       "  'multi',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'different',\n",
       "  'ways',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'queries',\n",
       "  'come',\n",
       "  'previous',\n",
       "  'decoder',\n",
       "  'layer',\n",
       "  'memory',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'come',\n",
       "  'output',\n",
       "  'encoder',\n",
       "  'allows',\n",
       "  'position',\n",
       "  'decoder',\n",
       "  'attend',\n",
       "  'positions',\n",
       "  'input',\n",
       "  'sequence',\n",
       "  'mimics',\n",
       "  'typical',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'attention',\n",
       "  'mechanisms',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'models',\n",
       "  'encoder',\n",
       "  'contains',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'keys',\n",
       "  'values',\n",
       "  'queries',\n",
       "  'come',\n",
       "  'place',\n",
       "  'case',\n",
       "  'output',\n",
       "  'previous',\n",
       "  'layer',\n",
       "  'encoder',\n",
       "  'position',\n",
       "  'encoder',\n",
       "  'attend',\n",
       "  'positions',\n",
       "  'previous',\n",
       "  'layer',\n",
       "  'encoder',\n",
       "  'similarly',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'decoder',\n",
       "  'allow'],\n",
       " ['case',\n",
       "  'output',\n",
       "  'previous',\n",
       "  'layer',\n",
       "  'encoder',\n",
       "  'position',\n",
       "  'encoder',\n",
       "  'attend',\n",
       "  'positions',\n",
       "  'previous',\n",
       "  'layer',\n",
       "  'encoder',\n",
       "  'similarly',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'decoder',\n",
       "  'allow',\n",
       "  'position',\n",
       "  'decoder',\n",
       "  'attend',\n",
       "  'positions',\n",
       "  'decoder',\n",
       "  'including',\n",
       "  'position',\n",
       "  'need',\n",
       "  'prevent',\n",
       "  'leftward',\n",
       "  'information',\n",
       "  'ow',\n",
       "  'decoder',\n",
       "  'preserve',\n",
       "  'auto',\n",
       "  'regressive',\n",
       "  'property',\n",
       "  'implement',\n",
       "  'inside',\n",
       "  'scaled',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'attention',\n",
       "  'masking',\n",
       "  'setting',\n",
       "  'values',\n",
       "  'input',\n",
       "  'softmax',\n",
       "  'correspond',\n",
       "  'illegal',\n",
       "  'connections',\n",
       "  'figure',\n",
       "  'position',\n",
       "  'wise',\n",
       "  'feed',\n",
       "  'forward',\n",
       "  'networks',\n",
       "  'addition',\n",
       "  'attention',\n",
       "  'sub',\n",
       "  'layers',\n",
       "  'layers',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'contains',\n",
       "  'fully',\n",
       "  'connected',\n",
       "  'feed',\n",
       "  'forward',\n",
       "  'network',\n",
       "  'applied',\n",
       "  'position',\n",
       "  'separately',\n",
       "  'identically',\n",
       "  'consists',\n",
       "  'linear',\n",
       "  'transformations',\n",
       "  'relu',\n",
       "  'activation',\n",
       "  'ffn',\n",
       "  'max',\n",
       "  'xw',\n",
       "  'linear',\n",
       "  'transformations',\n",
       "  'different',\n",
       "  'positions',\n",
       "  'different',\n",
       "  'parameters',\n",
       "  'layer',\n",
       "  'layer',\n",
       "  'way',\n",
       "  'describing',\n",
       "  'convolutions',\n",
       "  'kernel',\n",
       "  'size',\n",
       "  'dimensionality',\n",
       "  'input',\n",
       "  'output',\n",
       "  'dmodel',\n",
       "  'inner',\n",
       "  'layer',\n",
       "  'dimensionality',\n",
       "  'dff',\n",
       "  'embeddings',\n",
       "  'softmax',\n",
       "  'similarly',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'models',\n",
       "  'learned',\n",
       "  'embeddings',\n",
       "  'convert',\n",
       "  'input',\n",
       "  'tokens',\n",
       "  'output',\n",
       "  'tokens',\n",
       "  'vectors',\n",
       "  'dimension',\n",
       "  'dmodel',\n",
       "  'usual',\n",
       "  'learned',\n",
       "  'linear',\n",
       "  'transfor',\n",
       "  'mation',\n",
       "  'softmax',\n",
       "  'function',\n",
       "  'convert',\n",
       "  'decoder',\n",
       "  'output',\n",
       "  'predicted',\n",
       "  'token',\n",
       "  'probabilities',\n",
       "  'model',\n",
       "  'share',\n",
       "  'weight',\n",
       "  'matrix',\n",
       "  'embedding',\n",
       "  'layers',\n",
       "  'pre',\n",
       "  'softmax'],\n",
       " ['usual',\n",
       "  'learned',\n",
       "  'linear',\n",
       "  'transfor',\n",
       "  'mation',\n",
       "  'softmax',\n",
       "  'function',\n",
       "  'convert',\n",
       "  'decoder',\n",
       "  'output',\n",
       "  'predicted',\n",
       "  'token',\n",
       "  'probabilities',\n",
       "  'model',\n",
       "  'share',\n",
       "  'weight',\n",
       "  'matrix',\n",
       "  'embedding',\n",
       "  'layers',\n",
       "  'pre',\n",
       "  'softmax',\n",
       "  'linear',\n",
       "  'transformation',\n",
       "  'similar',\n",
       "  'embedding',\n",
       "  'layers',\n",
       "  'multiply',\n",
       "  'weights',\n",
       "  'dmodel',\n",
       "  'positional',\n",
       "  'encoding',\n",
       "  'model',\n",
       "  'contains',\n",
       "  'recurrence',\n",
       "  'convolution',\n",
       "  'order',\n",
       "  'model',\n",
       "  'order',\n",
       "  'sequence',\n",
       "  'inject',\n",
       "  'information',\n",
       "  'relative',\n",
       "  'absolute',\n",
       "  'position'],\n",
       " ['table',\n",
       "  'maximum',\n",
       "  'path',\n",
       "  'lengths',\n",
       "  'layer',\n",
       "  'complexity',\n",
       "  'minimum',\n",
       "  'number',\n",
       "  'sequential',\n",
       "  'operations',\n",
       "  'different',\n",
       "  'layer',\n",
       "  'types',\n",
       "  'sequence',\n",
       "  'length',\n",
       "  'representation',\n",
       "  'dimension',\n",
       "  'kernel',\n",
       "  'size',\n",
       "  'convolutions',\n",
       "  'size',\n",
       "  'neighborhood',\n",
       "  'restricted',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'type',\n",
       "  'complexity',\n",
       "  'layer',\n",
       "  'sequential',\n",
       "  'maximum',\n",
       "  'path',\n",
       "  'length',\n",
       "  'operations',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'recurrent',\n",
       "  'convolutional',\n",
       "  'logk',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'restricted',\n",
       "  'tokens',\n",
       "  'sequence',\n",
       "  'end',\n",
       "  'add',\n",
       "  'positional',\n",
       "  'encodings',\n",
       "  'input',\n",
       "  'embeddings',\n",
       "  'bottoms',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'stacks',\n",
       "  'positional',\n",
       "  'encodings',\n",
       "  'dimension',\n",
       "  'dmodel',\n",
       "  'embeddings',\n",
       "  'summed',\n",
       "  'choices',\n",
       "  'positional',\n",
       "  'encodings',\n",
       "  'learned',\n",
       "  'xed',\n",
       "  'work',\n",
       "  'sine',\n",
       "  'cosine',\n",
       "  'functions',\n",
       "  'different',\n",
       "  'frequencies',\n",
       "  'pe',\n",
       "  'pos',\n",
       "  'sin',\n",
       "  'pos',\n",
       "  'dmodel',\n",
       "  'pe',\n",
       "  'pos',\n",
       "  'cos',\n",
       "  'pos',\n",
       "  'dmodel',\n",
       "  'pos',\n",
       "  'position',\n",
       "  'dimension',\n",
       "  'dimension',\n",
       "  'positional',\n",
       "  'encoding',\n",
       "  'corresponds',\n",
       "  'sinusoid',\n",
       "  'wavelengths',\n",
       "  'form',\n",
       "  'geometric',\n",
       "  'progression',\n",
       "  'chose',\n",
       "  'function',\n",
       "  'hypothesized',\n",
       "  'allow',\n",
       "  'model',\n",
       "  'easily',\n",
       "  'learn',\n",
       "  'attend',\n",
       "  'relative',\n",
       "  'positions',\n",
       "  'xed',\n",
       "  'offset',\n",
       "  'pepos'],\n",
       " ['geometric',\n",
       "  'progression',\n",
       "  'chose',\n",
       "  'function',\n",
       "  'hypothesized',\n",
       "  'allow',\n",
       "  'model',\n",
       "  'easily',\n",
       "  'learn',\n",
       "  'attend',\n",
       "  'relative',\n",
       "  'positions',\n",
       "  'xed',\n",
       "  'offset',\n",
       "  'pepos',\n",
       "  'represented',\n",
       "  'linear',\n",
       "  'function',\n",
       "  'pepos',\n",
       "  'experimented',\n",
       "  'learned',\n",
       "  'positional',\n",
       "  'embeddings',\n",
       "  'instead',\n",
       "  'found',\n",
       "  'versions',\n",
       "  'produced',\n",
       "  'nearly',\n",
       "  'identical',\n",
       "  'results',\n",
       "  'table',\n",
       "  'row',\n",
       "  'chose',\n",
       "  'sinusoidal',\n",
       "  'version',\n",
       "  'allow',\n",
       "  'model',\n",
       "  'extrapolate',\n",
       "  'sequence',\n",
       "  'lengths',\n",
       "  'longer',\n",
       "  'ones',\n",
       "  'encountered',\n",
       "  'training',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'section',\n",
       "  'compare',\n",
       "  'aspects',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'recurrent',\n",
       "  'convolu',\n",
       "  'tional',\n",
       "  'layers',\n",
       "  'commonly',\n",
       "  'mapping',\n",
       "  'variable',\n",
       "  'length',\n",
       "  'sequence',\n",
       "  'symbol',\n",
       "  'representations',\n",
       "  'xn',\n",
       "  'sequence',\n",
       "  'equal',\n",
       "  'length',\n",
       "  'zn',\n",
       "  'xi',\n",
       "  'zi',\n",
       "  'rd',\n",
       "  'hidden',\n",
       "  'layer',\n",
       "  'typical',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'motivating',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'consider',\n",
       "  'desiderata',\n",
       "  'total',\n",
       "  'computational',\n",
       "  'complexity',\n",
       "  'layer',\n",
       "  'computation',\n",
       "  'parallelized',\n",
       "  'measured',\n",
       "  'minimum',\n",
       "  'number',\n",
       "  'sequential',\n",
       "  'operations',\n",
       "  'required',\n",
       "  'path',\n",
       "  'length',\n",
       "  'long',\n",
       "  'range',\n",
       "  'dependencies',\n",
       "  'network',\n",
       "  'learning',\n",
       "  'long',\n",
       "  'range',\n",
       "  'dependencies',\n",
       "  'key',\n",
       "  'challenge',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'tasks',\n",
       "  'key',\n",
       "  'factor',\n",
       "  'affecting',\n",
       "  'ability',\n",
       "  'learn',\n",
       "  'dependencies',\n",
       "  'length',\n",
       "  'paths',\n",
       "  'forward',\n",
       "  'backward',\n",
       "  'signals',\n",
       "  'traverse',\n",
       "  'network',\n",
       "  'shorter',\n",
       "  'paths',\n",
       "  'combination',\n",
       "  'positions',\n",
       "  'input',\n",
       "  'output',\n",
       "  'sequences',\n",
       "  'easier',\n",
       "  'learn',\n",
       "  'long',\n",
       "  'range',\n",
       "  'dependencies',\n",
       "  'compare',\n",
       "  'maximum',\n",
       "  'path',\n",
       "  'length',\n",
       "  'input',\n",
       "  'output',\n",
       "  'positions',\n",
       "  'networks',\n",
       "  'composed',\n",
       "  'different',\n",
       "  'layer'],\n",
       " ['paths',\n",
       "  'combination',\n",
       "  'positions',\n",
       "  'input',\n",
       "  'output',\n",
       "  'sequences',\n",
       "  'easier',\n",
       "  'learn',\n",
       "  'long',\n",
       "  'range',\n",
       "  'dependencies',\n",
       "  'compare',\n",
       "  'maximum',\n",
       "  'path',\n",
       "  'length',\n",
       "  'input',\n",
       "  'output',\n",
       "  'positions',\n",
       "  'networks',\n",
       "  'composed',\n",
       "  'different',\n",
       "  'layer',\n",
       "  'types',\n",
       "  'noted',\n",
       "  'table',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'connects',\n",
       "  'positions',\n",
       "  'constant',\n",
       "  'number',\n",
       "  'sequentially',\n",
       "  'executed',\n",
       "  'operations',\n",
       "  'recurrent',\n",
       "  'layer',\n",
       "  'requires',\n",
       "  'sequential',\n",
       "  'operations',\n",
       "  'terms',\n",
       "  'computational',\n",
       "  'complexity',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layers',\n",
       "  'faster',\n",
       "  'recurrent',\n",
       "  'layers',\n",
       "  'sequence',\n",
       "  'length',\n",
       "  'smaller',\n",
       "  'representation',\n",
       "  'dimensionality',\n",
       "  'case',\n",
       "  'sentence',\n",
       "  'representations',\n",
       "  'state',\n",
       "  'art',\n",
       "  'models',\n",
       "  'machine',\n",
       "  'translations',\n",
       "  'word',\n",
       "  'piece',\n",
       "  'byte',\n",
       "  'pair',\n",
       "  'representations',\n",
       "  'improve',\n",
       "  'computational',\n",
       "  'performance',\n",
       "  'tasks',\n",
       "  'involving',\n",
       "  'long',\n",
       "  'sequences',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'restricted',\n",
       "  'considering',\n",
       "  'neighborhood',\n",
       "  'size'],\n",
       " ['input',\n",
       "  'sequence',\n",
       "  'centered',\n",
       "  'respective',\n",
       "  'output',\n",
       "  'position',\n",
       "  'increase',\n",
       "  'maximum',\n",
       "  'path',\n",
       "  'length',\n",
       "  'plan',\n",
       "  'investigate',\n",
       "  'approach',\n",
       "  'future',\n",
       "  'work',\n",
       "  'single',\n",
       "  'convolutional',\n",
       "  'layer',\n",
       "  'kernel',\n",
       "  'width',\n",
       "  'connect',\n",
       "  'pairs',\n",
       "  'input',\n",
       "  'output',\n",
       "  'positions',\n",
       "  'requires',\n",
       "  'stack',\n",
       "  'convolutional',\n",
       "  'layers',\n",
       "  'case',\n",
       "  'contiguous',\n",
       "  'kernels',\n",
       "  'logk',\n",
       "  'case',\n",
       "  'dilated',\n",
       "  'convolutions',\n",
       "  'increasing',\n",
       "  'length',\n",
       "  'longest',\n",
       "  'paths',\n",
       "  'positions',\n",
       "  'network',\n",
       "  'convolutional',\n",
       "  'layers',\n",
       "  'generally',\n",
       "  'expensive',\n",
       "  'recurrent',\n",
       "  'layers',\n",
       "  'factor',\n",
       "  'separable',\n",
       "  'convolutions',\n",
       "  'decrease',\n",
       "  'complexity',\n",
       "  'considerably',\n",
       "  'complexity',\n",
       "  'separable',\n",
       "  'convolution',\n",
       "  'equal',\n",
       "  'combination',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'point',\n",
       "  'wise',\n",
       "  'feed',\n",
       "  'forward',\n",
       "  'layer',\n",
       "  'approach',\n",
       "  'model',\n",
       "  'bene',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'yield',\n",
       "  'interpretable',\n",
       "  'models',\n",
       "  'inspect',\n",
       "  'attention',\n",
       "  'distributions',\n",
       "  'models',\n",
       "  'present',\n",
       "  'discuss',\n",
       "  'examples',\n",
       "  'appendix',\n",
       "  'individual',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'clearly',\n",
       "  'learn',\n",
       "  'perform',\n",
       "  'different',\n",
       "  'tasks',\n",
       "  'appear',\n",
       "  'exhibit',\n",
       "  'behavior',\n",
       "  'related',\n",
       "  'syntactic',\n",
       "  'semantic',\n",
       "  'structure',\n",
       "  'sentences',\n",
       "  'training',\n",
       "  'section',\n",
       "  'describes',\n",
       "  'training',\n",
       "  'regime',\n",
       "  'models',\n",
       "  'training',\n",
       "  'data',\n",
       "  'batching',\n",
       "  'trained',\n",
       "  'standard',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'german',\n",
       "  'dataset',\n",
       "  'consisting',\n",
       "  'million',\n",
       "  'sentence',\n",
       "  'pairs',\n",
       "  'sentences',\n",
       "  'encoded',\n",
       "  'byte',\n",
       "  'pair',\n",
       "  'encoding',\n",
       "  'shared',\n",
       "  'source',\n",
       "  'target',\n",
       "  'vocabulary',\n",
       "  'tokens',\n",
       "  'english',\n",
       "  'french',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'larger',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'french',\n",
       "  'dataset',\n",
       "  'consisting'],\n",
       " ['encoded',\n",
       "  'byte',\n",
       "  'pair',\n",
       "  'encoding',\n",
       "  'shared',\n",
       "  'source',\n",
       "  'target',\n",
       "  'vocabulary',\n",
       "  'tokens',\n",
       "  'english',\n",
       "  'french',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'larger',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'french',\n",
       "  'dataset',\n",
       "  'consisting',\n",
       "  'sentences',\n",
       "  'split',\n",
       "  'tokens',\n",
       "  'word',\n",
       "  'piece',\n",
       "  'vocabulary',\n",
       "  'sentence',\n",
       "  'pairs',\n",
       "  'batched',\n",
       "  'approximate',\n",
       "  'sequence',\n",
       "  'length',\n",
       "  'training',\n",
       "  'batch',\n",
       "  'contained',\n",
       "  'set',\n",
       "  'sentence',\n",
       "  'pairs',\n",
       "  'containing',\n",
       "  'approximately',\n",
       "  'source',\n",
       "  'tokens',\n",
       "  'target',\n",
       "  'tokens',\n",
       "  'hardware',\n",
       "  'schedule',\n",
       "  'trained',\n",
       "  'models',\n",
       "  'machine',\n",
       "  'nvidia',\n",
       "  'gpus',\n",
       "  'base',\n",
       "  'models',\n",
       "  'hyperparameters',\n",
       "  'described',\n",
       "  'paper',\n",
       "  'training',\n",
       "  'step',\n",
       "  'took',\n",
       "  'seconds',\n",
       "  'trained',\n",
       "  'base',\n",
       "  'models',\n",
       "  'total',\n",
       "  'steps',\n",
       "  'hours',\n",
       "  'big',\n",
       "  'models',\n",
       "  'described',\n",
       "  'line',\n",
       "  'table',\n",
       "  'step',\n",
       "  'time',\n",
       "  'seconds',\n",
       "  'big',\n",
       "  'models',\n",
       "  'trained',\n",
       "  'steps',\n",
       "  'days',\n",
       "  'optimizer',\n",
       "  'adam',\n",
       "  'optimizer',\n",
       "  'unk',\n",
       "  'varied',\n",
       "  'learning',\n",
       "  'rate',\n",
       "  'course',\n",
       "  'training',\n",
       "  'according',\n",
       "  'formula',\n",
       "  'lrate',\n",
       "  'model',\n",
       "  'min',\n",
       "  'step',\n",
       "  'num',\n",
       "  'step',\n",
       "  'num',\n",
       "  'warmup',\n",
       "  'steps',\n",
       "  'corresponds',\n",
       "  'increasing',\n",
       "  'learning',\n",
       "  'rate',\n",
       "  'linearly',\n",
       "  'rst',\n",
       "  'warmup',\n",
       "  'steps',\n",
       "  'training',\n",
       "  'steps',\n",
       "  'decreasing',\n",
       "  'proportionally',\n",
       "  'inverse',\n",
       "  'square',\n",
       "  'root',\n",
       "  'step',\n",
       "  'number',\n",
       "  'warmup',\n",
       "  'steps',\n",
       "  'regularization',\n",
       "  'employ',\n",
       "  'types',\n",
       "  'regularization',\n",
       "  'training',\n",
       "  'residual',\n",
       "  'dropout',\n",
       "  'apply',\n",
       "  'dropout',\n",
       "  'output',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'added',\n",
       "  'sub'],\n",
       " ['warmup',\n",
       "  'steps',\n",
       "  'regularization',\n",
       "  'employ',\n",
       "  'types',\n",
       "  'regularization',\n",
       "  'training',\n",
       "  'residual',\n",
       "  'dropout',\n",
       "  'apply',\n",
       "  'dropout',\n",
       "  'output',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'added',\n",
       "  'sub',\n",
       "  'layer',\n",
       "  'input',\n",
       "  'normalized',\n",
       "  'addition',\n",
       "  'apply',\n",
       "  'dropout',\n",
       "  'sums',\n",
       "  'embeddings',\n",
       "  'positional',\n",
       "  'encodings',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'stacks',\n",
       "  'base',\n",
       "  'model',\n",
       "  'rate',\n",
       "  'pdrop'],\n",
       " ['table',\n",
       "  'transformer',\n",
       "  'achieves',\n",
       "  'better',\n",
       "  'bleu',\n",
       "  'scores',\n",
       "  'previous',\n",
       "  'state',\n",
       "  'art',\n",
       "  'models',\n",
       "  'english',\n",
       "  'german',\n",
       "  'english',\n",
       "  'french',\n",
       "  'newstest',\n",
       "  'tests',\n",
       "  'fraction',\n",
       "  'training',\n",
       "  'cost',\n",
       "  'model',\n",
       "  'bleu',\n",
       "  'training',\n",
       "  'cost',\n",
       "  'flops',\n",
       "  'en',\n",
       "  'de',\n",
       "  'en',\n",
       "  'fr',\n",
       "  'en',\n",
       "  'de',\n",
       "  'en',\n",
       "  'fr',\n",
       "  'bytenet',\n",
       "  'deep',\n",
       "  'att',\n",
       "  'posunk',\n",
       "  'gnmt',\n",
       "  'rl',\n",
       "  'convs',\n",
       "  'moe',\n",
       "  'deep',\n",
       "  'att',\n",
       "  'posunk',\n",
       "  'ensemble',\n",
       "  'gnmt',\n",
       "  'rl',\n",
       "  'ensemble',\n",
       "  'convs',\n",
       "  'ensemble',\n",
       "  'transformer',\n",
       "  'base',\n",
       "  'model',\n",
       "  'transformer',\n",
       "  'big',\n",
       "  'label',\n",
       "  'smoothing',\n",
       "  'training',\n",
       "  'employed',\n",
       "  'label',\n",
       "  'smoothing',\n",
       "  'value',\n",
       "  'unk',\n",
       "  'hurts',\n",
       "  'perplexity',\n",
       "  'model',\n",
       "  'learns',\n",
       "  'unsure',\n",
       "  'improves',\n",
       "  'accuracy',\n",
       "  'bleu',\n",
       "  'score',\n",
       "  'results',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'german',\n",
       "  'translation',\n",
       "  'task',\n",
       "  'big',\n",
       "  'transformer',\n",
       "  'model',\n",
       "  'transformer',\n",
       "  'big',\n",
       "  'table',\n",
       "  'outperforms',\n",
       "  'best',\n",
       "  'previously',\n",
       "  'reported',\n",
       "  'models',\n",
       "  'including',\n",
       "  'ensembles',\n",
       "  'bleu'],\n",
       " ['wmt',\n",
       "  'english',\n",
       "  'german',\n",
       "  'translation',\n",
       "  'task',\n",
       "  'big',\n",
       "  'transformer',\n",
       "  'model',\n",
       "  'transformer',\n",
       "  'big',\n",
       "  'table',\n",
       "  'outperforms',\n",
       "  'best',\n",
       "  'previously',\n",
       "  'reported',\n",
       "  'models',\n",
       "  'including',\n",
       "  'ensembles',\n",
       "  'bleu',\n",
       "  'establishing',\n",
       "  'new',\n",
       "  'state',\n",
       "  'art',\n",
       "  'bleu',\n",
       "  'score',\n",
       "  'con',\n",
       "  'guration',\n",
       "  'model',\n",
       "  'listed',\n",
       "  'line',\n",
       "  'table',\n",
       "  'training',\n",
       "  'took',\n",
       "  'days',\n",
       "  'gpus',\n",
       "  'base',\n",
       "  'model',\n",
       "  'surpasses',\n",
       "  'previously',\n",
       "  'published',\n",
       "  'models',\n",
       "  'ensembles',\n",
       "  'fraction',\n",
       "  'training',\n",
       "  'cost',\n",
       "  'competitive',\n",
       "  'models',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'french',\n",
       "  'translation',\n",
       "  'task',\n",
       "  'big',\n",
       "  'model',\n",
       "  'achieves',\n",
       "  'bleu',\n",
       "  'score',\n",
       "  'outperforming',\n",
       "  'previously',\n",
       "  'published',\n",
       "  'single',\n",
       "  'models',\n",
       "  'training',\n",
       "  'cost',\n",
       "  'previous',\n",
       "  'state',\n",
       "  'art',\n",
       "  'model',\n",
       "  'transformer',\n",
       "  'big',\n",
       "  'model',\n",
       "  'trained',\n",
       "  'english',\n",
       "  'french',\n",
       "  'dropout',\n",
       "  'rate',\n",
       "  'pdrop',\n",
       "  'instead',\n",
       "  'base',\n",
       "  'models',\n",
       "  'single',\n",
       "  'model',\n",
       "  'obtained',\n",
       "  'averaging',\n",
       "  'checkpoints',\n",
       "  'written',\n",
       "  'minute',\n",
       "  'intervals',\n",
       "  'big',\n",
       "  'models',\n",
       "  'averaged',\n",
       "  'checkpoints',\n",
       "  'beam',\n",
       "  'search',\n",
       "  'beam',\n",
       "  'size',\n",
       "  'length',\n",
       "  'penalty',\n",
       "  'hyperparameters',\n",
       "  'chosen',\n",
       "  'experimentation',\n",
       "  'development',\n",
       "  'set',\n",
       "  'set',\n",
       "  'maximum',\n",
       "  'output',\n",
       "  'length',\n",
       "  'inference',\n",
       "  'input',\n",
       "  'length',\n",
       "  'terminate',\n",
       "  'early',\n",
       "  'possible',\n",
       "  'table',\n",
       "  'summarizes',\n",
       "  'results',\n",
       "  'compares',\n",
       "  'translation',\n",
       "  'quality',\n",
       "  'training',\n",
       "  'costs',\n",
       "  'model',\n",
       "  'architectures',\n",
       "  'literature',\n",
       "  'estimate',\n",
       "  'number',\n",
       "  'oating',\n",
       "  'point',\n",
       "  'operations',\n",
       "  'train',\n",
       "  'model',\n",
       "  'multiplying',\n",
       "  'training',\n",
       "  'time',\n",
       "  'number',\n",
       "  'gpus',\n",
       "  'estimate',\n",
       "  'sustained',\n",
       "  'single',\n",
       "  'precision'],\n",
       " ['costs',\n",
       "  'model',\n",
       "  'architectures',\n",
       "  'literature',\n",
       "  'estimate',\n",
       "  'number',\n",
       "  'oating',\n",
       "  'point',\n",
       "  'operations',\n",
       "  'train',\n",
       "  'model',\n",
       "  'multiplying',\n",
       "  'training',\n",
       "  'time',\n",
       "  'number',\n",
       "  'gpus',\n",
       "  'estimate',\n",
       "  'sustained',\n",
       "  'single',\n",
       "  'precision',\n",
       "  'oating',\n",
       "  'point',\n",
       "  'capacity',\n",
       "  'gpu',\n",
       "  'model',\n",
       "  'variations',\n",
       "  'evaluate',\n",
       "  'importance',\n",
       "  'different',\n",
       "  'components',\n",
       "  'transformer',\n",
       "  'varied',\n",
       "  'base',\n",
       "  'model',\n",
       "  'different',\n",
       "  'ways',\n",
       "  'measuring',\n",
       "  'change',\n",
       "  'performance',\n",
       "  'english',\n",
       "  'german',\n",
       "  'translation',\n",
       "  'development',\n",
       "  'set',\n",
       "  'newstest',\n",
       "  'beam',\n",
       "  'search',\n",
       "  'described',\n",
       "  'previous',\n",
       "  'section',\n",
       "  'checkpoint',\n",
       "  'averaging',\n",
       "  'present',\n",
       "  'results',\n",
       "  'table',\n",
       "  'table',\n",
       "  'rows',\n",
       "  'vary',\n",
       "  'number',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'attention',\n",
       "  'key',\n",
       "  'value',\n",
       "  'dimensions',\n",
       "  'keeping',\n",
       "  'computation',\n",
       "  'constant',\n",
       "  'described',\n",
       "  'section',\n",
       "  'single',\n",
       "  'head',\n",
       "  'attention',\n",
       "  'bleu',\n",
       "  'worse',\n",
       "  'best',\n",
       "  'setting',\n",
       "  'quality',\n",
       "  'drops',\n",
       "  'heads',\n",
       "  'values',\n",
       "  'tflops',\n",
       "  'respectively'],\n",
       " ['table',\n",
       "  'variations',\n",
       "  'transformer',\n",
       "  'architecture',\n",
       "  'unlisted',\n",
       "  'values',\n",
       "  'identical',\n",
       "  'base',\n",
       "  'model',\n",
       "  'metrics',\n",
       "  'english',\n",
       "  'german',\n",
       "  'translation',\n",
       "  'development',\n",
       "  'set',\n",
       "  'newstest',\n",
       "  'listed',\n",
       "  'perplexities',\n",
       "  'wordpiece',\n",
       "  'according',\n",
       "  'byte',\n",
       "  'pair',\n",
       "  'encoding',\n",
       "  'compared',\n",
       "  'word',\n",
       "  'perplexities',\n",
       "  'dmodel',\n",
       "  'dff',\n",
       "  'dk',\n",
       "  'dv',\n",
       "  'pdrop',\n",
       "  'unk',\n",
       "  'train',\n",
       "  'ppl',\n",
       "  'bleu',\n",
       "  'params',\n",
       "  'steps',\n",
       "  'dev',\n",
       "  'dev',\n",
       "  'base',\n",
       "  'positional',\n",
       "  'embedding',\n",
       "  'instead',\n",
       "  'sinusoids',\n",
       "  'big',\n",
       "  'table',\n",
       "  'transformer',\n",
       "  'generalizes',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'results',\n",
       "  'section',\n",
       "  'wsj',\n",
       "  'parser',\n",
       "  'training',\n",
       "  'wsj',\n",
       "  'vinyals',\n",
       "  'kaiser',\n",
       "  'el',\n",
       "  'wsj'],\n",
       " ['transformer',\n",
       "  'generalizes',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'results',\n",
       "  'section',\n",
       "  'wsj',\n",
       "  'parser',\n",
       "  'training',\n",
       "  'wsj',\n",
       "  'vinyals',\n",
       "  'kaiser',\n",
       "  'el',\n",
       "  'wsj',\n",
       "  'discriminative',\n",
       "  'petrov',\n",
       "  'wsj',\n",
       "  'discriminative',\n",
       "  'zhu',\n",
       "  'wsj',\n",
       "  'discriminative',\n",
       "  'dyer',\n",
       "  'wsj',\n",
       "  'discriminative',\n",
       "  'transformer',\n",
       "  'layers',\n",
       "  'wsj',\n",
       "  'discriminative',\n",
       "  'zhu',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'huang',\n",
       "  'harper',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'mcclosky',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'vinyals',\n",
       "  'kaiser',\n",
       "  'el',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'transformer',\n",
       "  'layers',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'luong',\n",
       "  'multi',\n",
       "  'task',\n",
       "  'dyer',\n",
       "  'generative',\n",
       "  'table',\n",
       "  'rows',\n",
       "  'observe',\n",
       "  'reducing',\n",
       "  'attention',\n",
       "  'key',\n",
       "  'size',\n",
       "  'dk',\n",
       "  'hurts',\n",
       "  'model',\n",
       "  'quality',\n",
       "  'suggests',\n",
       "  'determining',\n",
       "  'compatibility',\n",
       "  'easy',\n",
       "  'sophisticated',\n",
       "  'compatibility',\n",
       "  'function',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'bene',\n",
       "  'cial',\n",
       "  'observe',\n",
       "  'rows',\n",
       "  'expected',\n",
       "  'bigger',\n",
       "  'models',\n",
       "  'better',\n",
       "  'dropout',\n",
       "  'helpful',\n",
       "  'avoiding',\n",
       "  'tting',\n",
       "  'row',\n",
       "  'replace',\n",
       "  'sinusoidal',\n",
       "  'positional',\n",
       "  'encoding',\n",
       "  'learned',\n",
       "  'positional',\n",
       "  'embeddings',\n",
       "  'observe',\n",
       "  'nearly',\n",
       "  'identical',\n",
       "  'results',\n",
       "  'base',\n",
       "  'model',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'evaluate'],\n",
       " ['tting',\n",
       "  'row',\n",
       "  'replace',\n",
       "  'sinusoidal',\n",
       "  'positional',\n",
       "  'encoding',\n",
       "  'learned',\n",
       "  'positional',\n",
       "  'embeddings',\n",
       "  'observe',\n",
       "  'nearly',\n",
       "  'identical',\n",
       "  'results',\n",
       "  'base',\n",
       "  'model',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'evaluate',\n",
       "  'transformer',\n",
       "  'generalize',\n",
       "  'tasks',\n",
       "  'performed',\n",
       "  'experiments',\n",
       "  'english',\n",
       "  'constituency',\n",
       "  'parsing',\n",
       "  'task',\n",
       "  'presents',\n",
       "  'speci',\n",
       "  'challenges',\n",
       "  'output',\n",
       "  'strong',\n",
       "  'structural'],\n",
       " ['constraints',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'longer',\n",
       "  'input',\n",
       "  'furthermore',\n",
       "  'rnn',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'models',\n",
       "  'able',\n",
       "  'attain',\n",
       "  'state',\n",
       "  'art',\n",
       "  'results',\n",
       "  'small',\n",
       "  'data',\n",
       "  'regimes',\n",
       "  'trained',\n",
       "  'layer',\n",
       "  'transformer',\n",
       "  'dmodel',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'journal',\n",
       "  'wsj',\n",
       "  'portion',\n",
       "  'penn',\n",
       "  'treebank',\n",
       "  'training',\n",
       "  'sentences',\n",
       "  'trained',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'setting',\n",
       "  'larger',\n",
       "  'high',\n",
       "  'con',\n",
       "  'dence',\n",
       "  'berkleyparser',\n",
       "  'corpora',\n",
       "  'approximately',\n",
       "  'sentences',\n",
       "  'vocabulary',\n",
       "  'tokens',\n",
       "  'wsj',\n",
       "  'setting',\n",
       "  'vocabulary',\n",
       "  'tokens',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'setting',\n",
       "  'performed',\n",
       "  'small',\n",
       "  'number',\n",
       "  'experiments',\n",
       "  'select',\n",
       "  'dropout',\n",
       "  'attention',\n",
       "  'residual',\n",
       "  'section',\n",
       "  'learning',\n",
       "  'rates',\n",
       "  'beam',\n",
       "  'size',\n",
       "  'section',\n",
       "  'development',\n",
       "  'set',\n",
       "  'parameters',\n",
       "  'remained',\n",
       "  'unchanged',\n",
       "  'english',\n",
       "  'german',\n",
       "  'base',\n",
       "  'translation',\n",
       "  'model',\n",
       "  'inference',\n",
       "  'increased',\n",
       "  'maximum',\n",
       "  'output',\n",
       "  'length',\n",
       "  'input',\n",
       "  'length',\n",
       "  'beam',\n",
       "  'size',\n",
       "  'wsj',\n",
       "  'semi',\n",
       "  'supervised',\n",
       "  'setting',\n",
       "  'results',\n",
       "  'table',\n",
       "  'despite',\n",
       "  'lack',\n",
       "  'task',\n",
       "  'speci',\n",
       "  'tuning',\n",
       "  'model',\n",
       "  'performs',\n",
       "  'sur',\n",
       "  'prisingly',\n",
       "  'yielding',\n",
       "  'better',\n",
       "  'results',\n",
       "  'previously',\n",
       "  'reported',\n",
       "  'models',\n",
       "  'exception',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'grammar',\n",
       "  'contrast',\n",
       "  'rnn',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'models',\n",
       "  'transformer',\n",
       "  'outperforms',\n",
       "  'berkeley',\n",
       "  'parser',\n",
       "  'training',\n",
       "  'wsj',\n",
       "  'training',\n",
       "  'set',\n",
       "  'sentences',\n",
       "  'conclusion',\n",
       "  'work',\n",
       "  'presented',\n",
       "  'transformer',\n",
       "  'rst',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'model',\n",
       "  'based',\n",
       "  'entirely',\n",
       "  'attention',\n",
       "  'replacing',\n",
       "  'recurrent',\n",
       "  'layers',\n",
       "  'commonly',\n",
       "  'encoder',\n",
       "  'decoder'],\n",
       " ['wsj',\n",
       "  'training',\n",
       "  'set',\n",
       "  'sentences',\n",
       "  'conclusion',\n",
       "  'work',\n",
       "  'presented',\n",
       "  'transformer',\n",
       "  'rst',\n",
       "  'sequence',\n",
       "  'transduction',\n",
       "  'model',\n",
       "  'based',\n",
       "  'entirely',\n",
       "  'attention',\n",
       "  'replacing',\n",
       "  'recurrent',\n",
       "  'layers',\n",
       "  'commonly',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'architectures',\n",
       "  'multi',\n",
       "  'headed',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'translation',\n",
       "  'tasks',\n",
       "  'transformer',\n",
       "  'trained',\n",
       "  'signi',\n",
       "  'cantly',\n",
       "  'faster',\n",
       "  'architectures',\n",
       "  'based',\n",
       "  'recurrent',\n",
       "  'convolutional',\n",
       "  'layers',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'german',\n",
       "  'wmt',\n",
       "  'english',\n",
       "  'french',\n",
       "  'translation',\n",
       "  'tasks',\n",
       "  'achieve',\n",
       "  'new',\n",
       "  'state',\n",
       "  'art',\n",
       "  'task',\n",
       "  'best',\n",
       "  'model',\n",
       "  'outperforms',\n",
       "  'previously',\n",
       "  'reported',\n",
       "  'ensembles',\n",
       "  'excited',\n",
       "  'future',\n",
       "  'attention',\n",
       "  'based',\n",
       "  'models',\n",
       "  'plan',\n",
       "  'apply',\n",
       "  'tasks',\n",
       "  'plan',\n",
       "  'extend',\n",
       "  'transformer',\n",
       "  'problems',\n",
       "  'involving',\n",
       "  'input',\n",
       "  'output',\n",
       "  'modalities',\n",
       "  'text',\n",
       "  'investigate',\n",
       "  'local',\n",
       "  'restricted',\n",
       "  'attention',\n",
       "  'mechanisms',\n",
       "  'ef',\n",
       "  'ciently',\n",
       "  'handle',\n",
       "  'large',\n",
       "  'inputs',\n",
       "  'outputs',\n",
       "  'images',\n",
       "  'audio',\n",
       "  'video',\n",
       "  'making',\n",
       "  'generation',\n",
       "  'sequential',\n",
       "  'research',\n",
       "  'goals',\n",
       "  'code',\n",
       "  'train',\n",
       "  'evaluate',\n",
       "  'models',\n",
       "  'available',\n",
       "  'https',\n",
       "  'github',\n",
       "  'com',\n",
       "  'tensorflow',\n",
       "  'tensor',\n",
       "  'tensor',\n",
       "  'grateful',\n",
       "  'nal',\n",
       "  'kalchbrenner',\n",
       "  'stephan',\n",
       "  'gouws',\n",
       "  'fruitful',\n",
       "  'comments',\n",
       "  'corrections',\n",
       "  'inspiration',\n",
       "  'references',\n",
       "  'jimmy',\n",
       "  'lei',\n",
       "  'ba',\n",
       "  'jamie',\n",
       "  'ryan',\n",
       "  'kiros',\n",
       "  'geoffrey',\n",
       "  'hinton',\n",
       "  'layer',\n",
       "  'normalization',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'dzmitry',\n",
       "  'bahdanau',\n",
       "  'kyunghyun',\n",
       "  'cho',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'jointly',\n",
       "  'learning',\n",
       "  'align',\n",
       "  'translate',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'denny',\n",
       "  'britz',\n",
       "  'anna',\n",
       "  'goldie',\n",
       "  'minh',\n",
       "  'thang',\n",
       "  'luong'],\n",
       " ['yoshua',\n",
       "  'bengio',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'jointly',\n",
       "  'learning',\n",
       "  'align',\n",
       "  'translate',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'denny',\n",
       "  'britz',\n",
       "  'anna',\n",
       "  'goldie',\n",
       "  'minh',\n",
       "  'thang',\n",
       "  'luong',\n",
       "  'quoc',\n",
       "  'le',\n",
       "  'massive',\n",
       "  'exploration',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'architectures',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'jianpeng',\n",
       "  'cheng',\n",
       "  'li',\n",
       "  'dong',\n",
       "  'mirella',\n",
       "  'lapata',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'machine',\n",
       "  'reading',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'kyunghyun',\n",
       "  'cho',\n",
       "  'bart',\n",
       "  'van',\n",
       "  'merrienboer',\n",
       "  'caglar',\n",
       "  'gulcehre',\n",
       "  'fethi',\n",
       "  'bougares',\n",
       "  'holger',\n",
       "  'schwenk',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'learning',\n",
       "  'phrase',\n",
       "  'representations',\n",
       "  'rnn',\n",
       "  'encoder',\n",
       "  'decoder',\n",
       "  'statistical',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'francois',\n",
       "  'chollet',\n",
       "  'xception',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'depthwise',\n",
       "  'separable',\n",
       "  'convolutions',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv'],\n",
       " ['junyoung',\n",
       "  'chung',\n",
       "  'caglar',\n",
       "  'gulcehre',\n",
       "  'kyunghyun',\n",
       "  'cho',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'empirical',\n",
       "  'evaluation',\n",
       "  'gated',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'sequence',\n",
       "  'modeling',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'chris',\n",
       "  'dyer',\n",
       "  'adhiguna',\n",
       "  'kuncoro',\n",
       "  'miguel',\n",
       "  'ballesteros',\n",
       "  'noah',\n",
       "  'smith',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'grammars',\n",
       "  'proc',\n",
       "  'naacl',\n",
       "  'jonas',\n",
       "  'gehring',\n",
       "  'michael',\n",
       "  'auli',\n",
       "  'david',\n",
       "  'grangier',\n",
       "  'denis',\n",
       "  'yarats',\n",
       "  'yann',\n",
       "  'dauphin',\n",
       "  'convolu',\n",
       "  'tional',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'learning',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'alex',\n",
       "  'graves',\n",
       "  'generating',\n",
       "  'sequences',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'kaiming',\n",
       "  'xiangyu',\n",
       "  'zhang',\n",
       "  'shaoqing',\n",
       "  'ren',\n",
       "  'jian',\n",
       "  'sun',\n",
       "  'deep',\n",
       "  'residual',\n",
       "  'learning',\n",
       "  'im',\n",
       "  'age',\n",
       "  'recognition',\n",
       "  'proceedings',\n",
       "  'ieee',\n",
       "  'conference',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'pattern',\n",
       "  'recognition',\n",
       "  'pages',\n",
       "  'sepp',\n",
       "  'hochreiter',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'paolo',\n",
       "  'frasconi',\n",
       "  'jurgen',\n",
       "  'schmidhuber',\n",
       "  'gradient',\n",
       "  'ow',\n",
       "  'recurrent',\n",
       "  'nets',\n",
       "  'dif',\n",
       "  'culty',\n",
       "  'learning',\n",
       "  'long',\n",
       "  'term',\n",
       "  'dependencies',\n",
       "  'sepp',\n",
       "  'hochreiter',\n",
       "  'jurgen',\n",
       "  'schmidhuber',\n",
       "  'long',\n",
       "  'short',\n",
       "  'term',\n",
       "  'memory',\n",
       "  'neural',\n",
       "  'computation',\n",
       "  'zhongqiang',\n",
       "  'huang',\n",
       "  'mary',\n",
       "  'harper',\n",
       "  'self',\n",
       "  'training',\n",
       "  'pcfg',\n",
       "  'grammars',\n",
       "  'latent',\n",
       "  'annotations',\n",
       "  'languages',\n",
       "  'proceedings',\n",
       "  'conference',\n",
       "  'empirical',\n",
       "  'methods',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'pages',\n",
       "  'acl',\n",
       "  'august'],\n",
       " ['mary',\n",
       "  'harper',\n",
       "  'self',\n",
       "  'training',\n",
       "  'pcfg',\n",
       "  'grammars',\n",
       "  'latent',\n",
       "  'annotations',\n",
       "  'languages',\n",
       "  'proceedings',\n",
       "  'conference',\n",
       "  'empirical',\n",
       "  'methods',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'pages',\n",
       "  'acl',\n",
       "  'august',\n",
       "  'rafal',\n",
       "  'jozefowicz',\n",
       "  'oriol',\n",
       "  'vinyals',\n",
       "  'mike',\n",
       "  'schuster',\n",
       "  'noam',\n",
       "  'shazeer',\n",
       "  'yonghui',\n",
       "  'wu',\n",
       "  'exploring',\n",
       "  'limits',\n",
       "  'language',\n",
       "  'modeling',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'ukasz',\n",
       "  'kaiser',\n",
       "  'samy',\n",
       "  'bengio',\n",
       "  'active',\n",
       "  'memory',\n",
       "  'replace',\n",
       "  'attention',\n",
       "  'advances',\n",
       "  'neural',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'systems',\n",
       "  'nips',\n",
       "  'ukasz',\n",
       "  'kaiser',\n",
       "  'ilya',\n",
       "  'sutskever',\n",
       "  'neural',\n",
       "  'gpus',\n",
       "  'learn',\n",
       "  'algorithms',\n",
       "  'international',\n",
       "  'conference',\n",
       "  'learning',\n",
       "  'representations',\n",
       "  'iclr',\n",
       "  'nal',\n",
       "  'kalchbrenner',\n",
       "  'lasse',\n",
       "  'espeholt',\n",
       "  'karen',\n",
       "  'simonyan',\n",
       "  'aaron',\n",
       "  'van',\n",
       "  'den',\n",
       "  'oord',\n",
       "  'alex',\n",
       "  'graves',\n",
       "  'ko',\n",
       "  'ray',\n",
       "  'kavukcuoglu',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'linear',\n",
       "  'time',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'yoon',\n",
       "  'kim',\n",
       "  'carl',\n",
       "  'denton',\n",
       "  'luong',\n",
       "  'hoang',\n",
       "  'alexander',\n",
       "  'rush',\n",
       "  'structured',\n",
       "  'attention',\n",
       "  'networks',\n",
       "  'international',\n",
       "  'conference',\n",
       "  'learning',\n",
       "  'representations',\n",
       "  'diederik',\n",
       "  'kingma',\n",
       "  'jimmy',\n",
       "  'ba',\n",
       "  'adam',\n",
       "  'method',\n",
       "  'stochastic',\n",
       "  'optimization',\n",
       "  'iclr',\n",
       "  'oleksii',\n",
       "  'kuchaiev',\n",
       "  'boris',\n",
       "  'ginsburg',\n",
       "  'factorization',\n",
       "  'tricks',\n",
       "  'lstm',\n",
       "  'networks',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'zhouhan',\n",
       "  'lin',\n",
       "  'minwei',\n",
       "  'feng',\n",
       "  'cicero',\n",
       "  'nogueira',\n",
       "  'dos',\n",
       "  'santos',\n",
       "  'mo',\n",
       "  'yu',\n",
       "  'bing',\n",
       "  'xiang',\n",
       "  'bowen',\n",
       "  'zhou',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'structured',\n",
       "  'self',\n",
       "  'attentive',\n",
       "  'sentence',\n",
       "  'embedding'],\n",
       " ['zhouhan',\n",
       "  'lin',\n",
       "  'minwei',\n",
       "  'feng',\n",
       "  'cicero',\n",
       "  'nogueira',\n",
       "  'dos',\n",
       "  'santos',\n",
       "  'mo',\n",
       "  'yu',\n",
       "  'bing',\n",
       "  'xiang',\n",
       "  'bowen',\n",
       "  'zhou',\n",
       "  'yoshua',\n",
       "  'bengio',\n",
       "  'structured',\n",
       "  'self',\n",
       "  'attentive',\n",
       "  'sentence',\n",
       "  'embedding',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'minh',\n",
       "  'thang',\n",
       "  'luong',\n",
       "  'quoc',\n",
       "  'le',\n",
       "  'ilya',\n",
       "  'sutskever',\n",
       "  'oriol',\n",
       "  'vinyals',\n",
       "  'lukasz',\n",
       "  'kaiser',\n",
       "  'multi',\n",
       "  'task',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'learning',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'minh',\n",
       "  'thang',\n",
       "  'luong',\n",
       "  'hieu',\n",
       "  'pham',\n",
       "  'christopher',\n",
       "  'manning',\n",
       "  'effective',\n",
       "  'approaches',\n",
       "  'attention',\n",
       "  'based',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'mitchell',\n",
       "  'marcus',\n",
       "  'mary',\n",
       "  'ann',\n",
       "  'marcinkiewicz',\n",
       "  'beatrice',\n",
       "  'santorini',\n",
       "  'building',\n",
       "  'large',\n",
       "  'annotated',\n",
       "  'corpus',\n",
       "  'english',\n",
       "  'penn',\n",
       "  'treebank',\n",
       "  'computational',\n",
       "  'linguistics',\n",
       "  'david',\n",
       "  'mcclosky',\n",
       "  'eugene',\n",
       "  'charniak',\n",
       "  'mark',\n",
       "  'johnson',\n",
       "  'effective',\n",
       "  'self',\n",
       "  'training',\n",
       "  'parsing',\n",
       "  'proceedings',\n",
       "  'human',\n",
       "  'language',\n",
       "  'technology',\n",
       "  'conference',\n",
       "  'naacl',\n",
       "  'main',\n",
       "  'conference',\n",
       "  'pages',\n",
       "  'acl',\n",
       "  'june'],\n",
       " ['ankur',\n",
       "  'parikh',\n",
       "  'oscar',\n",
       "  'tackstrom',\n",
       "  'dipanjan',\n",
       "  'das',\n",
       "  'jakob',\n",
       "  'uszkoreit',\n",
       "  'decomposable',\n",
       "  'attention',\n",
       "  'model',\n",
       "  'empirical',\n",
       "  'methods',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'romain',\n",
       "  'paulus',\n",
       "  'caiming',\n",
       "  'xiong',\n",
       "  'richard',\n",
       "  'socher',\n",
       "  'deep',\n",
       "  'reinforced',\n",
       "  'model',\n",
       "  'abstractive',\n",
       "  'summarization',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'slav',\n",
       "  'petrov',\n",
       "  'leon',\n",
       "  'barrett',\n",
       "  'romain',\n",
       "  'thibaux',\n",
       "  'dan',\n",
       "  'klein',\n",
       "  'learning',\n",
       "  'accurate',\n",
       "  'compact',\n",
       "  'interpretable',\n",
       "  'tree',\n",
       "  'annotation',\n",
       "  'proceedings',\n",
       "  'st',\n",
       "  'international',\n",
       "  'conference',\n",
       "  'computational',\n",
       "  'linguistics',\n",
       "  'th',\n",
       "  'annual',\n",
       "  'meeting',\n",
       "  'acl',\n",
       "  'pages',\n",
       "  'acl',\n",
       "  'july',\n",
       "  'press',\n",
       "  'lior',\n",
       "  'wolf',\n",
       "  'output',\n",
       "  'embedding',\n",
       "  'improve',\n",
       "  'language',\n",
       "  'models',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'rico',\n",
       "  'sennrich',\n",
       "  'barry',\n",
       "  'haddow',\n",
       "  'alexandra',\n",
       "  'birch',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'rare',\n",
       "  'words',\n",
       "  'subword',\n",
       "  'units',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'noam',\n",
       "  'shazeer',\n",
       "  'azalia',\n",
       "  'mirhoseini',\n",
       "  'krzysztof',\n",
       "  'maziarz',\n",
       "  'andy',\n",
       "  'davis',\n",
       "  'quoc',\n",
       "  'le',\n",
       "  'geoffrey',\n",
       "  'hinton',\n",
       "  'jeff',\n",
       "  'dean',\n",
       "  'outrageously',\n",
       "  'large',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'sparsely',\n",
       "  'gated',\n",
       "  'mixture',\n",
       "  'experts',\n",
       "  'layer',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'nitish',\n",
       "  'srivastava',\n",
       "  'geoffrey',\n",
       "  'hinton',\n",
       "  'alex',\n",
       "  'krizhevsky',\n",
       "  'ilya',\n",
       "  'sutskever',\n",
       "  'ruslan',\n",
       "  'salakhutdi',\n",
       "  'nov',\n",
       "  'dropout',\n",
       "  'simple',\n",
       "  'way',\n",
       "  'prevent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'tting',\n",
       "  'journal',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'research'],\n",
       " ['alex',\n",
       "  'krizhevsky',\n",
       "  'ilya',\n",
       "  'sutskever',\n",
       "  'ruslan',\n",
       "  'salakhutdi',\n",
       "  'nov',\n",
       "  'dropout',\n",
       "  'simple',\n",
       "  'way',\n",
       "  'prevent',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'tting',\n",
       "  'journal',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'research',\n",
       "  'sainbayar',\n",
       "  'sukhbaatar',\n",
       "  'arthur',\n",
       "  'szlam',\n",
       "  'jason',\n",
       "  'weston',\n",
       "  'rob',\n",
       "  'fergus',\n",
       "  'end',\n",
       "  'end',\n",
       "  'memory',\n",
       "  'networks',\n",
       "  'cortes',\n",
       "  'lawrence',\n",
       "  'lee',\n",
       "  'sugiyama',\n",
       "  'garnett',\n",
       "  'editors',\n",
       "  'advances',\n",
       "  'neural',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'systems',\n",
       "  'pages',\n",
       "  'curran',\n",
       "  'associates',\n",
       "  'inc',\n",
       "  'ilya',\n",
       "  'sutskever',\n",
       "  'oriol',\n",
       "  'vinyals',\n",
       "  'quoc',\n",
       "  'vv',\n",
       "  'le',\n",
       "  'sequence',\n",
       "  'sequence',\n",
       "  'learning',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'advances',\n",
       "  'neural',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'systems',\n",
       "  'pages',\n",
       "  'christian',\n",
       "  'szegedy',\n",
       "  'vincent',\n",
       "  'vanhoucke',\n",
       "  'sergey',\n",
       "  'ioffe',\n",
       "  'jonathon',\n",
       "  'shlens',\n",
       "  'zbigniew',\n",
       "  'wojna',\n",
       "  'rethinking',\n",
       "  'inception',\n",
       "  'architecture',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'vinyals',\n",
       "  'kaiser',\n",
       "  'koo',\n",
       "  'petrov',\n",
       "  'sutskever',\n",
       "  'hinton',\n",
       "  'grammar',\n",
       "  'foreign',\n",
       "  'language',\n",
       "  'advances',\n",
       "  'neural',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'systems',\n",
       "  'yonghui',\n",
       "  'wu',\n",
       "  'mike',\n",
       "  'schuster',\n",
       "  'zhifeng',\n",
       "  'chen',\n",
       "  'quoc',\n",
       "  'le',\n",
       "  'mohammad',\n",
       "  'norouzi',\n",
       "  'wolfgang',\n",
       "  'macherey',\n",
       "  'maxim',\n",
       "  'krikun',\n",
       "  'yuan',\n",
       "  'cao',\n",
       "  'qin',\n",
       "  'gao',\n",
       "  'klaus',\n",
       "  'macherey',\n",
       "  'google',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'system',\n",
       "  'bridging',\n",
       "  'gap',\n",
       "  'human',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'jie',\n",
       "  'zhou',\n",
       "  'ying',\n",
       "  'cao'],\n",
       " ['google',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'system',\n",
       "  'bridging',\n",
       "  'gap',\n",
       "  'human',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'arxiv',\n",
       "  'preprint',\n",
       "  'arxiv',\n",
       "  'jie',\n",
       "  'zhou',\n",
       "  'ying',\n",
       "  'cao',\n",
       "  'xuguang',\n",
       "  'wang',\n",
       "  'peng',\n",
       "  'li',\n",
       "  'wei',\n",
       "  'xu',\n",
       "  'deep',\n",
       "  'recurrent',\n",
       "  'models',\n",
       "  'fast',\n",
       "  'forward',\n",
       "  'connections',\n",
       "  'neural',\n",
       "  'machine',\n",
       "  'translation',\n",
       "  'corr',\n",
       "  'abs',\n",
       "  'muhua',\n",
       "  'zhu',\n",
       "  'yue',\n",
       "  'zhang',\n",
       "  'wenliang',\n",
       "  'chen',\n",
       "  'min',\n",
       "  'zhang',\n",
       "  'jingbo',\n",
       "  'zhu',\n",
       "  'fast',\n",
       "  'accurate',\n",
       "  'shift',\n",
       "  'reduce',\n",
       "  'constituent',\n",
       "  'parsing',\n",
       "  'proceedings',\n",
       "  'st',\n",
       "  'annual',\n",
       "  'meeting',\n",
       "  'acl',\n",
       "  'volume',\n",
       "  'long',\n",
       "  'papers',\n",
       "  'pages',\n",
       "  'acl',\n",
       "  'august'],\n",
       " ['attention',\n",
       "  'visualizations',\n",
       "  'input',\n",
       "  'input',\n",
       "  'layer',\n",
       "  'spirit',\n",
       "  'majority',\n",
       "  'american',\n",
       "  'governments',\n",
       "  'passed',\n",
       "  'new',\n",
       "  'laws',\n",
       "  'making',\n",
       "  'registration',\n",
       "  'voting',\n",
       "  'process',\n",
       "  'difficult',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'spirit',\n",
       "  'majority',\n",
       "  'american',\n",
       "  'governments',\n",
       "  'passed',\n",
       "  'new',\n",
       "  'laws',\n",
       "  'making',\n",
       "  'registration',\n",
       "  'voting',\n",
       "  'process',\n",
       "  'difficult',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'pad',\n",
       "  'figure',\n",
       "  'example',\n",
       "  'attention',\n",
       "  'mechanism',\n",
       "  'following',\n",
       "  'long',\n",
       "  'distance',\n",
       "  'dependencies',\n",
       "  'encoder',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'attend',\n",
       "  'distant',\n",
       "  'dependency',\n",
       "  'verb',\n",
       "  'making',\n",
       "  'completing',\n",
       "  'phrase',\n",
       "  'making',\n",
       "  'dif',\n",
       "  'cult',\n",
       "  'attentions',\n",
       "  'shown',\n",
       "  'word',\n",
       "  'making',\n",
       "  'different',\n",
       "  'colors',\n",
       "  'represent',\n",
       "  'different',\n",
       "  'heads',\n",
       "  'best',\n",
       "  'viewed',\n",
       "  'color'],\n",
       " ['input',\n",
       "  'input',\n",
       "  'layer',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'input',\n",
       "  'input',\n",
       "  'layer',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'figure',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'layer',\n",
       "  'apparently',\n",
       "  'involved',\n",
       "  'anaphora',\n",
       "  'resolution',\n",
       "  'attentions',\n",
       "  'head',\n",
       "  'isolated',\n",
       "  'attentions',\n",
       "  'word',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'note',\n",
       "  'attentions',\n",
       "  'sharp',\n",
       "  'word'],\n",
       " ['input',\n",
       "  'input',\n",
       "  'layer',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'input',\n",
       "  'input',\n",
       "  'layer',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'law',\n",
       "  'perfect',\n",
       "  'application',\n",
       "  'missing',\n",
       "  'opinion',\n",
       "  'pad',\n",
       "  'figure',\n",
       "  'attention',\n",
       "  'heads',\n",
       "  'exhibit',\n",
       "  'behaviour',\n",
       "  'related',\n",
       "  'structure',\n",
       "  'sentence',\n",
       "  'examples',\n",
       "  'different',\n",
       "  'heads',\n",
       "  'encoder',\n",
       "  'self',\n",
       "  'attention',\n",
       "  'layer',\n",
       "  'heads',\n",
       "  'clearly',\n",
       "  'learned',\n",
       "  'perform',\n",
       "  'different',\n",
       "  'tasks']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "dictionary = corpora.Dictionary(words)\n",
    "corpus = [dictionary.doc2bow(text) for text in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('google', 0.5669326680520711),\n",
      " ('com', 0.43503834052287116),\n",
      " ('brain', 0.1864450030812305),\n",
      " ('polosukhin', 0.15410455943296217),\n",
      " ('toronto', 0.15410455943296217),\n",
      " ('bleu', 0.12733917080866194),\n",
      " ('research', 0.12733917080866194),\n",
      " ('aidan', 0.12429666872082032),\n",
      " ('illia', 0.12429666872082032),\n",
      " ('llion', 0.12429666872082032)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# print top 10 words of tfidf\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint([(dictionary[w], s) for w , s in sorted(doc, key=lambda x: x[1], reverse=True)[:10]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"model\" + 0.013*\"arxiv\" + 0.013*\"attention\" + 0.010*\"models\" + '\n",
      "  '0.009*\"training\" + 0.009*\"translation\" + 0.009*\"neural\" + '\n",
      "  '0.009*\"transformer\" + 0.008*\"english\" + 0.008*\"sequence\"'),\n",
      " (1,\n",
      "  '0.032*\"attention\" + 0.014*\"layer\" + 0.011*\"decoder\" + 0.011*\"encoder\" + '\n",
      "  '0.011*\"sequence\" + 0.011*\"output\" + 0.010*\"layers\" + 0.010*\"model\" + '\n",
      "  '0.010*\"self\" + 0.010*\"models\"'),\n",
      " (2,\n",
      "  '0.015*\"attention\" + 0.014*\"input\" + 0.013*\"layer\" + 0.011*\"pad\" + '\n",
      "  '0.010*\"model\" + 0.008*\"models\" + 0.008*\"opinion\" + 0.008*\"sequence\" + '\n",
      "  '0.008*\"perfect\" + 0.008*\"missing\"'),\n",
      " (3,\n",
      "  '0.014*\"attention\" + 0.010*\"output\" + 0.009*\"sequence\" + 0.008*\"layer\" + '\n",
      "  '0.007*\"self\" + 0.007*\"function\" + 0.007*\"values\" + 0.006*\"neural\" + '\n",
      "  '0.006*\"models\" + 0.006*\"model\"'),\n",
      " (4,\n",
      "  '0.014*\"attention\" + 0.012*\"model\" + 0.009*\"training\" + 0.008*\"transformer\" '\n",
      "  '+ 0.007*\"english\" + 0.006*\"models\" + 0.006*\"neural\" + 0.006*\"arxiv\" + '\n",
      "  '0.006*\"dot\" + 0.006*\"translation\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lda_model[i] for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = [\" \".join([dictionary[i] for i, _ in lda_model.get_topic_terms(j)]) for j in range(num_topics)]\n",
    "smart_query = \" \".join(topic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search_with_score(\"Explain the causal attention with images in a blog post\", k=5)\n",
    "# Generate quiz blog post questions pretty image margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search_with_score(smart_query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.similarity_search_with_score(smart_query, k=5)[0][0].page_content in text_list_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger(lda_vec1, lda_vec2, lda_model):\n",
    "    dense1 = gensim.matutils.sparse2full(lda_vec1, lda_model.num_topics)\n",
    "    dense2 = gensim.matutils.sparse2full(lda_vec2, lda_model.num_topics)\n",
    "    return np.sqrt(0.5 * ((np.sqrt(dense1) - np.sqrt(dense2))**2).sum())\n",
    "\n",
    "def get_most_similar_documents(processed_query, corpus, lda_model):\n",
    "    query_bow = dictionary.doc2bow(processed_query)\n",
    "    query_lda = lda_model[query_bow]\n",
    "    # sort according to hellinger distance\n",
    "    dists = np.array([hellinger(query_lda, lda_model[corpus[i]], lda_model) for i in range(len(corpus))])\n",
    "    idx = np.argsort(dists)[:5]\n",
    "    return idx, dists[idx]\n",
    "\n",
    "def process_query(query):\n",
    "    query = process_data([query])[0]\n",
    "    query = gensim.utils.simple_preprocess(str(query), deacc=True)\n",
    "    query = [word for word in query if word not in stopwords]\n",
    "    return query\n",
    "\n",
    "def process_and_rank(query, corpus, lda_model):\n",
    "    processed_query = process_query(query)\n",
    "    print(processed_query)\n",
    "    return get_most_similar_documents(processed_query, corpus, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explain', 'transformer', 'model', 'works']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(11,\n",
       "   'usual learned linear transfor mation and softmax function to convert the decoder output to predicted next token probabilities in our model, we share the same weight matrix between the two embedding layers and the pre softmax linear transformation, similar to 30 in the embedding layers, we multiply those weights by dmodel 3 5 positional encoding since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the 5'),\n",
       "  (32,\n",
       "   'et al google s neural machine translation system bridging the gap between human and machine translation arxiv preprint arxiv 1609 08144, 2016 39 jie zhou, ying cao, xuguang wang, peng li, and wei xu deep recurrent models with fast forward connections for neural machine translation corr, abs 1606 04199, 2016 40 muhua zhu, yue zhang, wenliang chen, min zhang, and jingbo zhu fast and accurate shift reduce constituent parsing in proceedings of the 51st annual meeting of the acl volume 1 long papers , pages 434 443 acl, august 2013 12'),\n",
       "  (26,\n",
       "   'and yoshua bengio neural machine translation by jointly learning to align and translate corr, abs 1409 0473, 2014 3 denny britz, anna goldie, minh thang luong, and quoc v le massive exploration of neural machine translation architectures corr, abs 1703 03906, 2017 4 jianpeng cheng, li dong, and mirella lapata long short term memory networks for machine reading arxiv preprint arxiv 1601 06733, 2016 5 kyunghyun cho, bart van merrienboer, caglar gulcehre, fethi bougares, holger schwenk, and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation corr, abs 1406 1078, 2014 6 francois chollet xception deep learning with depthwise separable convolutions arxiv preprint arxiv 1610 02357, 2016 10'),\n",
       "  (18,\n",
       "   'table 2 the transformer achieves better bleu scores than previous state of the art models on the english to german and english to french newstest2014 tests at a fraction of the training cost model bleu training cost flops en de en fr en de en fr bytenet 18 23 75 deep att posunk 39 39 2 1 0 1020 gnmt rl 38 24 6 39 92 2 3 1019 1 4 1020 convs2s 9 25 16 40 46 9 6 1018 1 5 1020 moe 32 26 03 40 56 2 0 1019 1 2 1020 deep att posunk ensemble 39 40 4 8 0 1020 gnmt rl ensemble 38 26 30 41 16 1 8 1020 1 1 1021 convs2s ensemble 9 26 36 41 29 7 7 1019 1 2 1021 transformer base model 27 3 38 1 3 3 1018 transformer big 28 4 41 8 2 3 1019 label smoothing during training, we employed label smoothing of value UNK 0 1 36 this hurts perplexity, as the model learns to be more unsure, but improves accuracy and bleu score 6 results 6 1 machine translation on the wmt 2014 english to german translation task, the big transformer model transformer big in table 2 outperforms the best previously reported models including ensembles by more than 2 0 bleu,'),\n",
       "  (29,\n",
       "   ' 22, 2017 22 zhouhan lin, minwei feng, cicero nogueira dos santos, mo yu, bing xiang, bowen zhou, and yoshua bengio a structured self attentive sentence embedding arxiv preprint arxiv 1703 03130, 2017 23 minh thang luong, quoc v le, ilya sutskever, oriol vinyals, and lukasz kaiser multi task sequence to sequence learning arxiv preprint arxiv 1511 06114, 2015 24 minh thang luong, hieu pham, and christopher d manning effective approaches to attention based neural machine translation arxiv preprint arxiv 1508 04025, 2015 25 mitchell p marcus, mary ann marcinkiewicz, and beatrice santorini building a large annotated corpus of english the penn treebank computational linguistics, 19 2 313 330, 1993 26 david mcclosky, eugene charniak, and mark johnson effective self training for parsing in proceedings of the human language technology conference of the naacl, main conference, pages 152 159 acl, june 2006 11')],\n",
       " array([0.36939683, 0.38233893, 0.38261056, 0.38275288, 0.3827863 ]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"Generate a quiz with 3 questions based on these pdfs. Add images to make it pretty\"\n",
    "# query = \"Explain the causal attention with images in a blog post\"\n",
    "query = \"How do I calculate the attention weights in the transformer model\"\n",
    "query = \"Explain how the transformer model works\"\n",
    "# remove stopwords\n",
    "idx, dists = process_and_rank(query, corpus, lda_model)\n",
    "[(i, text_list_processed[i]) for i in idx], dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model arxiv attention models training translation neural transformer english sequence',\n",
       " 'attention layer decoder encoder sequence output layers model self models',\n",
       " 'attention input layer pad model models opinion sequence perfect missing',\n",
       " 'attention output sequence layer self function values neural models model',\n",
       " 'attention model training transformer english models neural arxiv dot translation']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05108256, 0.        , 0.        , 0.05108256, 0.05108256],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.16094379],\n",
       "       [0.09162907, 0.        , 0.        , 0.        , 0.09162907],\n",
       "       [0.        , 0.09162907, 0.09162907, 0.        , 0.        ],\n",
       "       [0.02231436, 0.02231436, 0.02231436, 0.        , 0.02231436],\n",
       "       [0.09162907, 0.09162907, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.05108256, 0.        , 0.05108256, 0.05108256, 0.        ],\n",
       "       [0.05108256, 0.        , 0.        , 0.05108256, 0.05108256],\n",
       "       [0.02231436, 0.02231436, 0.        , 0.02231436, 0.02231436],\n",
       "       [0.        , 0.16094379, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.16094379, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.16094379, 0.        ],\n",
       "       [0.        , 0.05108256, 0.05108256, 0.        , 0.05108256],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.09162907, 0.09162907, 0.        ],\n",
       "       [0.        , 0.        , 0.09162907, 0.09162907, 0.        ],\n",
       "       [0.        , 0.        , 0.16094379, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call ctfidf on lda topics \n",
    "tf_idf, count = c_tf_idf(topic_data, num_topics)\n",
    "tf_idf # topics x vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### junK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=[\"Doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=1)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
