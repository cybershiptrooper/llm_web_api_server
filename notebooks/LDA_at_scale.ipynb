{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "sys.path.append('../')\n",
    "from utils.nlp_trainers import LDATrainer\n",
    "# abs path of ../\n",
    "dir = os.path.abspath(\"../\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\")\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=2048, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "full_doc = \"\\n\".join([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [text.page_content for text in texts]\n",
    "doc_list = [doc.page_content for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2af646d00>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit LDATrainer(10, text_list, passes=1) # 1.95s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit LDATrainer(10, text_list, passes=10) # 2.08s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit LDATrainer(10, text_list, passes=100) # 3.21s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "lda = LDATrainer(10, text_list, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer', 'causal', 'attention', 'multi', 'head']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4, 9, 5, 2, 17],\n",
       " [0.3064436553268409,\n",
       "  0.3068407098409938,\n",
       "  0.3068876304280315,\n",
       "  0.3069295311858288,\n",
       "  0.6345334846327603],\n",
       " ['lution following sections describe transformer motivate self attention discuss advantages models model architecture competitive neural sequence transduction models encoder decoder structure encoder maps input sequence symbol representations xn sequence continuous representations zn given decoder generates output sequence ym symbols element time step model auto regressive consuming previously generated symbols additional input generating transformer follows overall architecture stacked self attention point wise fully connected layers encoder decoder shown left right halves figure respectively',\n",
       "  'multi head attention allows model jointly attend information different representation subspaces different positions single attention head averaging inhibits multihead concat head headh headi attention qw kw projections parameter matrices rdmodel dk rdmodel dk rdmodel dv rhdv dmodel work employ parallel attention layers heads dk dv dmodel reduced dimension head total computational cost similar single head attention dimensionality applications attention model transformer uses multi head attention different ways encoder decoder attention layers queries come previous decoder layer memory keys values come output encoder allows position decoder attend positions input sequence mimics typical encoder decoder attention mechanisms sequence sequence models encoder contains self attention layers self attention layer keys values queries come place case output previous layer encoder position encoder attend positions previous layer encoder similarly self attention layers decoder allow',\n",
       "  'figure transformer model architecture encoder decoder stacks encoder encoder composed stack identical layers layer sub layers rst multi head self attention mechanism second simple position wise fully connected feed forward network employ residual connection sub layers followed layer normalization output sub layer layernorm sublayer sublayer function implemented sub layer facilitate residual connections sub layers model embedding layers produce outputs dimension dmodel decoder decoder composed stack identical layers addition sub layers encoder layer decoder inserts sub layer performs multi head attention output encoder stack similar encoder employ residual connections sub layers followed layer normalization modify self attention sub layer decoder stack prevent positions attending subsequent positions masking combined fact output embeddings offset position ensures predictions position depend known outputs positions attention attention function described mapping query set key value pairs output query keys values output vectors output computed weighted sum values weight assigned',\n",
       "  'transduction problems language modeling machine translation numerous efforts continued push boundaries recurrent language models encoder decoder architectures recurrent models typically factor computation symbol positions input output sequences aligning positions steps computation time generate sequence hidden states ht function previous hidden state ht input position inherently sequential nature precludes parallelization training examples critical longer sequence lengths memory constraints limit batching examples recent work achieved signi cant improvements computational ef ciency factorization tricks conditional computation improving model performance case fundamental constraint sequential computation remains attention mechanisms integral compelling sequence modeling transduc tion models tasks allowing modeling dependencies regard distance input output sequences cases attention mechanisms conjunction recurrent network work propose transformer model architecture eschewing recurrence instead relying entirely attention mechanism draw global dependencies input output transformer allows signi cantly parallelization reach new state art translation quality trained little hours gpus background goal reducing sequential computation forms foundation extended neural gpu bytenet convs convolutional neural networks basic building block',\n",
       "  'warmup steps regularization employ types regularization training residual dropout apply dropout output sub layer added sub layer input normalized addition apply dropout sums embeddings positional encodings encoder decoder stacks base model rate pdrop'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query = \"Generate a quiz with 3 questions based on these pdfs. Add images to make it pretty\"\n",
    "# query = \"Explain the causal attention with images in a blog post\"\n",
    "# query = \"How do I calculate the attention weights in the transformer model\"\n",
    "# query = \"Explain how the transformer model works\"\n",
    "query = \"transformer causal attention multi-head\"\n",
    "rankings= lda.process_and_rank(query)\n",
    "[i for i, _, _ in rankings], [k for _, _, k in rankings], [' '.join(j) for _, j, _ in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = [\" \".join([lda.dictionary[i] for i, _ in lda.lda_model.get_topic_terms(j)]) for j in range(lda.lda_model.num_topics)]\n",
    "smart_query = \" \".join(topic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attention model transformer input sequence models training missing eos perfect',\n",
       " 'machine translation arxiv neural acl zhang fast zhu preprint long',\n",
       " 'attention encoder decoder layers layer output sub sequence model positions',\n",
       " 'attention translation transformer tasks based model layers english models arxiv',\n",
       " 'arxiv neural preprint learning recurrent networks acl empirical proceedings language',\n",
       " 'neural dropout networks machine processing sutskever information advances systems sequence',\n",
       " 'layer decoder positional position output softmax dmodel layers embeddings attention',\n",
       " 'attention dot values dk google function product keys com products',\n",
       " 'arxiv preprint wsj neural learning machine bengio semi supervised discriminative',\n",
       " 'attention models length training model layer pad self input sequence']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5 Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_pdfs/CS_781_Project.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/test_html2pdf_out.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_19_09_38.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_22_05_23.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/gpt2.pdf\"),]\n",
    "\n",
    "text_list = []\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=2048, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    text_list += [text.page_content for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "lda = LDATrainer(10, text_list, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['write', 'blog', 'gpt', 'different', 'vaswanis', 'transformers']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([13, 83, 12, 14, 120],\n",
       " [0.410745705785843,\n",
       "  0.4139372978215715,\n",
       "  0.4284277743314543,\n",
       "  0.5033413285710943,\n",
       "  0.5368317875108054],\n",
       " ['geometric progression chose function hypothesized allow model easily learn attend relative positions xed offset pepos represented linear function pepos experimented learned positional embeddings instead found versions produced nearly identical results table row chose sinusoidal version allow model extrapolate sequence lengths longer ones encountered training self attention section compare aspects self attention layers recurrent convolu tional layers commonly mapping variable length sequence symbol representations xn sequence equal length zn xi zi rd hidden layer typical sequence transduction encoder decoder motivating self attention consider desiderata total computational complexity layer computation parallelized measured minimum number sequential operations required path length long range dependencies network learning long range dependencies key challenge sequence transduction tasks key factor affecting ability learn dependencies length paths forward backward signals traverse network shorter paths combination positions input output sequences easier learn long range dependencies compare maximum path length input output positions networks composed different layer',\n",
       "  'glue especially unclear additional',\n",
       "  'table maximum path lengths layer complexity minimum number sequential operations different layer types sequence length representation dimension kernel size convolutions size neighborhood restricted self attention layer type complexity layer sequential maximum path length operations self attention recurrent convolutional logk self attention restricted tokens sequence end add positional encodings input embeddings bottoms encoder decoder stacks positional encodings dimension dmodel embeddings summed choices positional encodings learned xed work sine cosine functions different frequencies pe pos sin pos dmodel pe pos cos pos dmodel pos position dimension dimension positional encoding corresponds sinusoid wavelengths form geometric progression chose function hypothesized allow model easily learn attend relative positions xed offset pepos',\n",
       "  'paths combination positions input output sequences easier learn long range dependencies compare maximum path length input output positions networks composed different layer types noted table self attention layer connects positions constant number sequentially executed operations recurrent layer requires sequential operations terms computational complexity self attention layers faster recurrent layers sequence length smaller representation dimensionality case sentence representations state art models machine translations word piece byte pair representations improve computational performance tasks involving long sequences self attention restricted considering neighborhood size',\n",
       "  'shown truncated generations tokens random sampling generation'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a blog on how is gpt2 different from vaswani's transformers\"\n",
    "rankings= lda.process_and_rank(query)\n",
    "[i for i, _, _ in rankings], [k for _, _, k in rankings], [' '.join(j) for _, j, _ in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([120, 109, 44, 107, 34],\n",
       " [0.4072246414249033,\n",
       "  0.47544741326632445,\n",
       "  0.47577176389962206,\n",
       "  0.4768102345914698,\n",
       "  0.5062863525242751],\n",
       " ['shown truncated generations tokens random sampling generation',\n",
       "  'turning ghost location kind return began achieved ways simply leave location example pac man jump cliff return spot vanishing space good number possibilities ghost paths possible obvious ones player wants avoid getting caught game goal easy keeping pac man game long chasing pac man case similar pac man catch ght ghost chase enemies ful reasons pac man ai unlike ghost games ghost simply travels direction gpt completion cross intersection pac man able dodge ghosts projectiles return safe location safe house buster yellow ghost ai speci cally designed try avoid pac man leaving safe house player press space bar reveal safe house doors nd yellow ghost hiding room player note ghost attempt exit safe house sees pac man fact ghost attempt exit rst place safe house behaviors based fact player attempt reach safe house pac man blinky player beware glitch ghost player',\n",
       "  'sharif bhagavatula bauer reiter adversarial generative nets neural network attacks state art face recognition arxiv preprint dec url https arxiv org abs singh gehr uschel vechev abstract domain certifying neural networks proc acm program lang popl jan doi url https doi org links implementation reluplex https github com marabou deeppoly https github com taufeeque eranl',\n",
       "  'spain granada con quered table random unseen contexts non cherry picked completions smallest left largest right models contexts tokens approximately tokens worth paragraphs shown completions tokens fully shown random sampling generation',\n",
       "  'input input layer law perfect application missing opinion eos pad law perfect application missing opinion eos pad input input layer law perfect application missing opinion eos pad law perfect application missing opinion eos pad figure attention heads layer apparently involved anaphora resolution attentions head isolated attentions word attention heads note attentions sharp word'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"global warming\"\n",
    "rankings= lda.process_and_rank(query)\n",
    "[i for i, _, _ in rankings], [k for _, _, k in rankings], [' '.join(j) for _, j, _ in rankings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
