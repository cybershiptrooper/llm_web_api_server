{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "sys.path.append('../')\n",
    "# from utils.nlp_trainers import LDATrainer\n",
    "from utils.nlp_utils import process_data\n",
    "# abs path of ../\n",
    "dir = os.path.abspath(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_spacy(texts):\n",
    "    # Preprocess the texts\n",
    "    sentences = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        sentences.extend([sent.text for sent in doc.sents])\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        preprocessed_sentence = ' '.join([token.text for token in doc if not token.is_stop])\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    return preprocessed_sentences\n",
    "\n",
    "def textrank_graph_lda_spacy(texts, n_components=10):\n",
    "    # Compute the document-term matrix\n",
    "    dtm = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Fit the LDA model\n",
    "    lda = LatentDirichletAllocation(n_components=n_components)\n",
    "    lda_matrix = lda.fit_transform(dtm)\n",
    "\n",
    "    # Compute the similarity matrix\n",
    "    similarity_matrix = (lda_matrix @ lda_matrix.T)\n",
    "\n",
    "    # Create the graph\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Compute the PageRank scores\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    # Sort the sentences by their scores\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(texts)), reverse=True)\n",
    "\n",
    "    return graph, ranked_sentences, lda, lda_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_pdfs/CS_781_Project.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/test_html2pdf_out.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_19_09_38.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_22_05_23.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/gpt2.pdf\"),]\n",
    "\n",
    "text_list = []\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=2048, chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    text_list += [text.page_content for text in texts]\n",
    "\n",
    "text_list_processed = process_data(text_list)\n",
    "len(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_processed = preprocess_spacy(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, ranked_sentences, lda, lda_matrix = textrank_graph_lda_spacy(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_graph(texts, query, k=5):\n",
    "        # Preprocess the query\n",
    "    doc = nlp(query)\n",
    "    preprocessed_query = ' '.join([token.text for token in doc if not token.is_stop])\n",
    "\n",
    "    # Compute the query vector\n",
    "    query_vector = vectorizer.transform([preprocessed_query])\n",
    "    query_topic_distribution = lda.transform(query_vector)\n",
    "\n",
    "    # Compute the similarity between the query and the sentences\n",
    "    similarities = cosine_similarity(lda_matrix, query_topic_distribution)\n",
    "\n",
    "    # Sort the sentences by their similarity to the query\n",
    "    sorted_indices = np.argsort(similarities, axis=0)[::-1].flatten()\n",
    "    top_k_sentences = [texts[i] for i in sorted_indices[:k]]\n",
    "\n",
    "    return top_k_sentences, similarities[sorted_indices[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['glue , especially unclear additional',\n",
       "  'd wear denim , chinos , , recommend wearing variables involved t 3 dress nal',\n",
       "  'bit traditional , mentioned , s low budget item expensive de nitely issue trying work casual sneakermaker , s ok',\n",
       "  'know people opposed wearing sneakers turtleneck , wanted true , went slim tting turtleneck color similar favorite color day overall , s classic turtleneck , ashy',\n",
       "  'ymax ylabel hold entire abstraction condition hold approximations deeppoly reduces problem checking l perturbations , observe phenomenon experiments 2 2 exact solver reluplex exact solver 1 approximation directly feed l1 constraints solver , feeding l1 norm constraints need 2pixels equations practically feasible observe region mere 16 pixels causes memory issues standard computer auxilliary 2',\n",
       "  'd suggest wear sneakers khaki chino suit white linen suit sure ditch tie wear tee polo wearing tie sneakers bit small model completion t 2 high end t eye storm s hybrid asics streetwear like j crew j crew casual pieces bit traditional , mentioned , s low budget item expensive de nitely issue trying work casual sneakermaker ,',\n",
       "  'don t hesitate want try lower price versions ts version features low cut cut neckline bad thing , slightly longer legs , pair long legs things nice preferred color color items blue gray rst colors , light white , black favorite designs , pick colors store de sign high end , recommend t 3 high light high light look chance color think color based gpt 2 completion t 2 casual jacket',\n",
       "  'figure 3 adversaries obtained strongly perturbing regions note image large difference original figure displayed normalising perturbed image maximum possible perturbations figure 3 shows obtained adversaries , l1 norm constrained 1 113 176 verified correctly case adversaries examples skipped conciseness , obtained running code section 5 default settings 4 conclusion experiments , conclude l1 norm constraint verification deeppoly possible auxiliary network way abstractions constructed need general purpose way trace partition input like deeppoly geometric transformations reluplex , verification infeasible entire image allowed perturb reluplex handle constraints small region image , note current robustness analyzers capable handling l1 perturbations references 1 g katz , c barrett , d l dill , k julian , m j kochenderfer reluplex efficient smt solver verifying deep neural networks computer aided verification , pages 97 117 springer international publishing , 2017 doi 10 1007 978 3 319 63387 9 5 url https doi org 10 1007 2f978 3 319 63387 9 5 2 m sharif , s bhagavatula , l bauer , m k reiter adversarial generative',\n",
       "  'cs 781 project report verifying l1 adverserial robustness rohan gupta 180010048 iitb ac mohammad taufeeque 180050062 iitb ac abstract study performance popular neural network verification tools l1 perturbations feeding l1 constraints solver exponential order inputs , experiment function approximators possibly reduce complexity results tools , marabou reluplex eran deeppoly respectively keywords verification neural nets l1 attack 1 introduction verification tools strong experimental results l perturbations , attacks lp norms increasingly popular observed models robust l vulnerable small , perceptually minor departures family , small rotations trans lations , need look high performance perturbations main issue l perturbations lies making bounds larger pixel wise perturbations independent , easily cross decision boundaries 2 example adversaries , l norm large , lp norms evaluate performance extant tools l1 perturbations , modifications instead giving exponential number equations solver , auxilliary network find l1 norm conditions applied output observe key differences l1 l perturbations experiments approximations deeppoly 3 result poor performance case l1 norms , formulation ,',\n",
       "  'space deeppoly found option trace partitioning exists specific cases 1 geometric transformation image , 2 acasxu dataset higher dimensional space like images , trace partitioning o npixels time , n number partitions dimension 3 2 reluplex l1 norm constraint entire image takes long run reluplex , try experiment images conduct 3 experiments images perturb small regions , giving explicit constraints regions note perturbations subset l norms increased maximum change possible pixel ii perturb small regions auxilliary networks instead iii perturb entire image consider 3 types norms experiment , , l , l1 clipped norms given lc min l , l1 , l , l1 UNK , UNK benefit lc seen considering large l l1 norms search spaces l1 1 valid images s values lie 0 1 , large l bounds cross decision boundaries easily clipped norm manages avoid issues table 1 shows robustness sparse network different lc norms perturbing different regions explicit constraints limited 3x4 pixels , reluplex gives un sat time run experiments ii 173 regions run maximum l bounds 1 l1 restrictions gave un'],\n",
       " array([[0.89661748],\n",
       "        [0.88786542],\n",
       "        [0.88469817],\n",
       "        [0.88328481],\n",
       "        [0.88127399],\n",
       "        [0.8812397 ],\n",
       "        [0.88093746],\n",
       "        [0.88020237],\n",
       "        [0.88016421],\n",
       "        [0.88016419]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_graph(text_list_processed, \"Difference between gpt2 and original transformers\", k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
