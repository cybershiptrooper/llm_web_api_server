{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "sys.path.append('../')\n",
    "from utils.nlp_utils import *\n",
    "# abs path of ../\n",
    "dir = os.path.abspath(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "        os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\"),\n",
    "        os.path.join(dir, \"storage/test_pdfs/CS_781_Project.pdf\"),\n",
    "        os.path.join(dir, \"storage/test_responses/test_html2pdf_out.pdf\"),\n",
    "        os.path.join(dir, \"storage/test_responses/15_06_2023_19_09_38.pdf\"),\n",
    "        os.path.join(dir, \"storage/test_responses/15_06_2023_22_05_23.pdf\"),\n",
    "        os.path.join(dir, \"storage/test_responses/gpt2.pdf\"),\n",
    "    ]\n",
    "\n",
    "def split_docs(text_splitter):\n",
    "    text_list = []\n",
    "    title_list = []\n",
    "    for file_path in file_paths:\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        print(len(texts))\n",
    "        text_list.extend(texts)\n",
    "    print(len(text_list))\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process texts.page_content\n",
    "# text_list = process_texts_page_content(text_list, remove_new_lines=False): ig I cannot do this (why? :/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Transformers at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=128, chunk_overlap=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "22\n",
      "7\n",
      "2\n",
      "1\n",
      "207\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "texts = split_docs(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(documents=texts, embedding=SentenceTransformerEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': 's generations to the overlap rates of the ground - truth completions. the results of this analysis are shown below and suggest that gpt - 2 repeats text from the training set less often then the baseline rate of held - out articles. figure 5. cdf of percentage 8 - gram overlap with webtext train - ing set, for both webtext test set and samples ( conditioned on webtext test set, with top - k truncated random sampling with k = 40 ). most samples have less than 1 % overlap, including over 30 % of samples with no overlap, whereas the median for test set is 2. 6 % overlap. 8', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 12, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='s generations to the overlap rates of the ground - truth completions. the results of this analysis are shown below and suggest that gpt - 2 repeats text from the training set less often then the baseline rate of held - out articles. figure 5. cdf of percentage 8 - gram overlap with webtext train - ing set, for both webtext test set and samples ( conditioned on webtext test set, with top - k truncated random sampling with k = 40 ). most samples have less than 1 % overlap, including over 30 % of samples with no overlap, whereas the median for test set is 2. 6 % overlap. 8', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 12, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.2195634841918945),\n",
       " (Document(lc_kwargs={'page_content': '. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invo', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invo', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.2426859140396118),\n",
       " (Document(lc_kwargs={'page_content': 'gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.2621800899505615),\n",
       " (Document(lc_kwargs={'page_content': 'who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitative', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitative', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.319917917251587)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a blog on how is gpt2 different from vaswani's transformers\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  0.7201018929481506),\n",
       " (Document(lc_kwargs={'page_content': 'storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content='storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  0.8628621697425842),\n",
       " (Document(lc_kwargs={'page_content': '? a : model answer : everest turker answers : unknown, yes, yes, yes table 16. selected coqa completion.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 22, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='? a : model answer : everest turker answers : unknown, yes, yes, yes table 16. selected coqa completion.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 22, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.4990885257720947),\n",
       " (Document(lc_kwargs={'page_content': 'power plant that blew up in russia? chernobyl 45. 7 % who played john connor in the original terminator? arnold schwarzenegger 45. 2 % table 5. the 30 most conﬁdent answers generated by gpt - 2 on the development set of natural questions sorted by their probability according to gpt - 2. none of these questions appear in webtext according to the procedure described in section 4. ( conneau et al., 2017b ). on the wmt - 14 french - english test set, gpt - 2 is able to leverage its very strong english language model to perform signiﬁcan', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 6, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='power plant that blew up in russia? chernobyl 45. 7 % who played john connor in the original terminator? arnold schwarzenegger 45. 2 % table 5. the 30 most conﬁdent answers generated by gpt - 2 on the development set of natural questions sorted by their probability according to gpt - 2. none of these questions appear in webtext according to the procedure described in section 4. ( conneau et al., 2017b ). on the wmt - 14 french - english test set, gpt - 2 is able to leverage its very strong english language model to perform signiﬁcan', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 6, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.5920466184616089)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"greenhouse gases\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': 'so owners often decide to offer health and even life for their dog. in sweden dog owners must pay for any damage their dog does. a swedish kennel club ofﬁcial ex - plains what this means : if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay for any damage done to the car, even if your dog has been killed in the accident. q : how old is catherine? a : 54 q : where does she live? a : model answer : stockholm turker answers : sweden, sweden, in sweden, sweden table 17. selected coqa completion', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='so owners often decide to offer health and even life for their dog. in sweden dog owners must pay for any damage their dog does. a swedish kennel club ofﬁcial ex - plains what this means : if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay for any damage done to the car, even if your dog has been killed in the accident. q : how old is catherine? a : 54 q : where does she live? a : model answer : stockholm turker answers : sweden, sweden, in sweden, sweden table 17. selected coqa completion', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.6669961214065552),\n",
       " (Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  1.6845102310180664),\n",
       " (Document(lc_kwargs={'page_content': 'and tom live in sweden, a country where everyone is expected to lead an orderly life accord - ing to rules laid down by the government, which also provides a high level of care for its people. this level of care costs money. people in sweden pay taxes on everything, so aren ’ t surprised to ﬁnd that owning a dog means more taxes. some people are paying as much as 500 swedish kronor in taxes a year for the right to keep their dog, which is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. however, most such treatment is expensive, so owners often decide to', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='and tom live in sweden, a country where everyone is expected to lead an orderly life accord - ing to rules laid down by the government, which also provides a high level of care for its people. this level of care costs money. people in sweden pay taxes on everything, so aren ’ t surprised to ﬁnd that owning a dog means more taxes. some people are paying as much as 500 swedish kronor in taxes a year for the right to keep their dog, which is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. however, most such treatment is expensive, so owners often decide to', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.7712092399597168),\n",
       " (Document(lc_kwargs={'page_content': 'to cookie crumbs or chocolate chips. 1 ) when i buy raw sugar, i do not use it until it ’ s cooled down and has at least half a teaspoon of sugar on it. that ’ s my rule. and 2 ) if i have a recipe that calls for something and i want to use it in a recipe, i do not substitute it with something else on - line. like this. i found this really cute card with cute little kittens on gpt - 2 completion 3 recipe, which will likely yield about the same amount of cake. chocolate buttermilk cake print chunks of chocolate cake, coated', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 18, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='to cookie crumbs or chocolate chips. 1 ) when i buy raw sugar, i do not use it until it ’ s cooled down and has at least half a teaspoon of sugar on it. that ’ s my rule. and 2 ) if i have a recipe that calls for something and i want to use it in a recipe, i do not substitute it with something else on - line. like this. i found this really cute card with cute little kittens on gpt - 2 completion 3 recipe, which will likely yield about the same amount of cake. chocolate buttermilk cake print chunks of chocolate cake, coated', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 18, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.8580360412597656)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"is it wrong to hurt someone?\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': ') ( amodei et al., 2016 ). yet these systems are brittle and sensitive to slight changes in the data distribution ( recht et al., 2018 ) and task speciﬁcation ( kirkpatrick et al., 2017 ). current sys - tems are better characterized as narrow experts rather than *, * * equal contribution 1openai, san francisco, califor - nia, united states. correspondence to : alec radford < alec @ openai. com >. competent generalists. we would like to move towards more general systems which can perform many tasks – eventually without the need', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 0, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content=') ( amodei et al., 2016 ). yet these systems are brittle and sensitive to slight changes in the data distribution ( recht et al., 2018 ) and task speciﬁcation ( kirkpatrick et al., 2017 ). current sys - tems are better characterized as narrow experts rather than *, * * equal contribution 1openai, san francisco, califor - nia, united states. correspondence to : alec radford < alec @ openai. com >. competent generalists. we would like to move towards more general systems which can perform many tasks – eventually without the need', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 0, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.3930805921554565),\n",
       " (Document(lc_kwargs={'page_content': 'input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  1.4607871770858765),\n",
       " (Document(lc_kwargs={'page_content': ', in the context of dialog, for the need to develop systems capable of learning from natural language directly and demonstrated a proof of concept – learning a qa task without a reward signal by using forward prediction of a teacher ’ s outputs. while dialog is an attractive approach, we worry it is overly restrictive. the internet contains a vast amount of information that is passively available without the need for interactive communication. our speculation is that a language model with sufﬁcient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. if', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 2, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content=', in the context of dialog, for the need to develop systems capable of learning from natural language directly and demonstrated a proof of concept – learning a qa task without a reward signal by using forward prediction of a teacher ’ s outputs. while dialog is an attractive approach, we worry it is overly restrictive. the internet contains a vast amount of information that is passively available without the need for interactive communication. our speculation is that a language model with sufﬁcient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement. if', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 2, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.476681113243103),\n",
       " (Document(lc_kwargs={'page_content': 'j. building machines that learn and think like people. behavioral and brain sciences, 40, 2017. lample, g., conneau, a., denoyer, l., and ranzato, m. unsu - pervised machine translation using monolingual corpora only. arxiv preprint arxiv : 1711. 00043, 2017. levesque, h., davis, e., and morgenstern, l. the winograd schema challenge. in thirteenth international conference on the principles of knowledge representation and reasoning, 2012. levy, o. and goldberg', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='j. building machines that learn and think like people. behavioral and brain sciences, 40, 2017. lample, g., conneau, a., denoyer, l., and ranzato, m. unsu - pervised machine translation using monolingual corpora only. arxiv preprint arxiv : 1711. 00043, 2017. levesque, h., davis, e., and morgenstern, l. the winograd schema challenge. in thirteenth international conference on the principles of knowledge representation and reasoning, 2012. levy, o. and goldberg', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  1.478132963180542)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"moral implications of ai\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "9\n",
      "3\n",
      "1\n",
      "1\n",
      "79\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "texts = split_docs(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "vectordb = Chroma.from_documents(documents= texts, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': 'in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.3600834608078003),\n",
       " (Document(lc_kwargs={'page_content': 'attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.3880215585231781),\n",
       " (Document(lc_kwargs={'page_content': 'generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invoke task speciﬁc behavior in a language model with natural language. 3. 7. translation we test whether gpt - 2 has begun to learn how to translate from one language to another. in order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format english sentence = french sentence and then after a ﬁ - nal prompt of english sentence = we sample from the model with greedy decoding and use the ﬁrst generated sentence as the translation. on the wmt - 14 english - french test set, gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invoke task speciﬁc behavior in a language model with natural language. 3. 7. translation we test whether gpt - 2 has begun to learn how to translate from one language to another. in order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format english sentence = french sentence and then after a ﬁ - nal prompt of english sentence = we sample from the model with greedy decoding and use the ﬁrst generated sentence as the translation. on the wmt - 14 english - french test set, gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.40084108710289),\n",
       " (Document(lc_kwargs={'page_content': '. 6. 2 model variations to evaluate the importance of different components of the transformer, we varied our base model in different ways, measuring the change in performance on english - to - german translation on the development set, newstest2013. we used beam search as described in the previous section, but no checkpoint averaging. we present these results in table 3. in table 3 rows ( a ), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in section 3. 2. 2. while single - head attention is 0. 9 bleu worse than the best setting, quality also drops off with too many heads. 5we used values of 2. 8, 3. 7, 6. 0 and 9. 5 tflops for k80, k40, m40 and p100, respectively. 8', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='. 6. 2 model variations to evaluate the importance of different components of the transformer, we varied our base model in different ways, measuring the change in performance on english - to - german translation on the development set, newstest2013. we used beam search as described in the previous section, but no checkpoint averaging. we present these results in table 3. in table 3 rows ( a ), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in section 3. 2. 2. while single - head attention is 0. 9 bleu worse than the best setting, quality also drops off with too many heads. 5we used values of 2. 8, 3. 7, 6. 0 and 9. 5 tflops for k80, k40, m40 and p100, respectively. 8', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4189831018447876)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a blog on how is gpt2 different from vaswani's transformers\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': 'in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.3601120710372925),\n",
       " (Document(lc_kwargs={'page_content': 'attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.388257771730423),\n",
       " (Document(lc_kwargs={'page_content': 'generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invoke task speciﬁc behavior in a language model with natural language. 3. 7. translation we test whether gpt - 2 has begun to learn how to translate from one language to another. in order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format english sentence = french sentence and then after a ﬁ - nal prompt of english sentence = we sample from the model with greedy decoding and use the ﬁrst generated sentence as the translation. on the wmt - 14 english - french test set, gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speciﬁc details such as how many cars were involved in a crash or whether a logo was on a hat or shirt. on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article. gpt - 2 ’ s performance drops by 6. 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invoke task speciﬁc behavior in a language model with natural language. 3. 7. translation we test whether gpt - 2 has begun to learn how to translate from one language to another. in order to help it infer that this is the desired task, we condition the language model on a context of example pairs of the format english sentence = french sentence and then after a ﬁ - nal prompt of english sentence = we sample from the model with greedy decoding and use the ﬁrst generated sentence as the translation. on the wmt - 14 english - french test set, gpt - 2 gets 5 bleu, which is slightly worse than a word - by - word substitution with a bilingual lexicon in - ferred in previous work on unsupervised word translation', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.400892049074173),\n",
       " (Document(lc_kwargs={'page_content': '. 6. 2 model variations to evaluate the importance of different components of the transformer, we varied our base model in different ways, measuring the change in performance on english - to - german translation on the development set, newstest2013. we used beam search as described in the previous section, but no checkpoint averaging. we present these results in table 3. in table 3 rows ( a ), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in section 3. 2. 2. while single - head attention is 0. 9 bleu worse than the best setting, quality also drops off with too many heads. 5we used values of 2. 8, 3. 7, 6. 0 and 9. 5 tflops for k80, k40, m40 and p100, respectively. 8', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='. 6. 2 model variations to evaluate the importance of different components of the transformer, we varied our base model in different ways, measuring the change in performance on english - to - german translation on the development set, newstest2013. we used beam search as described in the previous section, but no checkpoint averaging. we present these results in table 3. in table 3 rows ( a ), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in section 3. 2. 2. while single - head attention is 0. 9 bleu worse than the best setting, quality also drops off with too many heads. 5we used values of 2. 8, 3. 7, 6. 0 and 9. 5 tflops for k80, k40, m40 and p100, respectively. 8', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4191219210624695)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a blog on how is gpt2 different from vaswani's transformers\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  0.47960034012794495),\n",
       " (Document(lc_kwargs={'page_content': '##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.4990575909614563),\n",
       " (Document(lc_kwargs={'page_content': 'language models are unsupervised multitask learners context ( passage and previous question / answer pairs ) tom goes everywhere with catherine green, a 54 - year - old secretary. he moves around her ofﬁce at work and goes shopping with her. ” most people don ’ t seem to mind tom, ” says catherine, who thinks he is wonderful. ” he ’ s my fourth child, ” she says. she may think of him and treat him that way as her son. he moves around buying his food, paying his health bills and his taxes, but in fact tom is a dog. catherine and tom live in sweden, a country where everyone is expected to lead an orderly life accord - ing to rules laid down by the government, which also provides a high level of care for its people. this level of care costs money. people in sweden pay taxes on everything, so aren ’ t surprised to ﬁnd that owning a dog means more taxes. some people are paying as much as 500 swedish kronor in taxes a year for the right to keep their dog, which is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. however, most such treatment is expensive, so owners often decide to offer health and even life for their dog. in sweden dog owners must pay for any damage their dog does. a swedish kennel club ofﬁcial ex - plains what this means : if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay for any damage done to the car, even if your dog has been killed in the accident. q : how old is catherine? a : 54 q : where does she live? a : model answer : stockholm turker answers : sweden, sweden, in sweden, sweden table 17. selected coqa completion.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='language models are unsupervised multitask learners context ( passage and previous question / answer pairs ) tom goes everywhere with catherine green, a 54 - year - old secretary. he moves around her ofﬁce at work and goes shopping with her. ” most people don ’ t seem to mind tom, ” says catherine, who thinks he is wonderful. ” he ’ s my fourth child, ” she says. she may think of him and treat him that way as her son. he moves around buying his food, paying his health bills and his taxes, but in fact tom is a dog. catherine and tom live in sweden, a country where everyone is expected to lead an orderly life accord - ing to rules laid down by the government, which also provides a high level of care for its people. this level of care costs money. people in sweden pay taxes on everything, so aren ’ t surprised to ﬁnd that owning a dog means more taxes. some people are paying as much as 500 swedish kronor in taxes a year for the right to keep their dog, which is spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. however, most such treatment is expensive, so owners often decide to offer health and even life for their dog. in sweden dog owners must pay for any damage their dog does. a swedish kennel club ofﬁcial ex - plains what this means : if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay for any damage done to the car, even if your dog has been killed in the accident. q : how old is catherine? a : 54 q : where does she live? a : model answer : stockholm turker answers : sweden, sweden, in sweden, sweden table 17. selected coqa completion.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.532875657081604),\n",
       " (Document(lc_kwargs={'page_content': 'gratuite qu ’ il avait subie pour soigner une hernie lui permettrait de travailler ` a nouveau. a man told me that the operation gratuity he had been promised would not allow him to travel. english reference gpt - 2 french translation speaking to an open government conference in london via video link, mr kerry said : ” there is no question that the president and i and others in government have actually learned of some things that had been happening on an automatic pilot because the ability has been there, going back to world war two and to the very difﬁcult years of the cold war, and then, of course, 9 / 11. ” le premier conference de l ’ ´edition internationale en ligne sur la vide, le ministre de l ’ ´edition internationale a ´et´e vide, lui dit : ” il n ’ y a qu ’ une question que le pr´esident et moi et tous les autres personnes d ’ une administration qui a ´et´e ´evident par la fois, par la part de la guerre, par la part de la guer french reference gpt - 2 english translation s ’ exprimant lors d ’ une conf´erence intergouvernementale ` a londres par liaison vid´eo, m. kerry a d´eclar´e : ” il est ind´eniable que le pr´esident, moi - [UNK] et d ’ autres mem - bres du gouvernement avons pris connaissance de cer - taines choses en mode pilote automatique parce que nous en avions la possibilit´e, d ` es la seconde guerre mondiale et jus', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 21, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='gratuite qu ’ il avait subie pour soigner une hernie lui permettrait de travailler ` a nouveau. a man told me that the operation gratuity he had been promised would not allow him to travel. english reference gpt - 2 french translation speaking to an open government conference in london via video link, mr kerry said : ” there is no question that the president and i and others in government have actually learned of some things that had been happening on an automatic pilot because the ability has been there, going back to world war two and to the very difﬁcult years of the cold war, and then, of course, 9 / 11. ” le premier conference de l ’ ´edition internationale en ligne sur la vide, le ministre de l ’ ´edition internationale a ´et´e vide, lui dit : ” il n ’ y a qu ’ une question que le pr´esident et moi et tous les autres personnes d ’ une administration qui a ´et´e ´evident par la fois, par la part de la guerre, par la part de la guer french reference gpt - 2 english translation s ’ exprimant lors d ’ une conf´erence intergouvernementale ` a londres par liaison vid´eo, m. kerry a d´eclar´e : ” il est ind´eniable que le pr´esident, moi - [UNK] et d ’ autres mem - bres du gouvernement avons pris connaissance de cer - taines choses en mode pilote automatique parce que nous en avions la possibilit´e, d ` es la seconde guerre mondiale et jus', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 21, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.5479007959365845)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"is it wrong to hurt someone?\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  0.326377272605896),\n",
       " (Document(lc_kwargs={'page_content': '2 % what river is associated with the city of rome? the tiber 48. 6 % who is the ﬁrst president to be impeached? andrew johnson 48. 3 % who is the head of the department of homeland security 2017? john kelly 47. 0 % what is the name given to the common currency to the european union? euro 46. 8 % what was the emperor name in star wars? palpatine 46. 5 % do you have to have a gun permit to shoot at a range? no 46. 4 % who proposed evolution in 1859 as the basis of biological development? charles darwin 45. 7 % nuclear power plant that blew up in russia? chernobyl 45. 7 % who played john connor in the original terminator? arnold schwarzenegger 45. 2 % table 5. the 30 most conﬁdent answers generated by gpt - 2 on the development set of natural questions sorted by their probability according to gpt - 2. none of these questions appear in webtext according to the procedure described in section 4. ( conneau et al., 2017b ). on the wmt - 14 french - english test set, gpt - 2 is able to leverage its very strong english language model to perform signiﬁcantly better, achieving 11. 5 bleu. this outperforms several unsupervised machine translation baselines from ( artetxe et al., 2017 ) and ( lample et al., 2017 ) but is still much worse than the 33. 5 bleu of the current best unsupervised machine translation approach ( artetxe et al., 2019 ). performance on this task was sur - prising to us, since we deliberately removed non - english webpages from webtext as a ﬁltering step. in order to con - ﬁrm this, we ran a', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 6, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='2 % what river is associated with the city of rome? the tiber 48. 6 % who is the ﬁrst president to be impeached? andrew johnson 48. 3 % who is the head of the department of homeland security 2017? john kelly 47. 0 % what is the name given to the common currency to the european union? euro 46. 8 % what was the emperor name in star wars? palpatine 46. 5 % do you have to have a gun permit to shoot at a range? no 46. 4 % who proposed evolution in 1859 as the basis of biological development? charles darwin 45. 7 % nuclear power plant that blew up in russia? chernobyl 45. 7 % who played john connor in the original terminator? arnold schwarzenegger 45. 2 % table 5. the 30 most conﬁdent answers generated by gpt - 2 on the development set of natural questions sorted by their probability according to gpt - 2. none of these questions appear in webtext according to the procedure described in section 4. ( conneau et al., 2017b ). on the wmt - 14 french - english test set, gpt - 2 is able to leverage its very strong english language model to perform signiﬁcantly better, achieving 11. 5 bleu. this outperforms several unsupervised machine translation baselines from ( artetxe et al., 2017 ) and ( lample et al., 2017 ) but is still much worse than the 33. 5 bleu of the current best unsupervised machine translation approach ( artetxe et al., 2019 ). performance on this task was sur - prising to us, since we deliberately removed non - english webpages from webtext as a ﬁltering step. in order to con - ﬁrm this, we ran a', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 6, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.5226604342460632),\n",
       " (Document(lc_kwargs={'page_content': 'on a bigger region ( 16x16 ) 3', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}}, page_content='on a bigger region ( 16x16 ) 3', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}),\n",
       "  0.528068482875824),\n",
       " (Document(lc_kwargs={'page_content': 'requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.5293908715248108)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"greenhouse gases\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  0.3863450884819031),\n",
       " (Document(lc_kwargs={'page_content': 'with deep convolutional neural networks. in advances in neural information processing systems, pp. 1097 – 1105, 2012. kwiatkowski, t., palomaki, j., rhinehart, o., collins, m., parikh, a., alberti, c., epstein, d., polosukhin, i., kelcey, m., devlin, j., et al. natural questions : a benchmark for question answering research. 2019. lake, b. m., ullman, t. d., tenenbaum, j. b., and gershman, s. j. building machines that learn and think like people. behavioral and brain sciences, 40, 2017. lample, g., conneau, a., denoyer, l., and ranzato, m. unsu - pervised machine translation using monolingual corpora only. arxiv preprint arxiv : 1711. 00043, 2017. levesque, h., davis, e., and morgenstern, l. the winograd schema challenge. in thirteenth international conference on the principles of knowledge representation and reasoning, 2012. levy, o. and goldberg, y. neural word embedding as implicit ma - trix factorization. in advances in neural information processing systems, pp. 2177 – 2185, 2014. liu, p. j., saleh, m., pot, e., goodrich, b., sepassi, r., kaiser, l., and shazeer, n. generating wikipedia by summarizing long sequences. arxiv preprint arxiv : 1801. 10198, 2018. mccann, b., bradbury, j., xiong', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='with deep convolutional neural networks. in advances in neural information processing systems, pp. 1097 – 1105, 2012. kwiatkowski, t., palomaki, j., rhinehart, o., collins, m., parikh, a., alberti, c., epstein, d., polosukhin, i., kelcey, m., devlin, j., et al. natural questions : a benchmark for question answering research. 2019. lake, b. m., ullman, t. d., tenenbaum, j. b., and gershman, s. j. building machines that learn and think like people. behavioral and brain sciences, 40, 2017. lample, g., conneau, a., denoyer, l., and ranzato, m. unsu - pervised machine translation using monolingual corpora only. arxiv preprint arxiv : 1711. 00043, 2017. levesque, h., davis, e., and morgenstern, l. the winograd schema challenge. in thirteenth international conference on the principles of knowledge representation and reasoning, 2012. levy, o. and goldberg, y. neural word embedding as implicit ma - trix factorization. in advances in neural information processing systems, pp. 2177 – 2185, 2014. liu, p. j., saleh, m., pot, e., goodrich, b., sepassi, r., kaiser, l., and shazeer, n. generating wikipedia by summarizing long sequences. arxiv preprint arxiv : 1801. 10198, 2018. mccann, b., bradbury, j., xiong', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.451031357049942),\n",
       " (Document(lc_kwargs={'page_content': '##4, 2018. alberti, c., lee, k., and collins, m. a bert baseline for the natural questions. arxiv preprint arxiv : 1901. 08634, 2019. alcorn, m. a., li, q., gong, z., wang, c., mai, l., ku, w. - s., and nguyen, a. strike ( with ) a pose : neural networks are easily fooled by strange poses of familiar objects. arxiv preprint arxiv : 1811. 11553, 2018. amodei, d., ananthanarayanan, s., anubhai, r., bai, j., batten - berg, e., case, c., casper, j., catanzaro, b., cheng, q., chen, g., et al. deep speech 2 : end - to - end speech recognition in english and mandarin. in international conference on machine learning, pp. 173 – 182, 2016. artetxe, m., labaka, g., agirre, e., and cho, k. unsupervised neural machine translation. arxiv preprint arxiv : 1710. 11041, 2017. artetxe, m., labaka, g., and agirre, e. an effective ap - proach to unsupervised machine translation. arxiv preprint arxiv : 1902. 01313, 2019. 5preliminary code for downloading and using the small model is available at https : / / github. com / openai / gpt - 2 ba, j. l., kiros, j. r., and hinton, g. e.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 9, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##4, 2018. alberti, c., lee, k., and collins, m. a bert baseline for the natural questions. arxiv preprint arxiv : 1901. 08634, 2019. alcorn, m. a., li, q., gong, z., wang, c., mai, l., ku, w. - s., and nguyen, a. strike ( with ) a pose : neural networks are easily fooled by strange poses of familiar objects. arxiv preprint arxiv : 1811. 11553, 2018. amodei, d., ananthanarayanan, s., anubhai, r., bai, j., batten - berg, e., case, c., casper, j., catanzaro, b., cheng, q., chen, g., et al. deep speech 2 : end - to - end speech recognition in english and mandarin. in international conference on machine learning, pp. 173 – 182, 2016. artetxe, m., labaka, g., agirre, e., and cho, k. unsupervised neural machine translation. arxiv preprint arxiv : 1710. 11041, 2017. artetxe, m., labaka, g., and agirre, e. an effective ap - proach to unsupervised machine translation. arxiv preprint arxiv : 1902. 01313, 2019. 5preliminary code for downloading and using the small model is available at https : / / github. com / openai / gpt - 2 ba, j. l., kiros, j. r., and hinton, g. e.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 9, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.46619606018066406),\n",
       " (Document(lc_kwargs={'page_content': 'natural language processing, pages 832 – 841. acl, august 2009. [ 15 ] rafal jozefowicz, oriol vinyals, mike schuster, noam shazeer, and yonghui wu. exploring the limits of language modeling. arxiv preprint arxiv : 1602. 02410, 2016. [ 16 ] łukasz kaiser and samy bengio. can active memory replace attention? in advances in neural information processing systems, ( nips ), 2016. [ 17 ] łukasz kaiser and ilya sutskever. neural gpus learn algorithms. in international conference on learning representations ( iclr ), 2016. [ 18 ] nal kalchbrenner, lasse espeholt, karen simonyan, aaron van den oord, alex graves, and ko - ray kavukcuoglu. neural machine translation in linear time. arxiv preprint arxiv : 1610. 10099v2, 2017. [ 19 ] yoon kim, carl denton, luong hoang, and alexander m. rush. structured attention networks. in international conference on learning representations, 2017. [ 20 ] diederik kingma and jimmy ba. adam : a method for stochastic optimization. in iclr, 2015. [ 21 ] oleksii kuchaiev and boris ginsburg. factorization tricks for lstm networks. arxiv preprint arxiv : 1703. 10722, 2017. [ 22 ] zhouhan lin, minwei feng, cicero nogueira dos santos, mo yu, bing xiang, bowen zhou, and yoshua bengio. a structured self - attentive sentence embedding. arxiv preprint arxiv : 1703. 03130, 2017. [ 23 ] minh - thang luong, quoc', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 10, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='natural language processing, pages 832 – 841. acl, august 2009. [ 15 ] rafal jozefowicz, oriol vinyals, mike schuster, noam shazeer, and yonghui wu. exploring the limits of language modeling. arxiv preprint arxiv : 1602. 02410, 2016. [ 16 ] łukasz kaiser and samy bengio. can active memory replace attention? in advances in neural information processing systems, ( nips ), 2016. [ 17 ] łukasz kaiser and ilya sutskever. neural gpus learn algorithms. in international conference on learning representations ( iclr ), 2016. [ 18 ] nal kalchbrenner, lasse espeholt, karen simonyan, aaron van den oord, alex graves, and ko - ray kavukcuoglu. neural machine translation in linear time. arxiv preprint arxiv : 1610. 10099v2, 2017. [ 19 ] yoon kim, carl denton, luong hoang, and alexander m. rush. structured attention networks. in international conference on learning representations, 2017. [ 20 ] diederik kingma and jimmy ba. adam : a method for stochastic optimization. in iclr, 2015. [ 21 ] oleksii kuchaiev and boris ginsburg. factorization tricks for lstm networks. arxiv preprint arxiv : 1703. 10722, 2017. [ 22 ] zhouhan lin, minwei feng, cicero nogueira dos santos, mo yu, bing xiang, bowen zhou, and yoshua bengio. a structured self - attentive sentence embedding. arxiv preprint arxiv : 1703. 03130, 2017. [ 23 ] minh - thang luong, quoc', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 10, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.4662942588329315)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"moral implications of ai\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  0.42206308245658875),\n",
       " (Document(lc_kwargs={'page_content': 'did they climb any mountains? a : model answer : everest turker answers : unknown, yes, yes, yes table 16. selected coqa completion.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 22, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='did they climb any mountains? a : model answer : everest turker answers : unknown, yes, yes, yes table 16. selected coqa completion.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 22, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  0.5520509481430054),\n",
       " (Document(lc_kwargs={'page_content': 'attention visualizations input - input layer5 it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult. < eos > <pad> <pad> <pad> <pad> <pad> <pad> it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult. < eos > <pad> <pad> <pad> <pad> <pad> <pad> figure 3 : an example of the attention mechanism following long - distance dependencies in the encoder self - attention in layer 5 of 6. many of the attention heads attend to a distant dependency of the verb ‘ making ’, completing the phrase ‘ making... more difﬁcult ’. attentions here shown only for the word ‘ making ’. different colors represent different heads. best viewed in color. 13', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention visualizations input - input layer5 it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult. < eos > <pad> <pad> <pad> <pad> <pad> <pad> it is in this spirit that a majority of american governments have passed new laws since 2009 making the registration or voting process more difficult. < eos > <pad> <pad> <pad> <pad> <pad> <pad> figure 3 : an example of the attention mechanism following long - distance dependencies in the encoder self - attention in layer 5 of 6. many of the attention heads attend to a distant dependency of the verb ‘ making ’, completing the phrase ‘ making... more difﬁcult ’. attentions here shown only for the word ‘ making ’. different colors represent different heads. best viewed in color. 13', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.559424638748169),\n",
       " (Document(lc_kwargs={'page_content': 'input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 5 : many of the attention heads exhibit behaviour that seems related to the structure of the sentence. we give two such examples above, from two different heads from the encoder self - attention at layer 5 of 6. the heads clearly learned to perform different tasks. 15', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 5 : many of the attention heads exhibit behaviour that seems related to the structure of the sentence. we give two such examples above, from two different heads from the encoder self - attention at layer 5 of 6. the heads clearly learned to perform different tasks. 15', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  0.5628006458282471)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"explain the effect of rising heat on polar caps\"\n",
    "vectordb.similarity_search_with_score(query, top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMR vs Similarity: wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the queries used before\n",
    "queries = [\n",
    "    \"write a blog on how is gpt2 different from vaswani's transformers\",\n",
    "    \"greenhouse gases\",\n",
    "    \"is it wrong to hurt someone?\",\n",
    "    \"moral implications of ai\",\n",
    "    \"explain the effect of rising heat on polar caps\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [vectordb.max_marginal_relevance_search(query, top_k=5, diversity=0.5) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(lc_kwargs={'page_content': 'in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'table 3 : variations on the transformer architecture. unlisted values are identical to those of the base model. all metrics are on the english - to - german translation development set, newstest2013. listed perplexities are per - wordpiece, according to our byte - pair encoding, and should not be compared to per - word perplexities. n dmodel dff h dk dv pdrop [UNK] train ppl bleu params steps ( dev ) ( dev ) ×106 base 6 512 2048 8 64 64 0. 1 0. 1 100k 4. 92 25. 8 65 ( a ) 1 512 512 5. 29 24. 9 4 128 128 5. 00 25. 5 16 32 32 4. 91 25. 8 32 16 16 5. 01 25. 4 ( b ) 16 5. 16 25. 1 58 32 5. 01 25. 4 60 ( c ) 2 6. 11 23. 7 36 4 5. 19 25. 3 50 8 4. 88 25. 5 80 256 32 32 5. 75 24. 5 28 1024 128 128 4. 66 26. 0 168 1024 5. 12 25. 4 53 4096 4. 75 26. 2 90 ( d ) 0. 0 5. 77 24. 6 0. 2 4. 95 25. 5 0. 0 4. 67 25. 3 0. 2 5. 47 25. 7 ( e ) positional embedding instead of sinusoids 4. 92 25. 7 big 6 1024 4096 16 0. 3 300k 4. 33 26. 4 213 table 4 : the transformer generalizes well to english constituency parsing ( results are on section 23 of wsj ) parser training wsj 23 f1 vinyals & kaiser el al. ( 2014 ) [ 37 ] wsj only,', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='table 3 : variations on the transformer architecture. unlisted values are identical to those of the base model. all metrics are on the english - to - german translation development set, newstest2013. listed perplexities are per - wordpiece, according to our byte - pair encoding, and should not be compared to per - word perplexities. n dmodel dff h dk dv pdrop [UNK] train ppl bleu params steps ( dev ) ( dev ) ×106 base 6 512 2048 8 64 64 0. 1 0. 1 100k 4. 92 25. 8 65 ( a ) 1 512 512 5. 29 24. 9 4 128 128 5. 00 25. 5 16 32 32 4. 91 25. 8 32 16 16 5. 01 25. 4 ( b ) 16 5. 16 25. 1 58 32 5. 01 25. 4 60 ( c ) 2 6. 11 23. 7 36 4 5. 19 25. 3 50 8 4. 88 25. 5 80 256 32 32 5. 75 24. 5 28 1024 128 128 4. 66 26. 0 168 1024 5. 12 25. 4 53 4096 4. 75 26. 2 90 ( d ) 0. 0 5. 77 24. 6 0. 2 4. 95 25. 5 0. 0 4. 67 25. 3 0. 2 5. 47 25. 7 ( e ) positional embedding instead of sinusoids 4. 92 25. 7 big 6 1024 4096 16 0. 3 300k 4. 33 26. 4 213 table 4 : the transformer generalizes well to english constituency parsing ( results are on section 23 of wsj ) parser training wsj 23 f1 vinyals & kaiser el al. ( 2014 ) [ 37 ] wsj only,', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##s were trained on. the supervised sota, a bert based system ( devlin et al., r - 1 r - 2 r - l r - avg bottom - up sum 41. 22 18. 68 38. 34 32. 75 lede - 3 40. 38 17. 66 36. 62 31. 55 seq2seq + attn 31. 33 11. 81 28. 83 23. 99 gpt - 2 tl ; dr : 29. 34 8. 27 26. 58 21. 40 random - 3 28. 78 8. 63 25. 52 20. 98 gpt - 2 no hint 21. 58 4. 03 19. 47 15. 03 table 4. summarization performance as measured by rouge f1 metrics on the cnn and daily mail dataset. bottom - up sum is the sota model from ( gehrmann et al., 2018 ) 2018 ), is nearing the 89 f1 performance of humans. while gpt - 2 ’ s performance is exciting for a system without any su - pervised training, some inspection of its answers and errors suggests gpt - 2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble sum', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##s were trained on. the supervised sota, a bert based system ( devlin et al., r - 1 r - 2 r - l r - avg bottom - up sum 41. 22 18. 68 38. 34 32. 75 lede - 3 40. 38 17. 66 36. 62 31. 55 seq2seq + attn 31. 33 11. 81 28. 83 23. 99 gpt - 2 tl ; dr : 29. 34 8. 27 26. 58 21. 40 random - 3 28. 78 8. 63 25. 52 20. 98 gpt - 2 no hint 21. 58 4. 03 19. 47 15. 03 table 4. summarization performance as measured by rouge f1 metrics on the cnn and daily mail dataset. bottom - up sum is the sota model from ( gehrmann et al., 2018 ) 2018 ), is nearing the 89 f1 performance of humans. while gpt - 2 ’ s performance is exciting for a system without any su - pervised training, some inspection of its answers and errors suggests gpt - 2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble sum', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'sweden, sweden table 17. selected coqa completion.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='sweden, sweden table 17. selected coqa completion.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 4 : two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. top : full attentions for head 5. bottom : isolated attentions from just the word ‘ its ’ for attention heads 5 and 6. note that the attentions are very sharp for this word. 14', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 4 : two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. top : full attentions for head 5. bottom : isolated attentions from just the word ‘ its ’ for attention heads 5 and 6. note that the attentions are very sharp for this word. 14', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##nitely be an issue if you ’ re trying to work as a casual sneakermaker, and that ’ s ok but don ’ t hesitate if you want to try the lower price versions of their outﬁts! this version features a very low cut - cut neckline ( which is not a bad thing ), slightly longer legs, and a pair of long legs to keep things nice. my preferred color of color these items are blue - gray ( which is my ﬁrst colors, as they are a light white ), or black ( which is my favorite of my designs ). as always, my pick of colors from this store have nothing to do with that other than the de - sign. for a high end, i would recommend this to everyone! outﬁt 3 : high light this was a ” high light ” look while i had a chance to get some color. i think the color is based on gpt - 2 completion for me! outﬁt 2 : casual jacket i know that some people might be opposed to wearing sneakers in a turtleneck, but i wanted to be true to myself, so i went with a slim ﬁtting turtleneck in a color more similar to my favorite color of the day. overall, it ’ s a classic turtleneck, with nothing too ﬂashy. i did swap my shoes for these asics tiger killshots and i wish i hadn ’ t did that. the shoe itself is very comfortable, however, i found that wearing them in these sneakers made my feet look a bit larger. i ’ d wear them with denim, chinos, whatever, but would never recommend wearing them alone. there are just too many variables involved. outﬁt 3 : dress this is the ﬁnal outﬁt i wore the asics tiger killshots. this version of the asics is a much more modern and streamlined version. the shoe', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 13, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##nitely be an issue if you ’ re trying to work as a casual sneakermaker, and that ’ s ok but don ’ t hesitate if you want to try the lower price versions of their outﬁts! this version features a very low cut - cut neckline ( which is not a bad thing ), slightly longer legs, and a pair of long legs to keep things nice. my preferred color of color these items are blue - gray ( which is my ﬁrst colors, as they are a light white ), or black ( which is my favorite of my designs ). as always, my pick of colors from this store have nothing to do with that other than the de - sign. for a high end, i would recommend this to everyone! outﬁt 3 : high light this was a ” high light ” look while i had a chance to get some color. i think the color is based on gpt - 2 completion for me! outﬁt 2 : casual jacket i know that some people might be opposed to wearing sneakers in a turtleneck, but i wanted to be true to myself, so i went with a slim ﬁtting turtleneck in a color more similar to my favorite color of the day. overall, it ’ s a classic turtleneck, with nothing too ﬂashy. i did swap my shoes for these asics tiger killshots and i wish i hadn ’ t did that. the shoe itself is very comfortable, however, i found that wearing them in these sneakers made my feet look a bit larger. i ’ d wear them with denim, chinos, whatever, but would never recommend wearing them alone. there are just too many variables involved. outﬁt 3 : dress this is the ﬁnal outﬁt i wore the asics tiger killshots. this version of the asics is a much more modern and streamlined version. the shoe', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 13, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##1. 06440, 2015. trichelair, p., emami, a., cheung, j. c. k., trischler, a., sule - man, k., and diaz, f. on the evaluation of common - sense reasoning in natural language understanding. arxiv preprint arxiv : 1811. 01778, 2018. trinh, t. h. and le, q. v. a simple method for commonsense reasoning. arxiv preprint arxiv : 1806. 02847, 2018. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, ł., and polosukhin, i. attention is all you need. in advances in neural information processing systems, pp. 5998 – 6008, 2017. vinyals, o. and le, q. a neural conversational model. arxiv preprint arxiv : 1506. 05869, 2015. vinyals, o., fortunato, m., and jaitly, n. pointer networks. in advances in neural information processing systems, pp. 2692 – 2700, 2015. wang, a., singh, a., michael, j., hill, f., levy, o., and bow - man, s. r. glue : a multi - task benchmark and analysis platform for natural language understanding. arxiv preprint arxiv : 1804. 07461, 2018. weston, j. e. dialog - based language learning. in advances in neural information processing systems, pp. 829 – 837, 2016. wieting, j. and kiel', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 11, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##1. 06440, 2015. trichelair, p., emami, a., cheung, j. c. k., trischler, a., sule - man, k., and diaz, f. on the evaluation of common - sense reasoning in natural language understanding. arxiv preprint arxiv : 1811. 01778, 2018. trinh, t. h. and le, q. v. a simple method for commonsense reasoning. arxiv preprint arxiv : 1806. 02847, 2018. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, ł., and polosukhin, i. attention is all you need. in advances in neural information processing systems, pp. 5998 – 6008, 2017. vinyals, o. and le, q. a neural conversational model. arxiv preprint arxiv : 1506. 05869, 2015. vinyals, o., fortunato, m., and jaitly, n. pointer networks. in advances in neural information processing systems, pp. 2692 – 2700, 2015. wang, a., singh, a., michael, j., hill, f., levy, o., and bow - man, s. r. glue : a multi - task benchmark and analysis platform for natural language understanding. arxiv preprint arxiv : 1804. 07461, 2018. weston, j. e. dialog - based language learning. in advances in neural information processing systems, pp. 829 – 837, 2016. wieting, j. and kiel', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 11, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'language models are unsupervised multitask learners context ( webtext test ) pinky the pink ghost ’ s ai is designed to ” feel ” opposite of the red ghost ’ s behavior. pinky actually attempts to get out in front of pac - man. this is accomplished by setting the target 4 tiles ahead of pac - man ’ s current location in the direction that pac - man is travelling. one exception to this is when pac - man is traveling up. due to an overﬂow bug in the code, the calculation includes a left offset equal to the expected up offset. the use of pac - man ’ s expected position allows the player to exploit the navigation code for pinky. this was most often seen in situations where pac - man and pinky are in close proximity to each other. for example, if they were headed toward one another, once they are less than 4 tiles apart, the target tile is now behind pinky. if he encounters an intersection while in this conﬁguration, he will attempt to double - back ( by turning ) in order to reach the target tile. player ’ s would often perform what was referred to as a ” head fake ” by changing directions rapidly just as pinky was approaching an intersection. by displacing the target tile just as pinky small model completion was coming to the left, they would get the ” correct ” direction. if the location is actually farther from what pac - man was at, the player ’ s will not respond to the ghost ’ s movement, and the ghost will instead return to the player ’ s current location. this is because pac - man does not move by turning. the ghost will have to go to another location of some kind to return where he began, and this is achieved in many ways. it will often simply leave the location, as for example if pac - man did jump off a cliff', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 15, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='language models are unsupervised multitask learners context ( webtext test ) pinky the pink ghost ’ s ai is designed to ” feel ” opposite of the red ghost ’ s behavior. pinky actually attempts to get out in front of pac - man. this is accomplished by setting the target 4 tiles ahead of pac - man ’ s current location in the direction that pac - man is travelling. one exception to this is when pac - man is traveling up. due to an overﬂow bug in the code, the calculation includes a left offset equal to the expected up offset. the use of pac - man ’ s expected position allows the player to exploit the navigation code for pinky. this was most often seen in situations where pac - man and pinky are in close proximity to each other. for example, if they were headed toward one another, once they are less than 4 tiles apart, the target tile is now behind pinky. if he encounters an intersection while in this conﬁguration, he will attempt to double - back ( by turning ) in order to reach the target tile. player ’ s would often perform what was referred to as a ” head fake ” by changing directions rapidly just as pinky was approaching an intersection. by displacing the target tile just as pinky small model completion was coming to the left, they would get the ” correct ” direction. if the location is actually farther from what pac - man was at, the player ’ s will not respond to the ghost ’ s movement, and the ghost will instead return to the player ’ s current location. this is because pac - man does not move by turning. the ghost will have to go to another location of some kind to return where he began, and this is achieved in many ways. it will often simply leave the location, as for example if pac - man did jump off a cliff', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 15, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##ula, l. bauer, and m. k. reiter. adversarial generative nets : neural network attacks on state - of - the - art face recognition. arxiv preprint 1801. 00349, dec. 2017. url https : / / arxiv. org / abs / 1801. 00349. [ 3 ] g. singh, t. gehr, m. p¨uschel, and m. vechev. an abstract domain for certifying neural networks. proc. acm program. lang., 3 ( popl ), jan 2019. doi : 10. 1145 / 3290354. url https : / / doi. org / 10. 1145 / 3290354. 5 links to implementation reluplex : https : / / github. com / cybershiptrooper / marabou deeppoly https : / / github. com / taufeeque9 / eranl1 4', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}}, page_content='##ula, l. bauer, and m. k. reiter. adversarial generative nets : neural network attacks on state - of - the - art face recognition. arxiv preprint 1801. 00349, dec. 2017. url https : / / arxiv. org / abs / 1801. 00349. [ 3 ] g. singh, t. gehr, m. p¨uschel, and m. vechev. an abstract domain for certifying neural networks. proc. acm program. lang., 3 ( popl ), jan 2019. doi : 10. 1145 / 3290354. url https : / / doi. org / 10. 1145 / 3290354. 5 links to implementation reluplex : https : / / github. com / cybershiptrooper / marabou deeppoly https : / / github. com / taufeeque9 / eranl1 4', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'war. as an engineer, collins also had training in engi - neering from his native new york. after graduate studies in the union college of engineering, he was employed in the united states army for two years at the same time. there he worked under captain daniel pfeiffer and was promoted to colonel in may 1870, three years after captain pfeiffer ’ s enlistment. collins returned home to england after completing his ﬁrst year in the first division to serve the next two years in the army corps of engineers, and by his second year, he began to experience his service duty. ¡ — endoftext — ¿ moscow, july 5 ( reuters ) - u. k. - based russian state media and government sources and experts say moscow has sought to downplay the ” military buildup ” of the syrian oppo - sition while maintaining support for president donald trump ’ s plan to build a wall along the u. s. - mexico border, calling it a ” military coup ”. u. s. - led syria war veteran and syria envoy michael flynn ( r ), right, gestures with u. s. - based syrian political analyst ahmet koryat, with gpt - 2 completion in june of that year. sentenced to be shot for desertion, he remained on active duty. captain james j. dall was born in virginia in 1829 during the revolution and joined his father in the army in 1836. he served with the 2d virginia infantry until 1845, when he went to the western reserve regiment ; and, on his discharge, he reenlisted with company i, 25th new york volunteers ( which in 1845 numbered about 4, 500 men ). one of his brothers, james, served with his company in the same capacity in the same brigade. while the regiment remained in connecticut through 1846, captain dall served with company i at various times before his', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 16, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='war. as an engineer, collins also had training in engi - neering from his native new york. after graduate studies in the union college of engineering, he was employed in the united states army for two years at the same time. there he worked under captain daniel pfeiffer and was promoted to colonel in may 1870, three years after captain pfeiffer ’ s enlistment. collins returned home to england after completing his ﬁrst year in the first division to serve the next two years in the army corps of engineers, and by his second year, he began to experience his service duty. ¡ — endoftext — ¿ moscow, july 5 ( reuters ) - u. k. - based russian state media and government sources and experts say moscow has sought to downplay the ” military buildup ” of the syrian oppo - sition while maintaining support for president donald trump ’ s plan to build a wall along the u. s. - mexico border, calling it a ” military coup ”. u. s. - led syria war veteran and syria envoy michael flynn ( r ), right, gestures with u. s. - based syrian political analyst ahmet koryat, with gpt - 2 completion in june of that year. sentenced to be shot for desertion, he remained on active duty. captain james j. dall was born in virginia in 1829 during the revolution and joined his father in the army in 1836. he served with the 2d virginia infantry until 1845, when he went to the western reserve regiment ; and, on his discharge, he reenlisted with company i, 25th new york volunteers ( which in 1845 numbered about 4, 500 men ). one of his brothers, james, served with his company in the same capacity in the same brigade. while the regiment remained in connecticut through 1846, captain dall served with company i at various times before his', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 16, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '2 ) on the acasxu dataset. on a higher dimensional space like images, trace partitioning would take o ( npixels ) time, where n is the number of partitions per dimension. 3. 2 reluplex the l1 norm constraint on the entire image takes too long to run on reluplex. hence, we could only try the experiment on few images. we conduct 3 experiments for these few images : ( i ) perturb small regions only, giving explicit constraints for these regions. note that these perturbations are not a subset of l∞ if the norms are increased to the maximum change possible for each pixel. ( ii ) perturb small regions using the auxilliary networks instead. ( iii ) perturb entire image. we can further consider 3 types of norms in each experiment, namely, l∞, l1 and clipped norms given by lc = min ( l∞, l1 ), where l∞ ≤ δ, l1 ≤ [UNK], and [UNK] > δ. the benefit of lc can be seen by considering large l∞ and l1 norms as our search spaces : any l1 > 1 is not valid for images who ’ s values lie between 0 and 1, and large l∞ bounds cross the decision boundaries easily. the clipped norm manages to avoid both these issues. table 1 shows the robustness of our sparse network on different lc norms. since perturbing different regions using explicit constraints is limited to 3x4 pixels, the reluplex gives un - sat almost all the time. we run experiments ( i ) and ( ii ) for 173 regions. ( i ) was run with maximum l∞ bounds ( = 1 ) and no l1 restrictions and it gave unsat for all values. ( ii ) was run on a bigger region ( 16x16 ) 3', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}}, page_content='2 ) on the acasxu dataset. on a higher dimensional space like images, trace partitioning would take o ( npixels ) time, where n is the number of partitions per dimension. 3. 2 reluplex the l1 norm constraint on the entire image takes too long to run on reluplex. hence, we could only try the experiment on few images. we conduct 3 experiments for these few images : ( i ) perturb small regions only, giving explicit constraints for these regions. note that these perturbations are not a subset of l∞ if the norms are increased to the maximum change possible for each pixel. ( ii ) perturb small regions using the auxilliary networks instead. ( iii ) perturb entire image. we can further consider 3 types of norms in each experiment, namely, l∞, l1 and clipped norms given by lc = min ( l∞, l1 ), where l∞ ≤ δ, l1 ≤ [UNK], and [UNK] > δ. the benefit of lc can be seen by considering large l∞ and l1 norms as our search spaces : any l1 > 1 is not valid for images who ’ s values lie between 0 and 1, and large l∞ bounds cross the decision boundaries easily. the clipped norm manages to avoid both these issues. table 1 shows the robustness of our sparse network on different lc norms. since perturbing different regions using explicit constraints is limited to 3x4 pixels, the reluplex gives un - sat almost all the time. we run experiments ( i ) and ( ii ) for 173 regions. ( i ) was run with maximum l∞ bounds ( = 1 ) and no l1 restrictions and it gave unsat for all values. ( ii ) was run on a bigger region ( 16x16 ) 3', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''})]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(lc_kwargs={'page_content': 'in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='in many ways. gpt - 2 is also able to write news articles about the discovery of talking unicorns. an example is provided in table 13. 5. related work a signiﬁcant portion of this work measured the performance of larger language models trained on larger datasets. this', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 7, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='attention is all you need ashish vaswani∗ google brain avaswani @ google. com noam shazeer∗ google brain noam @ google. com niki parmar∗ google research nikip @ google. com jakob uszkoreit∗ google research usz @ google. com llion jones∗ google research llion @ google. com aidan n. gomez∗ † university of toronto aidan @ cs. toronto. edu łukasz kaiser∗ google brain lukaszkaiser @ google. com illia polosukhin∗ ‡ illia. polosukhin @ gmail. com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. the best performing models also connect the encoder and decoder through an attention mechanism. we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. our model achieves 28. 4 bleu on the wmt 2014 english - to - german translation task, improving over the existing best results, including ensembles, by over 2 bleu. on the wmt 2014 english - to - french translation task, our model establishes a new single - model state - of - the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. 1 introduction recurrent neural networks, long short - term memory', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'table 3 : variations on the transformer architecture. unlisted values are identical to those of the base model. all metrics are on the english - to - german translation development set, newstest2013. listed perplexities are per - wordpiece, according to our byte - pair encoding, and should not be compared to per - word perplexities. n dmodel dff h dk dv pdrop [UNK] train ppl bleu params steps ( dev ) ( dev ) ×106 base 6 512 2048 8 64 64 0. 1 0. 1 100k 4. 92 25. 8 65 ( a ) 1 512 512 5. 29 24. 9 4 128 128 5. 00 25. 5 16 32 32 4. 91 25. 8 32 16 16 5. 01 25. 4 ( b ) 16 5. 16 25. 1 58 32 5. 01 25. 4 60 ( c ) 2 6. 11 23. 7 36 4 5. 19 25. 3 50 8 4. 88 25. 5 80 256 32 32 5. 75 24. 5 28 1024 128 128 4. 66 26. 0 168 1024 5. 12 25. 4 53 4096 4. 75 26. 2 90 ( d ) 0. 0 5. 77 24. 6 0. 2 4. 95 25. 5 0. 0 4. 67 25. 3 0. 2 5. 47 25. 7 ( e ) positional embedding instead of sinusoids 4. 92 25. 7 big 6 1024 4096 16 0. 3 300k 4. 33 26. 4 213 table 4 : the transformer generalizes well to english constituency parsing ( results are on section 23 of wsj ) parser training wsj 23 f1 vinyals & kaiser el al. ( 2014 ) [ 37 ] wsj only,', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='table 3 : variations on the transformer architecture. unlisted values are identical to those of the base model. all metrics are on the english - to - german translation development set, newstest2013. listed perplexities are per - wordpiece, according to our byte - pair encoding, and should not be compared to per - word perplexities. n dmodel dff h dk dv pdrop [UNK] train ppl bleu params steps ( dev ) ( dev ) ×106 base 6 512 2048 8 64 64 0. 1 0. 1 100k 4. 92 25. 8 65 ( a ) 1 512 512 5. 29 24. 9 4 128 128 5. 00 25. 5 16 32 32 4. 91 25. 8 32 16 16 5. 01 25. 4 ( b ) 16 5. 16 25. 1 58 32 5. 01 25. 4 60 ( c ) 2 6. 11 23. 7 36 4 5. 19 25. 3 50 8 4. 88 25. 5 80 256 32 32 5. 75 24. 5 28 1024 128 128 4. 66 26. 0 168 1024 5. 12 25. 4 53 4096 4. 75 26. 2 90 ( d ) 0. 0 5. 77 24. 6 0. 2 4. 95 25. 5 0. 0 4. 67 25. 3 0. 2 5. 47 25. 7 ( e ) positional embedding instead of sinusoids 4. 92 25. 7 big 6 1024 4096 16 0. 3 300k 4. 33 26. 4 213 table 4 : the transformer generalizes well to english constituency parsing ( results are on section 23 of wsj ) parser training wsj 23 f1 vinyals & kaiser el al. ( 2014 ) [ 37 ] wsj only,', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##s were trained on. the supervised sota, a bert based system ( devlin et al., r - 1 r - 2 r - l r - avg bottom - up sum 41. 22 18. 68 38. 34 32. 75 lede - 3 40. 38 17. 66 36. 62 31. 55 seq2seq + attn 31. 33 11. 81 28. 83 23. 99 gpt - 2 tl ; dr : 29. 34 8. 27 26. 58 21. 40 random - 3 28. 78 8. 63 25. 52 20. 98 gpt - 2 no hint 21. 58 4. 03 19. 47 15. 03 table 4. summarization performance as measured by rouge f1 metrics on the cnn and daily mail dataset. bottom - up sum is the sota model from ( gehrmann et al., 2018 ) 2018 ), is nearing the 89 f1 performance of humans. while gpt - 2 ’ s performance is exciting for a system without any su - pervised training, some inspection of its answers and errors suggests gpt - 2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble sum', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##s were trained on. the supervised sota, a bert based system ( devlin et al., r - 1 r - 2 r - l r - avg bottom - up sum 41. 22 18. 68 38. 34 32. 75 lede - 3 40. 38 17. 66 36. 62 31. 55 seq2seq + attn 31. 33 11. 81 28. 83 23. 99 gpt - 2 tl ; dr : 29. 34 8. 27 26. 58 21. 40 random - 3 28. 78 8. 63 25. 52 20. 98 gpt - 2 no hint 21. 58 4. 03 19. 47 15. 03 table 4. summarization performance as measured by rouge f1 metrics on the cnn and daily mail dataset. bottom - up sum is the sota model from ( gehrmann et al., 2018 ) 2018 ), is nearing the 89 f1 performance of humans. while gpt - 2 ’ s performance is exciting for a system without any su - pervised training, some inspection of its answers and errors suggests gpt - 2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question. 3. 6. summarization we test gpt - 2 ’ s ability to perform summarization on the cnn and daily mail dataset ( nallapati et al., 2016 ). to in - duce summarization behavior we add the text tl ; dr : after the article and generate 100 tokens with top - k random sam - pling ( fan et al., 2018 ) with k = 2 which reduces repetition and encourages more abstractive summaries than greedy de - coding. we use the ﬁrst 3 generated sentences in these 100 tokens as the summary. while qualitatively the generations resemble sum', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 5, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='requiring a broad discourse context. arxiv preprint arxiv : 1606. 06031, 2016. pennington, j., socher, r., and manning, c. glove : global vectors for word representation. in proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp ), pp. 1532 – 1543, 2014.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 10, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'sweden, sweden table 17. selected coqa completion.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='sweden, sweden table 17. selected coqa completion.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 23, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##carf. only a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started. she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried. it could have been worse though. they could have still been in their house when the ﬁrst missile landed. ” our neighbor shouted to my husband ’ you have to leave, they ’ re coming. ’ and we just ran. as soon as we left the house, the ﬁrst missile fell right by it and then a second on it. it burned everything to the ground, ” qassim tells us... gpt - 2 : yemen is in the middle of a civil war. saudi arabia is leading the coalition bombing campaign. it ’ s been bombing yemen for more than two months now. reference : amina ali qassim ’ s family sought shelter in a mosque before ﬂeeing yemen. thousands like them are boarding boats to sail to djibouti. saudi arabia has been pounding yemen in a bid to defeat houthi rebels. table 14. summaries generated by gpt - 2 and reference summaries on the test set of the cnn and daily mail dataset.', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 4 : two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. top : full attentions for head 5. bottom : isolated attentions from just the word ‘ its ’ for attention heads 5 and 6. note that the attentions are very sharp for this word. 14', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}}, page_content='input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> input - input layer5 the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> the law will never be perfect, but its application should be just - this is what we are missing, in my opinion. < eos > <pad> figure 4 : two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. top : full attentions for head 5. bottom : isolated attentions from just the word ‘ its ’ for attention heads 5 and 6. note that the attentions are very sharp for this word. 14', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/transformers_vasvani.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20171207010315Z', 'modDate': 'D:20171207010315Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##nitely be an issue if you ’ re trying to work as a casual sneakermaker, and that ’ s ok but don ’ t hesitate if you want to try the lower price versions of their outﬁts! this version features a very low cut - cut neckline ( which is not a bad thing ), slightly longer legs, and a pair of long legs to keep things nice. my preferred color of color these items are blue - gray ( which is my ﬁrst colors, as they are a light white ), or black ( which is my favorite of my designs ). as always, my pick of colors from this store have nothing to do with that other than the de - sign. for a high end, i would recommend this to everyone! outﬁt 3 : high light this was a ” high light ” look while i had a chance to get some color. i think the color is based on gpt - 2 completion for me! outﬁt 2 : casual jacket i know that some people might be opposed to wearing sneakers in a turtleneck, but i wanted to be true to myself, so i went with a slim ﬁtting turtleneck in a color more similar to my favorite color of the day. overall, it ’ s a classic turtleneck, with nothing too ﬂashy. i did swap my shoes for these asics tiger killshots and i wish i hadn ’ t did that. the shoe itself is very comfortable, however, i found that wearing them in these sneakers made my feet look a bit larger. i ’ d wear them with denim, chinos, whatever, but would never recommend wearing them alone. there are just too many variables involved. outﬁt 3 : dress this is the ﬁnal outﬁt i wore the asics tiger killshots. this version of the asics is a much more modern and streamlined version. the shoe', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 13, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##nitely be an issue if you ’ re trying to work as a casual sneakermaker, and that ’ s ok but don ’ t hesitate if you want to try the lower price versions of their outﬁts! this version features a very low cut - cut neckline ( which is not a bad thing ), slightly longer legs, and a pair of long legs to keep things nice. my preferred color of color these items are blue - gray ( which is my ﬁrst colors, as they are a light white ), or black ( which is my favorite of my designs ). as always, my pick of colors from this store have nothing to do with that other than the de - sign. for a high end, i would recommend this to everyone! outﬁt 3 : high light this was a ” high light ” look while i had a chance to get some color. i think the color is based on gpt - 2 completion for me! outﬁt 2 : casual jacket i know that some people might be opposed to wearing sneakers in a turtleneck, but i wanted to be true to myself, so i went with a slim ﬁtting turtleneck in a color more similar to my favorite color of the day. overall, it ’ s a classic turtleneck, with nothing too ﬂashy. i did swap my shoes for these asics tiger killshots and i wish i hadn ’ t did that. the shoe itself is very comfortable, however, i found that wearing them in these sneakers made my feet look a bit larger. i ’ d wear them with denim, chinos, whatever, but would never recommend wearing them alone. there are just too many variables involved. outﬁt 3 : dress this is the ﬁnal outﬁt i wore the asics tiger killshots. this version of the asics is a much more modern and streamlined version. the shoe', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 13, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}}, page_content=\"quiz this quiz is about kant's philosophy and the moral implications of killing. answer the questions below to test your knowledge. 1. what is the reason for the emergence of morality? evolution logic compassion materialism 2. what is the consequence of not being'compatible'with other humans? death isolation rejection abandonment 3. what is the base for altruism? probabilistic reason logic evolution submit\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_22_05_23.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615163523+00'00'\", 'modDate': \"D:20230615163523+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##1. 06440, 2015. trichelair, p., emami, a., cheung, j. c. k., trischler, a., sule - man, k., and diaz, f. on the evaluation of common - sense reasoning in natural language understanding. arxiv preprint arxiv : 1811. 01778, 2018. trinh, t. h. and le, q. v. a simple method for commonsense reasoning. arxiv preprint arxiv : 1806. 02847, 2018. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, ł., and polosukhin, i. attention is all you need. in advances in neural information processing systems, pp. 5998 – 6008, 2017. vinyals, o. and le, q. a neural conversational model. arxiv preprint arxiv : 1506. 05869, 2015. vinyals, o., fortunato, m., and jaitly, n. pointer networks. in advances in neural information processing systems, pp. 2692 – 2700, 2015. wang, a., singh, a., michael, j., hill, f., levy, o., and bow - man, s. r. glue : a multi - task benchmark and analysis platform for natural language understanding. arxiv preprint arxiv : 1804. 07461, 2018. weston, j. e. dialog - based language learning. in advances in neural information processing systems, pp. 829 – 837, 2016. wieting, j. and kiel', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 11, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='##1. 06440, 2015. trichelair, p., emami, a., cheung, j. c. k., trischler, a., sule - man, k., and diaz, f. on the evaluation of common - sense reasoning in natural language understanding. arxiv preprint arxiv : 1811. 01778, 2018. trinh, t. h. and le, q. v. a simple method for commonsense reasoning. arxiv preprint arxiv : 1806. 02847, 2018. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l., gomez, a. n., kaiser, ł., and polosukhin, i. attention is all you need. in advances in neural information processing systems, pp. 5998 – 6008, 2017. vinyals, o. and le, q. a neural conversational model. arxiv preprint arxiv : 1506. 05869, 2015. vinyals, o., fortunato, m., and jaitly, n. pointer networks. in advances in neural information processing systems, pp. 2692 – 2700, 2015. wang, a., singh, a., michael, j., hill, f., levy, o., and bow - man, s. r. glue : a multi - task benchmark and analysis platform for natural language understanding. arxiv preprint arxiv : 1804. 07461, 2018. weston, j. e. dialog - based language learning. in advances in neural information processing systems, pp. 829 – 837, 2016. wieting, j. and kiel', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 11, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'language models are unsupervised multitask learners context ( webtext test ) pinky the pink ghost ’ s ai is designed to ” feel ” opposite of the red ghost ’ s behavior. pinky actually attempts to get out in front of pac - man. this is accomplished by setting the target 4 tiles ahead of pac - man ’ s current location in the direction that pac - man is travelling. one exception to this is when pac - man is traveling up. due to an overﬂow bug in the code, the calculation includes a left offset equal to the expected up offset. the use of pac - man ’ s expected position allows the player to exploit the navigation code for pinky. this was most often seen in situations where pac - man and pinky are in close proximity to each other. for example, if they were headed toward one another, once they are less than 4 tiles apart, the target tile is now behind pinky. if he encounters an intersection while in this conﬁguration, he will attempt to double - back ( by turning ) in order to reach the target tile. player ’ s would often perform what was referred to as a ” head fake ” by changing directions rapidly just as pinky was approaching an intersection. by displacing the target tile just as pinky small model completion was coming to the left, they would get the ” correct ” direction. if the location is actually farther from what pac - man was at, the player ’ s will not respond to the ghost ’ s movement, and the ghost will instead return to the player ’ s current location. this is because pac - man does not move by turning. the ghost will have to go to another location of some kind to return where he began, and this is achieved in many ways. it will often simply leave the location, as for example if pac - man did jump off a cliff', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 15, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='language models are unsupervised multitask learners context ( webtext test ) pinky the pink ghost ’ s ai is designed to ” feel ” opposite of the red ghost ’ s behavior. pinky actually attempts to get out in front of pac - man. this is accomplished by setting the target 4 tiles ahead of pac - man ’ s current location in the direction that pac - man is travelling. one exception to this is when pac - man is traveling up. due to an overﬂow bug in the code, the calculation includes a left offset equal to the expected up offset. the use of pac - man ’ s expected position allows the player to exploit the navigation code for pinky. this was most often seen in situations where pac - man and pinky are in close proximity to each other. for example, if they were headed toward one another, once they are less than 4 tiles apart, the target tile is now behind pinky. if he encounters an intersection while in this conﬁguration, he will attempt to double - back ( by turning ) in order to reach the target tile. player ’ s would often perform what was referred to as a ” head fake ” by changing directions rapidly just as pinky was approaching an intersection. by displacing the target tile just as pinky small model completion was coming to the left, they would get the ” correct ” direction. if the location is actually farther from what pac - man was at, the player ’ s will not respond to the ghost ’ s movement, and the ghost will instead return to the player ’ s current location. this is because pac - man does not move by turning. the ghost will have to go to another location of some kind to return where he began, and this is achieved in many ways. it will often simply leave the location, as for example if pac - man did jump off a cliff', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 15, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '##ula, l. bauer, and m. k. reiter. adversarial generative nets : neural network attacks on state - of - the - art face recognition. arxiv preprint 1801. 00349, dec. 2017. url https : / / arxiv. org / abs / 1801. 00349. [ 3 ] g. singh, t. gehr, m. p¨uschel, and m. vechev. an abstract domain for certifying neural networks. proc. acm program. lang., 3 ( popl ), jan 2019. doi : 10. 1145 / 3290354. url https : / / doi. org / 10. 1145 / 3290354. 5 links to implementation reluplex : https : / / github. com / cybershiptrooper / marabou deeppoly https : / / github. com / taufeeque9 / eranl1 4', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}}, page_content='##ula, l. bauer, and m. k. reiter. adversarial generative nets : neural network attacks on state - of - the - art face recognition. arxiv preprint 1801. 00349, dec. 2017. url https : / / arxiv. org / abs / 1801. 00349. [ 3 ] g. singh, t. gehr, m. p¨uschel, and m. vechev. an abstract domain for certifying neural networks. proc. acm program. lang., 3 ( popl ), jan 2019. doi : 10. 1145 / 3290354. url https : / / doi. org / 10. 1145 / 3290354. 5 links to implementation reluplex : https : / / github. com / cybershiptrooper / marabou deeppoly https : / / github. com / taufeeque9 / eranl1 4', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 3, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''})],\n",
       " [Document(lc_kwargs={'page_content': \"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}}, page_content=\"global warming blog global warming is the gradual increase in the average temperature of the earth's atmosphere and oceans. it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm. this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation. the [UNK] of global warming are far - reaching and wide - ranging. rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme. this can lead to more frequent and intense storms, ﬂoods, droughts, and heat waves. it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity. the good news is that global warming can be slowed and even reversed. we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy [UNK]. we can also protect and restore forests, which absorb carbon dioxide from the atmosphere. by taking these steps, we can help to reduce the [UNK] of global warming and create a more sustainable future for our planet.\", metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/15_06_2023_19_09_38.pdf', 'page': 0, 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'Chromium', 'producer': 'Skia/PDF m108', 'creationDate': \"D:20230615133938+00'00'\", 'modDate': \"D:20230615133938+00'00'\", 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='and details preserved in the original cave. article : ‘ if you can ﬁnd my submarine, it ’ s yours, ’ russian oil billionaire roman abramovich once said. and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his £300million superyacht eclipse ( perfect for getting to shore undetected ), is not the only wealthy businessman splashing out on underwater exploration. dubbed earth ’ s real ‘ ﬁnal frontier ’, the oceans are still so little - explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world ’ s oceans. so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these... whale of a time : the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration... gpt - 2 : the u - boat worx submarine can dive to 984 feet ( 300 metres ) and is capable of diving to the bottom of the ocean. it can be used for diving, snorkelling and even snorkel diving. the submarine is also equipped with a snorkelling platform. reference : feel like a james bond villain with the most advanced submerged ocean vehicles. some convert from a yacht into a submarine, and others detached off for a speedy exploration. oliver ’ s travels offers a mile low package on its submarine, complete with chef and butler for £175, 000 a night. article : amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf. only a few months old, this is the baby girl whose ears she desperately tried to', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 20, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': 'war. as an engineer, collins also had training in engi - neering from his native new york. after graduate studies in the union college of engineering, he was employed in the united states army for two years at the same time. there he worked under captain daniel pfeiffer and was promoted to colonel in may 1870, three years after captain pfeiffer ’ s enlistment. collins returned home to england after completing his ﬁrst year in the first division to serve the next two years in the army corps of engineers, and by his second year, he began to experience his service duty. ¡ — endoftext — ¿ moscow, july 5 ( reuters ) - u. k. - based russian state media and government sources and experts say moscow has sought to downplay the ” military buildup ” of the syrian oppo - sition while maintaining support for president donald trump ’ s plan to build a wall along the u. s. - mexico border, calling it a ” military coup ”. u. s. - led syria war veteran and syria envoy michael flynn ( r ), right, gestures with u. s. - based syrian political analyst ahmet koryat, with gpt - 2 completion in june of that year. sentenced to be shot for desertion, he remained on active duty. captain james j. dall was born in virginia in 1829 during the revolution and joined his father in the army in 1836. he served with the 2d virginia infantry until 1845, when he went to the western reserve regiment ; and, on his discharge, he reenlisted with company i, 25th new york volunteers ( which in 1845 numbered about 4, 500 men ). one of his brothers, james, served with his company in the same capacity in the same brigade. while the regiment remained in connecticut through 1846, captain dall served with company i at various times before his', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 16, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}}, page_content='war. as an engineer, collins also had training in engi - neering from his native new york. after graduate studies in the union college of engineering, he was employed in the united states army for two years at the same time. there he worked under captain daniel pfeiffer and was promoted to colonel in may 1870, three years after captain pfeiffer ’ s enlistment. collins returned home to england after completing his ﬁrst year in the first division to serve the next two years in the army corps of engineers, and by his second year, he began to experience his service duty. ¡ — endoftext — ¿ moscow, july 5 ( reuters ) - u. k. - based russian state media and government sources and experts say moscow has sought to downplay the ” military buildup ” of the syrian oppo - sition while maintaining support for president donald trump ’ s plan to build a wall along the u. s. - mexico border, calling it a ” military coup ”. u. s. - led syria war veteran and syria envoy michael flynn ( r ), right, gestures with u. s. - based syrian political analyst ahmet koryat, with gpt - 2 completion in june of that year. sentenced to be shot for desertion, he remained on active duty. captain james j. dall was born in virginia in 1829 during the revolution and joined his father in the army in 1836. he served with the 2d virginia infantry until 1845, when he went to the western reserve regiment ; and, on his discharge, he reenlisted with company i, 25th new york volunteers ( which in 1845 numbered about 4, 500 men ). one of his brothers, james, served with his company in the same capacity in the same brigade. while the regiment remained in connecticut through 1846, captain dall served with company i at various times before his', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_responses/gpt2.pdf', 'page': 16, 'total_pages': 24, 'format': 'PDF 1.5', 'title': 'Language Models are Unsupervised Multitask Learners', 'author': 'Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei**, Ilya Sutskever**', 'subject': 'Proceedings of the International Conference on Machine Learning 2019', 'keywords': 'Machine Learning, ICML', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.18', 'creationDate': 'D:20190214232139Z', 'modDate': 'D:20190214232139Z', 'trapped': ''}),\n",
       "  Document(lc_kwargs={'page_content': '2 ) on the acasxu dataset. on a higher dimensional space like images, trace partitioning would take o ( npixels ) time, where n is the number of partitions per dimension. 3. 2 reluplex the l1 norm constraint on the entire image takes too long to run on reluplex. hence, we could only try the experiment on few images. we conduct 3 experiments for these few images : ( i ) perturb small regions only, giving explicit constraints for these regions. note that these perturbations are not a subset of l∞ if the norms are increased to the maximum change possible for each pixel. ( ii ) perturb small regions using the auxilliary networks instead. ( iii ) perturb entire image. we can further consider 3 types of norms in each experiment, namely, l∞, l1 and clipped norms given by lc = min ( l∞, l1 ), where l∞ ≤ δ, l1 ≤ [UNK], and [UNK] > δ. the benefit of lc can be seen by considering large l∞ and l1 norms as our search spaces : any l1 > 1 is not valid for images who ’ s values lie between 0 and 1, and large l∞ bounds cross the decision boundaries easily. the clipped norm manages to avoid both these issues. table 1 shows the robustness of our sparse network on different lc norms. since perturbing different regions using explicit constraints is limited to 3x4 pixels, the reluplex gives un - sat almost all the time. we run experiments ( i ) and ( ii ) for 173 regions. ( i ) was run with maximum l∞ bounds ( = 1 ) and no l1 restrictions and it gave unsat for all values. ( ii ) was run on a bigger region ( 16x16 ) 3', 'metadata': {'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''}}, page_content='2 ) on the acasxu dataset. on a higher dimensional space like images, trace partitioning would take o ( npixels ) time, where n is the number of partitions per dimension. 3. 2 reluplex the l1 norm constraint on the entire image takes too long to run on reluplex. hence, we could only try the experiment on few images. we conduct 3 experiments for these few images : ( i ) perturb small regions only, giving explicit constraints for these regions. note that these perturbations are not a subset of l∞ if the norms are increased to the maximum change possible for each pixel. ( ii ) perturb small regions using the auxilliary networks instead. ( iii ) perturb entire image. we can further consider 3 types of norms in each experiment, namely, l∞, l1 and clipped norms given by lc = min ( l∞, l1 ), where l∞ ≤ δ, l1 ≤ [UNK], and [UNK] > δ. the benefit of lc can be seen by considering large l∞ and l1 norms as our search spaces : any l1 > 1 is not valid for images who ’ s values lie between 0 and 1, and large l∞ bounds cross the decision boundaries easily. the clipped norm manages to avoid both these issues. table 1 shows the robustness of our sparse network on different lc norms. since perturbing different regions using explicit constraints is limited to 3x4 pixels, the reluplex gives un - sat almost all the time. we run experiments ( i ) and ( ii ) for 173 regions. ( i ) was run with maximum l∞ bounds ( = 1 ) and no l1 restrictions and it gave unsat for all values. ( ii ) was run on a bigger region ( 16x16 ) 3', metadata={'source': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'file_path': '/Users/rohangupta/work/web_api_server/storage/test_pdfs/CS_781_Project.pdf', 'page': 2, 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.23', 'creationDate': 'D:20230425114654Z', 'modDate': 'D:20230425114654Z', 'trapped': ''})]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vectordb.max_marginal_relevance_search(query, top_k=5, lambda_mult=0.0) for query in queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
