{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "sys.path.append('../')\n",
    "from utils.nlp_utils import *\n",
    "# abs path of ../\n",
    "dir = os.path.abspath(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/rohangupta/miniconda3/envs/gen_ai_hackweek/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, OpenAI\n",
    "import openai\n",
    "import os\n",
    "# ctfidf_model = ClassTfidfTransformer()\n",
    "# topic_model = BERTopic(ctfidf_model=ctfidf_model )\n",
    "# set openai api key\n",
    "\n",
    "kbm = KeyBERTInspired()\n",
    "openai.api_key = \"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\"\n",
    "gpt = OpenAI()\n",
    "# need to fit this to some data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from bertopic.vectorizers import OnlineCountVectorizer, ClassTfidfTransformer\n",
    "\n",
    "def init_bertopic_model(num_topics=5, ngram_range=(1,4)):\n",
    "   # Prepare sub-models that support online learning\n",
    "   umap_model = IncrementalPCA(n_components=10)\n",
    "   cluster_model = MiniBatchKMeans(n_clusters=num_topics, random_state=0)\n",
    "   vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01)\n",
    "   topic_model = BERTopic(\n",
    "                     representation_model=kbm,\n",
    "                     # ctfidf_model= ClassTfidfTransformer(),\n",
    "                     umap_model=BaseDimensionalityReduction(),\n",
    "                     hdbscan_model=cluster_model,\n",
    "                     vectorizer_model=vectorizer_model, \n",
    "                     n_gram_range=ngram_range,\n",
    "                  )\n",
    "   return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_from_topics(topic, topic_model, docs):\n",
    "    T = topic_model.get_document_info(docs)\n",
    "    docs_per_topics = T.groupby([\"Topic\"]).apply(lambda x: x.index).to_dict()\n",
    "    return docs_per_topics[topic]\n",
    "\n",
    "def get_topk_docs(topics, scores, topic_model: BERTopic, docs, k=5):\n",
    "    distribs, _ = topic_model.approximate_distribution(docs)\n",
    "    topic_vector = np.zeros((len(topic_model.get_topics())))\n",
    "    topic_vector[topics] = scores\n",
    "    dists = np.dot(distribs, topic_vector)\n",
    "    idx = np.argsort(dists)[::-1][:k]\n",
    "    return [(i, docs[i], dists[i]) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceTransformersTokenTextSplitter(tokens_per_chunk=128, chunk_overlap=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\")\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "documents = loader.load()\n",
    "texts = text_splitter.split_documents(documents)\n",
    "full_doc = \"\\n\".join([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [text.page_content for text in texts]\n",
    "doc_list = [doc.page_content for doc in documents]\n",
    "text_list_processed = process_data(text_list)\n",
    "len(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = init_bertopic_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x2e80298e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.partial_fit(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 3, 4, 0, 2],\n",
       " [0.3991175, 0.39850435, 0.38054556, 0.35410535, 0.34186167],\n",
       " [' com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder the best performing models also connect the encoder and decoder through an attention mechanism we propose a new simple network architecture, the transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signi cantly less time to train our model achieves 28 4 bleu on',\n",
       "  'training data 1 introduction recurrent neural networks, long short term memory 13 and gated recurrent 7 neural networks in particular, have been rmly established as state of the art approaches in sequence modeling and equal contribution listing order is random jakob proposed replacing rnns with self attention and started the effort to evaluate this idea ashish, with illia, designed and implemented the rst transformer models and has been crucially involved in every aspect of this work noam proposed scaled dot product attention, multi head attention and the parameter free position representation and became the other',\n",
       "  'representation and became the other person involved in nearly every detail niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor llion also experimented with novel model variants, was responsible for our initial codebase, and ef cient inference and visualizations lukasz and aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research work performed while at google brain work performed while at google research 31st conference on neural information processing systems nips 2017 , long',\n",
       "  'attention is all you need ashish vaswani google brain avaswani google com noam shazeer google brain noam google com niki parmar google research nikip google com jakob uszkoreit google research usz google com llion jones google research llion google com aidan n gomez university of toronto aidan cs toronto edu ukasz kaiser google brain lukaszkaiser google com illia polosukhin illia polosukhin gmail com abstract the dominant',\n",
       "  '4 bleu on the wmt 2014 english to german translation task, improving over the existing best results, including ensembles, by over 2 bleu on the wmt 2014 english to french translation task, our model establishes a new single model state of the art bleu score of 41 8 after training for 3 5 days on eight gpus, a small fraction of the training costs of the best models from the literature we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data 1 introduction'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, scores = topic_model.find_topics(\"transformer causal attention multi-head\", top_n=5)\n",
    "idx, scores, [text_list_processed[i] for i in idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5 Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = [os.path.join(dir, \"storage/test_pdfs/transformers_vasvani.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_pdfs/CS_781_Project.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/test_html2pdf_out.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_19_09_38.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/15_06_2023_22_05_23.pdf\"),\n",
    "              os.path.join(dir, \"storage/test_responses/gpt2.pdf\"),]\n",
    "\n",
    "text_list = []\n",
    "for file_path in file_paths:\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    text_list += [text.page_content for text in texts]\n",
    "\n",
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list_processed = process_data(text_list)\n",
    "len(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = init_bertopic_model(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x2e9609be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.partial_fit(text_list_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 4, 9, 3, 2],\n",
       " [0.15003857, 0.13568193, 0.13269934, 0.13221404, 0.1013174],\n",
       " [' r and decoder, shown in the left and right halves of figure 1, respectively 2',\n",
       "  'global warming blog global warming is the gradual increase in the average temperature of the earths atmosphere and oceans it is caused by the emission of greenhouse gases, such as carbon dioxide, which trap heat in the atmosphere and cause the earth to warm this phenomenon has been occurring since the industrial revolution, and has been accelerating in recent decades due to human activities such as burning fossil fuels and deforestation the UNK of global warming are far reaching and wide ranging rising temperatures are causing sea levels to rise, glaciers to melt, and weather patterns to become more extreme this can lead to more frequent and intense storms, oods',\n",
       "  'storms, oods, droughts, and heat waves it can also cause changes in ecosystems, such as the spread of disease, the migration of species, and the loss of biodiversity the good news is that global warming can be slowed and even reversed we can reduce our emissions of greenhouse gases by switching to renewable energy sources, such as solar and wind power, and by improving energy UNK we can also protect and restore forests, which absorb carbon dioxide from the atmosphere by taking these steps, we can help to reduce the UNK of global warming and create a more sustainable future for our planet ',\n",
       "  'quiz this quiz is about kants philosophy and the moral implications of killing answer the questions below to test your knowledge 1 what is the reason for the emergence of morality evolution logic compassion materialism 2 what is the consequence of not beingcompatiblewith other humans death isolation rejection abandonment 3 what is the base for altruism probabilistic reason logic evolution submit',\n",
       "  'data issues in our initial experiments with i m not the cleverest man in the world, but like they say in french je ne suis pas un imbecile i m not a fool in a now deleted post from aug 16, soheil eid, tory candidate in the riding of joliette, wrote in french mentez mentez, il en restera toujours quelque chose, which translates as, lie lie and something will always remain i hate the word perfume, burr says it ',\n",
       "  'says it s somewhat better in french parfum if listened carefully at 29 55, a conversation can be heard between two guys in french comment on fait pour aller de l autre cot e quel autre cot e , which means how do you get to the other side what side if this sounds like a bit of a stretch, consider this ques tion in french as tu aller au cin ema , or did you go to the movies , which literally translates',\n",
       "  '21 58 4 03 19 47 15 03 table 4 summarization performance as measured by rouge f1 metrics on the cnn and daily mail dataset bottom up sum is the sota model from gehrmann et al , 2018 2018 , is nearing the 89 f1 performance of humans while gpt 2 s performance is exciting for a system without any su pervised training, some inspection of its answers and errors suggests gpt 2 often uses simple retrieval based heuristics such as answer with a name from the document in response to a who question 3 ',\n",
       "  'who question 3 6 summarization we test gpt 2 s ability to perform summarization on the cnn and daily mail dataset nallapati et al , 2016 to in duce summarization behavior we add the text tl dr after the article and generate 100 tokens with top k random sam pling fan et al , 2018 with k 2 which reduces repetition and encourages more abstractive summaries than greedy de coding we use the rst 3 generated sentences in these 100 tokens as the summary while qualitative',\n",
       "  ' while qualitatively the generations resemble summaries, as shown in table 14, they often focus on recent content from the article or confuse speci c details such as how many cars were involved in a crash or whether a logo was on a hat or shirt on the commonly reported rouge 1, 2, l metrics the generated summaries only begin to approach the performance of classic neural baselines and just barely outperforms selecting 3 random sentences from the article gpt 2 s performance drops by 6 4 points on the aggregate metric when the task hint is removed which demonstrates the ability to invo',\n",
       "  'language models are unsupervised multitask learners question generated answer correct probability who wrote the book the origin of species charles darwin 83 4 who is the founder of the ubuntu project mark shuttleworth 82 0 who is the quarterback for the green bay packers aaron rodgers 81 1 panda is a national animal of which country china 76 8 who came up with the theory of relativity albert einstein 76 4 when was the rst star wars lm released 1977 71 4 what is the most common blood type in sweden a 70 6 who',\n",
       "  '70 6 who is regarded as the founder of psychoanalysis sigmund freud 69 3 who took the rst steps on the moon in 1969 neil armstrong 66 8 who is the largest supermarket chain in the uk tesco 65 3 what is the meaning of shalom in english peace 64 0 who was the author of the art of war sun tzu 59 6 largest state in the us by land mass california 59 2 green algae is an example of which type of reproduction parthenogenesis 56 5 vikram samvat calender is of',\n",
       "  ' vat calender is of cial in which country india 55 6 who is mostly responsible for writing the declaration of independence thomas jefferson 53 3 what us state forms the western boundary of montana montana 52 3 who plays ser davos in game of thrones peter dinklage 52 1 who appoints the chair of the federal reserve system janet yellen 51 5 state the process that divides one nucleus into two genetically identical nuclei mitosis 50 7 who won the most mvp awards in the nba michael jordan 50 2 what river is associated with the city of',\n",
       "  'power plant that blew up in russia chernobyl 45 7 who played john connor in the original terminator arnold schwarzenegger 45 2 table 5 the 30 most con dent answers generated by gpt 2 on the development set of natural questions sorted by their probability according to gpt 2 none of these questions appear in webtext according to the procedure described in section 4 conneau et al , 2017b on the wmt 14 french english test set, gpt 2 is able to leverage its very strong english language model to perform signi can',\n",
       "  ' le, 2015 reported qualitative results due to the lack of high quality evaluation datasets the recently introduced natural questions dataset kwiatkowski et al , 2https github com cld2owners cld2 2019 is a promising resource to test this more quantita tively similar to translation, the context of the language model is seeded with example question answer pairs which helps the model infer the short answer style of the dataset gpt 2 answers 4 1 of questions correctly when evalu ate',\n",
       "  'calibrated and gpt 2 has an accuracy of 63 1 on the 1 of questions it is most con dent in the 30 most con dent answers generated by gpt 2 on develop ment set questions are shown in table 5 the performance of gpt 2 is still much, much, worse than the 30 to 50 range of open domain question answering systems which hybridize information retrieval with extractive document question answering alberti et al , 2019 3alec, who previously thought of himself as good at random trivia, answered 17 of 100 randomly sampled examples correctly',\n",
       "  '100 randomly sampled examples correctly when tested in the same setting as gpt 2 he actually only got 14 right but he should have gotten those other 3',\n",
       "  ' rst sentence and a half of the gettysburg address which occurs approximately 40 times throughout webtext , an argmax decode from gpt 2 recovers the speech even when sampling without truncation, we nd that the model copies the speech for awhile before drifting, albeit in a similar style it typically drifts within 100 200 tokens, and displays widening diversity once it drifts to quantify how often exact memorization shows up in samples, we generated samples from gpt 2 conditioned on webtext test set articles and compared the overlap rates of gpt 2 s generations to the overlap',\n",
       "  'use ful for many reasons, both for pac man and his ai unlike most ghost games, this ghost simply travels in the direction from gpt 2 completion was about to cross the intersection, pac man would be able to dodge the ghosts projectiles and return to the safe location of the safe house buster the yellow ghost s ai is speci cally designed to try to avoid pac man from leaving the safe house the player must press the space bar to reveal the safe house s doors, then go into them to nd the yellow ghost hiding in the next room the player must',\n",
       "  ' moscow, july 5 reuters u k based russian state media and government sources and experts say moscow has sought to downplay the military buildup of the syrian oppo sition while maintaining support for president donald trump s plan to build a wall along the u s mexico border, calling it a military coup u s led syria war veteran and syria envoy michael flynn r , right, gestures with u s based syrian political analyst ahmet koryat, with gpt 2 completion in june of that year sentenced to',\n",
       "  'or at least a change in social organization, said the scientist table 13 conditional generation on an out of distribution context by gpt 2 cherry pick of 10 samples generated with k 40 ',\n",
       "  'original cave article if you can nd my submarine, it s yours, russian oil billionaire roman abramovich once said and it seems the oligarch and chelsea fc owner, whose submarine is just one of the extras that came with his 300million superyacht eclipse perfect for getting to shore undetected , is not the only wealthy businessman splashing out on underwater exploration dubbed earth s real nal frontier , the oceans are still so little explored that billionaires are queuing up to buy vessels that give them a glimpse of the dramatic seascapes and incredible',\n",
       "  'used for diving, snorkelling and even snorkel diving the submarine is also equipped with a snorkelling platform reference feel like a james bond villain with the most advanced submerged ocean vehicles some convert from a yacht into a submarine, and others detached off for a speedy exploration oliver s travels offers a mile low package on its submarine, complete with chef and butler for 175, 000 a night article amina ali qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf only a few months old, this is the baby girl whose',\n",
       "  'is the baby girl whose ears she desperately tried to cover the night the aerial bombardment started she lay awake, she says, in a village mosque on the yemeni island of birim, counting explosions as the baby cried it could have been worse though they could have still been in their house when the rst missile landed our neighbor shouted to my husband you have to leave, they re coming and we just ran as soon as we left the house, the rst missile fell right by it and then a second on it it burned everything to the ground, qassim tells us ',\n",
       "  ' assim tells us gpt 2 yemen is in the middle of a civil war saudi arabia is leading the coalition bombing campaign it s been bombing yemen for more than two months now reference amina ali qassim s family sought shelter in a mosque before eeing yemen thousands like them are boarding boats to sail to djibouti saudi arabia has been pounding yemen in a bid to defeat houthi rebels table 14 summaries generated by gpt 2 and reference summaries on the test set of the cnn and daily mail dataset ',\n",
       "  ' french reference gpt 2 english translation un homme expliquait que le fonctionnement de la hernia fonctionnelle qu il avait UNK avant de faire, le fonc tionnement de la hernia fonctionnelle que j ai r eussi, j ai r eussi this release, originally released on three discs the origi nal album, sessions studio in edites and remixes, a dvd containing the four clips that were released on the previous day english reference gpt 2 french translation one man',\n",
       "  '2 french translation one man explained that the free hernia surgery he d re ceived will allow him to work again un homme expliquait que le fonctionnement de la hernia fonctionnelle qu il avait UNK avant de faire, le fonc tionnement de la hernia fonctionnelle que j ai r eussi, j ai r eussi french reference gpt 2 english translation un homme a expliqu e que l op eration gratuite qu il avait sub',\n",
       "  ' il avait subie pour soigner une hernie lui permettrait de travailler a nouveau a man told me that the operation gratuity he had been promised would not allow him to travel english reference gpt 2 french translation speaking to an open government conference in london via video link, mr kerry said there is no question that the president and i and others in government have actually learned of some things that had been happening on an automatic pilot because the ability has been there, going back to world war two and to the very dif cult years of the cold war',\n",
       "  ' 2 english translation s exprimant lors d une conf erence intergouvernementale a londres par liaison vid eo, m kerry a d eclar e il est ind eniable que le pr esident, moi UNK et d autres mem bres du gouvernement avons pris connaissance de cer taines choses en mode pilote automatique parce que nous en avions la possibilit e, d es la seconde guerre mon',\n",
       "  'la seconde guerre mondiale et jusqu aux ann ees dif ciles de la guerre froide, puis bien UNK le 11 septembre in a conf erence between the united states and london, secretary of state john kerry said it is indeniable that the president, myself and others of the government have been aware of certain certain choices that have been made in the past in order to be able to do certain things in a more automated way table 15 english to french and french to english translations generated by gpt 2 '])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"write a blog on how is gpt2 different from vaswani's transformers\"\n",
    "idx, scores = topic_model.find_topics(query, top_n=5)\n",
    "idx, scores, [text_list_processed[i] for i in get_docs_from_topics(idx[0], topic_model, text_list_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 1, 9, 7, 0],\n",
       " [0.1682396, 0.16194534, 0.14726132, 0.14177978, 0.12215993],\n",
       " [' ps 2017 , long beach, ca, usa arxiv 1706 03762v5 cs cl 6 dec 2017',\n",
       "  ' 1020 convs2s 9 25 16 40 46 9 6 1018 1 5 1020 moe 32 26 03 40 56 2 0 1019 1 2 1020 deep att posunk ensemble 39 40 4 8 0 1020 gnmt rl ensemble 38 26 30 41 16 1 8 1020 1 1 1021 convs2s ensemble 9 26 36 41 29 7 7 1019 1 2 1021 transformer ',\n",
       "  ' 92 25 8 65 a 1 512 512 5 29 24 9 4 128 128 5 00 25 5 16 32 32 4 91 25 8 32 16 16 5 01 25 4 b 16 5 16 25 1 58 32 5 01 25 4 60 c 2 6 11 23 7 36 4 5 19 25 3 50 8 4 88 25 5 80 256 32 32 5 75 24 5 28 1024 128 128 4 66 26 0 168 1024 5 12 25 4 53 4096 4 75 26',\n",
       "  ' 65 83 4 29 41 65 85 1 16 1 17 37 50 75 20 345m 15 60 55 48 92 35 87 1 22 76 47 33 1 01 1 06 26 37 55 72 762m 10 87 60 12 93 45 88 0 19 93 40 31 0 97 1 02 22 05 44 575 1542m 8 63 63 24 93 30 89 05 18 34 35 76 0 93 0 98 17 48 42 16 table 3 zero shot',\n",
       "  'language models are unsupervised multitask learners context webtext test aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so i threw some of those in between the layers i also had a few peppermint jo jos on hand so i crushed them up and threw some of those in along with some crushed meringue cookies because, why not it s a total smorgasbord of minty chocolate chippy cookie crunchy goodness i didn t measure how much of each topping i used, but after i tasted the nished product, i',\n",
       "  ' nished product, i wish i had added more you can add anything you want crushed candy canes, peppermint bark, etc and don t be afraid to use a heavy hand texture good if you don t have 7 inch cake pans, you can get 3 shorter 8 inch layers out of this gpt 2 completion 1 recipe i made the longer version, then used the pan for a layer of buttercream frosting and stuck 1 3 of it into the fridge for a frosting topper just make sure you have all of this frosting prepared earlier than',\n",
       "  'frosting prepared earlier than you make this cake and you won t have to wait on it as your egg whites sit on the bottom of your mixing bowl you want the egg whites whipped to stiff peaks then put your cake dish over a pot of simmering water and heat it up until it s just barely warm it doesn t take more than 3 5 minutes just enough to get it started gpt 2 completion 2 recipe if you do, and you know how to make that cake, then your life will never be the same i have two very simple rules when it comes to cookie crumbs',\n",
       "  'to cookie crumbs or chocolate chips 1 when i buy raw sugar, i do not use it until it s cooled down and has at least half a teaspoon of sugar on it that s my rule and 2 if i have a recipe that calls for something and i want to use it in a recipe, i do not substitute it with something else on line like this i found this really cute card with cute little kittens on gpt 2 completion 3 recipe, which will likely yield about the same amount of cake chocolate buttermilk cake print chunks of chocolate cake, coated',\n",
       "  'of chocolate cake, coated in buttermilk and topped with crunchy chopped chocolate, drizzled with chocolate sauce simple but a nice change from chocolate cake author momma made ingredients for the chocolate buttermilk cake 1 1 2 cup unsalted butter, room temperature 1 1 2 cup granulated sugar 3 large eggs plus 1 egg yolk for the chocolate glaze 1 3 cup cocoa powder 1 3 4 cups powdered sugar 6 ounces semis gpt 2 completion 4 recipe meringue cookies 1 3 4 cup butter, softened 1 cup sugar 1 egg yolk 3 t ',\n",
       "  'yolk 3 t heavy cream 3 1 2 4 cups our pinch of salt peppermint jojo topping 1 1 4 cups powdered sugar 1 cup chopped pecans 1 2 cup nely chopped mint leaves 1 2 cup chopped fresh mint about 1 2 sheet 1 tsp peppermint extract 1 4 tsp sea salt pinch of sea gpt 2 completion 5 recipe my 2 failures at christmas morning were not due to poor frosting skills that was another success story the cake was beautifully moist, and well, just a beautiful cake it s',\n",
       "  'cake it s got very little air in it or moisture, but all the chocolate and mint was perfectly blended together one thing that made me cringe a bit was that all those ingredients egg yolks, sugar, melted butter, chocolate chips, melted chocolate, and mint had to be mixed right into the cake batter, super time consuming which is a friend suggested that if i put the batter in a separate bowl, i gpt 2 completion 6 recipe, which will give you about 6 9 servings of delicious chocolate cake to prevent your layers from sticking together, i used my 8 inch',\n",
       "  'used my 8 inch round tart pan, lined it with parchment paper, then cut up 4 7 inch round cakes from it and sandwiched the layers between them this also worked well for the 1 layer chocolate cake as well when you re ready to bake this cake, remove the parchment paper, preheat oven to 350 degrees and bake for about 30 40 minutes, until the top is golden and a tester comes out clean chocolate buttermilk layer cake with peppermint jo table 12 non cherry picked completions from gpt 2 generated from the same context '])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"greenhouse gases\"\n",
    "idx, scores = topic_model.find_topics(query, top_n=5)\n",
    "idx, scores, [text_list_processed[i] for i in get_docs_from_topics(idx[0], topic_model, text_list_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9, 4, 2, 8, 7],\n",
       " [0.24774578, 0.21856363, 0.20958671, 0.20594718, 0.20328423],\n",
       " ['transduction problems such as language modeling and machine translation 35, 2, 5 numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 38, 24, 15 recurrent models typically factor computation along the symbol positions of the input and output sequences aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht 1 and the input for position t this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints',\n",
       "  'all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions in these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for convs2s and logarithmically for bytenet this makes it more dif cult to learn dependencies between distant positions 12 in the transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention weighted positions, an effect we counteract with multi',\n",
       "  'figure 1 the transformer model architecture 3 1 encoder and decoder stacks encoder the encoder is composed of a stack of n 6 identical layers each layer has two sub layers the rst is a multi head self attention mechanism, and the second is a simple, position wise fully connected feed forward network we employ a residual connection 11 around each of the two sub layers, followed by layer normalization 1 that is, the output of each sub layer is layernorm x sublayer x ,',\n",
       "  ' dmodel in this work we employ h 8 parallel attention layers, or heads for each of these we use dk dv dmodel h 64 due to the reduced dimension of each head, the total computational cost is similar to that of single head attention with full dimensionality 3 2 3 applications of attention in our model the transformer uses multi head attention in three different ways in encoder decoder attention layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the en',\n",
       "  'dff 2048 3 4 embeddings and softmax similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel we also use the usual learned linear transfor mation and softmax function to convert the decoder output to predicted next token probabilities in our model, we share the same weight matrix between the two embedding layers and the pre softmax linear transformation, similar to 30 in the embedding layers, we multiply those weights by dmode',\n",
       "  'weights by dmodel 3 5 positional encoding since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the 5',\n",
       "  'table 1 maximum path lengths, per layer complexity and minimum number of sequential operations for different layer types n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention layer type complexity per layer sequential maximum path length operations self attention o n2 d o 1 o 1 recurrent o n d2 o n o n convolutional o k n d2 o 1 o logk n ',\n",
       "  ' sine functions of different frequencies pe pos, 2i sin pos 100002i dmodel pe pos, 2i 1 cos pos 100002i dmodel where pos is the position and i is the dimension that is, each dimension of the positional encoding corresponds to a sinusoid the wavelengths form a geometric progression from 2 to 10000 2 we chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for',\n",
       "  ' encies in the network learning long range dependencies is a key challenge in many sequence transduction tasks one key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network the shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long range dependencies 12 hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types as noted in table 1, a self attention layer connects all positions with a constant number of sequentially executed operations',\n",
       "  ' cantly larger wmt 2014 english french dataset consisting of 36m sentences and split tokens into a 32000 word piece vocabulary 38 sentence pairs were batched together by approximate sequence length each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens 5 2 hardware and schedule we trained our models on one machine with 8 nvidia p100 gpus for our base models using the hyperparameters described throughout the paper, each training step took about 0 4 seconds we trained the base models for a total of 100, 000 steps',\n",
       "  'of 100, 000 steps or 12 hours for our big models, described on the bottom line of table 3 , step time was 1 0 seconds the big models were trained for 300, 000 steps 3 5 days 5 3 optimizer we used the adam optimizer 20 with 1 0 9, 2 0 98 and UNK 10 9 we varied the learning rate over the course of training, according to the formula lrate d 0 5 model min step num 0 5, step num',\n",
       "  ', step num warmup steps 1 5 3 this corresponds to increasing the learning rate linearly for the rst warmup steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number we used warmup steps 4000 5 4 regularization we employ three types of regularization during training residual dropout we apply dropout 33 to the output of each sub layer, before it is added to the sub layer input and normalized in addition, we apply dropout to the sums of the embeddings and the',\n",
       "  ' beddings and the positional encodings in both the encoder and decoder stacks for the base model, we use a rate of pdrop 0 1 7',\n",
       "  'models including ensembles by more than 2 0 bleu, establishing a new state of the art bleu score of 28 4 the con guration of this model is listed in the bottom line of table 3 training took 3 5 days on 8 p100 gpus even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models on the wmt 2014 english to french translation task, our big model achieves a bleu score of 41 0, outperforming all of the',\n",
       "  ' forming all of the previously published single models, at less than 1 4 the training cost of the previous state of the art model the transformer big model trained for english to french used dropout rate pdrop 0 1, instead of 0 3 for the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10 minute intervals for the big models, we averaged the last 20 checkpoints we used beam search with a beam size of 4 and length penalty 0 6 38 these hyperpara',\n",
       "  ' these hyperparameters were chosen after experimentation on the development set we set the maximum output length during inference to input length 50, but terminate early when possible 38 table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature we estimate the number of oating point operations used to train a model by multiplying the training time, the number of gpus used, and an estimate of the sustained single precision oating point capacity of each gpu 5 6 2 model variations to evaluate the importance of different components of the',\n",
       "  'constraints and is signi cantly longer than the input furthermore, rnn sequence to sequence models have not been able to attain state of the art results in small data regimes 37 we trained a 4 layer transformer with dmodel 1024 on the wall street journal wsj portion of the penn treebank 25 , about 40k training sentences we also trained it in a semi supervised setting, using the larger high con dence and berkleyparser corpora from with approximately 17m sentences 37 we used a',\n",
       "  ' we used a vocabulary of 16k tokens for the wsj only setting and a vocabulary of 32k tokens for the semi supervised setting we performed only a small number of experiments to select the dropout, both attention and residual section 5 4 , learning rates and beam size on the section 22 development set, all other parameters remained unchanged from the english to german base translation model during inference, we increased the maximum output length to input length 300 we used a beam size of 21 and 0 3 for both wsj only and the semi supervised setting our',\n",
       "  'noam shazeer, azalia mirhoseini, krzysztof maziarz, andy davis, quoc le, geoffrey hinton, and jeff dean outrageously large neural networks the sparsely gated mixture of experts layer arxiv preprint arxiv 1701 06538, 2017 33 nitish srivastava, geoffrey e hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdi nov dropout a simple way to prevent neural networks from over tting journal of machine',\n",
       "  'other datasets and could complicate analysis due to over 1https github com codelucas newspaper',\n",
       "  ' ick et al 2015 , current byte level lms are not competitive with word level lms on large scale datasets such as the one billion word benchmark al rfou et al , 2018 we observed a similar performance gap in our own attempts to train standard byte level lms on webtext byte pair encoding bpe sennrich et al , 2015 is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and char acter level inputs for infrequent symbol sequences ',\n",
       "  ' equent symbol sequences despite its name, reference bpe implementations often operate on unicode code points and not byte sequences these imple mentations would require including the full space of uni code symbols in order to model all unicode strings this would result in a base vocabulary of over 130, 000 before any multi symbol tokens are added this is prohibitively large compared to the 32, 000 to 64, 000 token vocabularies often used with bpe in contrast, a byte level version of bpe only requires a base vocabulary of size 256 however, directly applying bpe to the byte',\n",
       "  'bpe to the byte sequence results in sub optimal merges due to bpe using a greedy frequency based heuristic for building the token vocabulary we observed bpe including many versions of common words like dog since they occur in many variations such as dog dog dog this results in a sub optimal allocation of limited vocabulary slots and model capacity to avoid this, we pre vent bpe from merging across character categories for any byte sequence we add an exception for spaces which sig ni cantly improves the compression ef ciency while adding only minimal fragmentation of words across multiple vocab token',\n",
       "  'multiple vocab tokens this input representation allows us to combine the empirical bene ts of word level lms with the generality of byte level approaches since our approach can assign a probability to any unicode string, this allows us to evaluate our lms on any dataset regardless of pre processing, tokenization, or vocab size 2 3 model we use a transformer vaswani et al , 2017 based archi tecture for our lms the model largely follows the details of the openai gpt model radford et al , 2018',\n",
       "  'et al , 2018 with a parameters layers dmodel 117m 12 768 345m 24 1024 762m 36 1280 1542m 48 1600 table 2 architecture hyperparameters for the 4 model sizes few modi cations layer normalization ba et al , 2016 was moved to the input of each sub block, similar to a pre activation residual network he et al , 2016 and an additional layer normalization was added after the nal self attention block a modi ed initialization which accounts for the accumulation on the residual path with model',\n",
       "  'the residual path with model depth is used we scale the weights of residual layers at initial ization by a factor of 1 n where n is the number of residual layers the vocabulary is expanded to 50, 257 we also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used 3 experiments we trained and benchmarked four lms with approximately log uniformly spaced sizes the architectures are summa rized in table 2 the smallest model is equivalent to the original gpt, and the second smallest equivalent to the largest model from bert devlin',\n",
       "  '7 out of the 8 datasets in a zero shot setting large improvements are noticed on small datasets such as penn treebank and wikitext 2 which have only 1 to 2 million training tokens large improvements are also noticed on datasets created to measure long term dependencies like lambada paperno et al , 2016 and the children s book test hill et al , 2015 our model is still signi cantly worse than prior work on the one billion word benchmark chelba et al , 2013 this is likely due to',\n",
       "  'this is likely due to a combination of it being both the largest dataset and having some of the most destructive pre processing 1bw s sentence level shuf ing removes all long range structure 3 2 children s book test figure 2 performance on the children s book test as a function of model capacity human performance are from bajgar et al 2016 , instead of the much lower estimates from the original paper the children s book test cbt hill et al , 2015 was created to examine the performance of lms on different cat egories',\n",
       "  'is increased and closes the majority of the gap to human performance on this test data overlap analysis showed one of the cbt test set books, the jungle book by rudyard kipling, is in webtext, so we report results on the validation set which has no signi cant overlap gpt 2 achieves new state of the art results of 93 3 on common nouns and 89 1 on named entities a de tokenizer was applied to remove ptb style tokenization artifacts from cbt 3 3 lambada the lambada dataset paperno et al , 2016',\n",
       "  'the test set of wikitext 103 has an article which is also in the training dataset since there are only 60 articles in the test set there is at least an overlap of 1 6 4 potentially more worryingly, 1bw has an overlap of nearly 13 2 with its own training set according to our procedure for the winograd schema challenge, we found only 10 schemata which had any 8 gram overlaps with the webtext training set of these, 2 were spurious matches of the remaining 8, only 1 schema appeared in any contexts that 4a signi',\n",
       "  ' text training data and speci c evaluation datasets pro vides a small but consistent bene t to reported results how ever, for most datasets we do not notice signi cantly larger overlaps than those already existing between standard train ing and test sets, as table 6 highlights understanding and quantifying how highly similar text im pacts performance is an important research question better de duplication techniques such as scalable fuzzy matching could also help better answer these questions for now, we recommend the use of n gram overlap based de duplication as an important ve',\n",
       "  'worth of paragraphs shown completions are 256 tokens and fully shown top k random sampling with k 40 was used for generation ',\n",
       "  ' top k random sampling with k 40 was used for generation ',\n",
       "  ' at this point, the ghost will attempt to exit the safe house in a similar manner as it is in chase mode pac man there are some ghosts that follow pac man these ghosts are table 9 random unseen contexts top , and non cherry picked completions from both the smallest left and largest right models contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown completions are 256 tokens and fully shown top k random sampling with k 40 was used for generation ',\n",
       "  'random unseen contexts top , and non cherry picked completions from both the smallest left and largest right models contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown completions are 256 tokens and fully shown top k random sampling with k 40 was used for generation ',\n",
       "  'the color is assigned to your table 11 random unseen contexts top , and non cherry picked completions from both the smallest left and largest right models contexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown completions are 256 tokens and fully shown top k random sampling with k 40 was used for generation ',\n",
       "  'from the same context from webtext test context is 384 tokens shown truncated , and generations are 128 tokens top k random sampling with k 40 was used for generation ',\n",
       "  'dataset ',\n",
       "  'language models are unsupervised multitask learners context passage and previous question answer pairs the 2008 summer olympics torch relay was run from march 24 until august 8, 2008, prior to the 2008 summer olympics, with the theme of one world, one dream plans for the relay were announced on april 26, 2007, in beijing, china the relay, also called by the organizers as the journey of harmony , lasted 129 days and carried the torch 137, 000 km 85, 000 mi the longest distance of any olympic torch relay since the tradition was started ahead of the 1936 summer olympics'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"killing philosophy\"\n",
    "idx, scores = topic_model.find_topics(query, top_n=5)\n",
    "idx, scores, [text_list_processed[i] for i in get_docs_from_topics(idx[0], topic_model, text_list_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          [
           0,
           "gpt | translation | fonctionnelle | french | fonctionnement",
           29
          ],
          [
           1,
           "cygwin | gwin | ssh | permissions | polosukhin",
           12
          ],
          [
           2,
           "multitask | learners | language | learning | supervised",
           78
          ],
          [
           3,
           "deeppoly | constraints | mlp | l1 | neural",
           26
          ],
          [
           4,
           "neural | attention | learning | recurrent | decoder",
           85
          ],
          [
           5,
           "recipe | cake | chocolate | cookies | bake",
           12
          ],
          [
           6,
           "enlisted | lieutenant | enlistment | regiment | infantry",
           6
          ],
          [
           7,
           "economy | economic | rates | cpi | demand",
           23
          ],
          [
           8,
           "prehistoric | cave | reconquista | spain | iberian",
           17
          ],
          [
           9,
           "completions | contexts | paragraphs | attention | tokens",
           39
          ]
         ],
         "hovertemplate": "<b>Topic %{customdata[0]}</b><br>%{customdata[1]}<br>Size: %{customdata[2]}",
         "legendgroup": "",
         "marker": {
          "color": "#B0BEC5",
          "line": {
           "color": "DarkSlateGrey",
           "width": 2
          },
          "size": [
           29,
           12,
           78,
           26,
           85,
           12,
           6,
           23,
           17,
           39
          ],
          "sizemode": "area",
          "sizeref": 0.053125,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          8.166999816894531,
          7.955646514892578,
          9.244952201843262,
          8.881340980529785,
          9.708738327026367,
          10.610133171081543,
          8.723332405090332,
          8.594858169555664,
          8.532580375671387,
          10.377021789550781
         ],
         "xaxis": "x",
         "y": [
          8.181861877441406,
          8.73430061340332,
          11.015793800354004,
          12.293052673339844,
          11.225058555603027,
          11.936985969543457,
          8.425163269042969,
          12.676069259643555,
          9.179332733154297,
          11.558425903320312
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "showarrow": false,
          "text": "D1",
          "x": 6.762299537658691,
          "y": 10.766031122207643,
          "yshift": 10
         },
         {
          "showarrow": false,
          "text": "D2",
          "x": 9.481976342201232,
          "xshift": 10,
          "y": 14.577479648590089
         }
        ],
        "height": 650,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "legend": {
         "itemsizing": "constant",
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "#CFD8DC",
           "width": 2
          },
          "type": "line",
          "x0": 9.481976342201232,
          "x1": 9.481976342201232,
          "y0": 6.954582595825196,
          "y1": 14.577479648590089
         },
         {
          "line": {
           "color": "#9E9E9E",
           "width": 2
          },
          "type": "line",
          "x0": 6.762299537658691,
          "x1": 12.201653146743775,
          "y0": 10.766031122207643,
          "y1": 10.766031122207643
         }
        ],
        "sliders": [
         {
          "active": 0,
          "pad": {
           "t": 50
          },
          "steps": [
           {
            "args": [
             {
              "marker.color": [
               [
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 0",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 1",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 2",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 3",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 4",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 5",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 6",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 7",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red",
                "#B0BEC5"
               ]
              ]
             }
            ],
            "label": "Topic 8",
            "method": "update"
           },
           {
            "args": [
             {
              "marker.color": [
               [
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "#B0BEC5",
                "red"
               ]
              ]
             }
            ],
            "label": "Topic 9",
            "method": "update"
           }
          ]
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Intertopic Distance Map</b>",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 650,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "range": [
          6.762299537658691,
          12.201653146743775
         ],
         "title": {
          "text": ""
         },
         "visible": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "range": [
          6.954582595825196,
          14.577479648590089
         ],
         "title": {
          "text": ""
         },
         "visible": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiz this quiz is about kants philosophy and the moral implications of killing answer the questions below to test your knowledge 1 what is the reason for the emergence of morality evolution logic compassion materialism 2 what is the consequence of not beingcompatiblewith other humans death isolation rejection abandonment 3 what is the base for altruism probabilistic reason logic evolution submit\n"
     ]
    }
   ],
   "source": [
    "for text in text_list_processed:\n",
    "    if \"killing\" in text:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9, 4, 2, 8, 7],\n",
       " [0.24774578, 0.21856363, 0.20958671, 0.20594718, 0.20328423],\n",
       " [(13,\n",
       "   ' r and decoder, shown in the left and right halves of figure 1, respectively 2',\n",
       "   0.24774578213691711),\n",
       "  (32,\n",
       "   ' sine functions of different frequencies pe pos, 2i sin pos 100002i dmodel pe pos, 2i 1 cos pos 100002i dmodel where pos is the position and i is the dimension that is, each dimension of the positional encoding corresponds to a sinusoid the wavelengths form a geometric progression from 2 to 10000 2 we chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for',\n",
       "   0.24774578213691711),\n",
       "  (51,\n",
       "   'drops off with too many heads 5we used values of 2 8, 3 7, 6 0 and 9 5 tflops for k80, k40, m40 and p100, respectively 8',\n",
       "   0.24774578213691711),\n",
       "  (307,\n",
       "   'seascapes and incredible wildlife of the world s oceans so if you have a spare few million in the bank and want some holiday snaps and instagram posts that will really trump everyone else, you may want to snap up one of these whale of a time the orcasub takes you 2000 feet beneath the surface in two pressurised perspex viewing domes for optimum exploration gpt 2 the u boat worx submarine can dive to 984 feet 300 metres and is capable of diving to the bottom of the ocean it can be used for diving, s',\n",
       "   0.24774578213691711),\n",
       "  (41,\n",
       "   'of 100, 000 steps or 12 hours for our big models, described on the bottom line of table 3 , step time was 1 0 seconds the big models were trained for 300, 000 steps 3 5 days 5 3 optimizer we used the adam optimizer 20 with 1 0 9, 2 0 98 and UNK 10 9 we varied the learning rate over the course of training, according to the formula lrate d 0 5 model min step num 0 5, step num',\n",
       "   0.23851256263971554)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"killing philosophy\"\n",
    "idx, scores = topic_model.find_topics(query, top_n=5)\n",
    "idx, scores, get_topk_docs(idx, scores, topic_model, text_list_processed, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_hackweek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
